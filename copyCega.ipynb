{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:shuffel = False\n",
      "eval:shuffel = False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 13, got 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdataloader\u001b[39;00m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m----> 4\u001b[0m trainloader ,evalloader, testloader ,X_train, X_eval, X_test, y_train, y_eval, y_test , inputFeatures, outputFeatures, datasetName, features_names\u001b[39m=\u001b[39m dataloader\u001b[39m.\u001b[39mload_kaggle_diabetes_dataset(batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_breast_cancer\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(features_names)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 13, got 10)"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import dataloader\n",
    "batch_size = 4\n",
    "trainloader ,evalloader, testloader ,X_train, X_eval, X_test, y_train, y_eval, y_test , inputFeatures, outputFeatures, datasetName, features_names= dataloader.load_kaggle_diabetes_dataset(batch_size=batch_size)\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "print(features_names)\n",
    "#data = load_breast_cancer()\n",
    "#features_names = data.feature_names\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "      Training_acc: 0.6351791530944625\n",
      "-------------------\n",
      "\n",
      "Epoch: 1\n",
      "      Training_acc: 0.6677524429967426\n",
      "-------------------\n",
      "\n",
      "Epoch: 2\n",
      "      Training_acc: 0.6807817589576547\n",
      "-------------------\n",
      "\n",
      "Epoch: 3\n",
      "      Training_acc: 0.6856677524429967\n",
      "-------------------\n",
      "\n",
      "Epoch: 4\n",
      "      Training_acc: 0.6905537459283387\n",
      "-------------------\n",
      "\n",
      "Epoch: 5\n",
      "      Training_acc: 0.6986970684039088\n",
      "-------------------\n",
      "\n",
      "Epoch: 6\n",
      "      Training_acc: 0.6905537459283387\n",
      "-------------------\n",
      "\n",
      "Epoch: 7\n",
      "      Training_acc: 0.7019543973941368\n",
      "-------------------\n",
      "\n",
      "Epoch: 8\n",
      "      Training_acc: 0.7003257328990228\n",
      "-------------------\n",
      "\n",
      "Epoch: 9\n",
      "      Training_acc: 0.6840390879478827\n",
      "-------------------\n",
      "\n",
      "Epoch: 10\n",
      "      Training_acc: 0.6791530944625407\n",
      "-------------------\n",
      "\n",
      "Epoch: 11\n",
      "      Training_acc: 0.7003257328990228\n",
      "-------------------\n",
      "\n",
      "Epoch: 12\n",
      "      Training_acc: 0.7019543973941368\n",
      "-------------------\n",
      "\n",
      "Epoch: 13\n",
      "      Training_acc: 0.6954397394136808\n",
      "-------------------\n",
      "\n",
      "Epoch: 14\n",
      "      Training_acc: 0.7068403908794788\n",
      "-------------------\n",
      "\n",
      "Epoch: 15\n",
      "      Training_acc: 0.7052117263843648\n",
      "-------------------\n",
      "\n",
      "Epoch: 16\n",
      "      Training_acc: 0.7100977198697068\n",
      "-------------------\n",
      "\n",
      "Epoch: 17\n",
      "      Training_acc: 0.6938110749185668\n",
      "-------------------\n",
      "\n",
      "Epoch: 18\n",
      "      Training_acc: 0.6872964169381107\n",
      "-------------------\n",
      "\n",
      "Epoch: 19\n",
      "      Training_acc: 0.6921824104234527\n",
      "-------------------\n",
      "\n",
      "Epoch: 20\n",
      "      Training_acc: 0.6824104234527687\n",
      "-------------------\n",
      "\n",
      "Epoch: 21\n",
      "      Training_acc: 0.7003257328990228\n",
      "-------------------\n",
      "\n",
      "Epoch: 22\n",
      "      Training_acc: 0.7100977198697068\n",
      "-------------------\n",
      "\n",
      "Epoch: 23\n",
      "      Training_acc: 0.7084690553745928\n",
      "-------------------\n",
      "\n",
      "Epoch: 24\n",
      "      Training_acc: 0.6921824104234527\n",
      "-------------------\n",
      "\n",
      "Epoch: 25\n",
      "      Training_acc: 0.7117263843648208\n",
      "-------------------\n",
      "\n",
      "Epoch: 26\n",
      "      Training_acc: 0.7084690553745928\n",
      "-------------------\n",
      "\n",
      "Epoch: 27\n",
      "      Training_acc: 0.7019543973941368\n",
      "-------------------\n",
      "\n",
      "Epoch: 28\n",
      "      Training_acc: 0.7052117263843648\n",
      "-------------------\n",
      "\n",
      "Epoch: 29\n",
      "      Training_acc: 0.7068403908794788\n",
      "-------------------\n",
      "\n",
      "Epoch: 30\n",
      "      Training_acc: 0.7035830618892508\n",
      "-------------------\n",
      "\n",
      "Epoch: 31\n",
      "      Training_acc: 0.7280130293159609\n",
      "-------------------\n",
      "\n",
      "Epoch: 32\n",
      "      Training_acc: 0.7133550488599348\n",
      "-------------------\n",
      "\n",
      "Epoch: 33\n",
      "      Training_acc: 0.7198697068403909\n",
      "-------------------\n",
      "\n",
      "Epoch: 34\n",
      "      Training_acc: 0.7133550488599348\n",
      "-------------------\n",
      "\n",
      "Epoch: 35\n",
      "      Training_acc: 0.7019543973941368\n",
      "-------------------\n",
      "\n",
      "Epoch: 36\n",
      "      Training_acc: 0.7133550488599348\n",
      "-------------------\n",
      "\n",
      "Epoch: 37\n",
      "      Training_acc: 0.7231270358306189\n",
      "-------------------\n",
      "\n",
      "Epoch: 38\n",
      "      Training_acc: 0.7280130293159609\n",
      "-------------------\n",
      "\n",
      "Epoch: 39\n",
      "      Training_acc: 0.7084690553745928\n",
      "-------------------\n",
      "\n",
      "Epoch: 40\n",
      "      Training_acc: 0.7068403908794788\n",
      "-------------------\n",
      "\n",
      "Epoch: 41\n",
      "      Training_acc: 0.7019543973941368\n",
      "-------------------\n",
      "\n",
      "Epoch: 42\n",
      "      Training_acc: 0.7117263843648208\n",
      "-------------------\n",
      "\n",
      "Epoch: 43\n",
      "      Training_acc: 0.7149837133550488\n",
      "-------------------\n",
      "\n",
      "Epoch: 44\n",
      "      Training_acc: 0.7100977198697068\n",
      "-------------------\n",
      "\n",
      "Epoch: 45\n",
      "      Training_acc: 0.7052117263843648\n",
      "-------------------\n",
      "\n",
      "Epoch: 46\n",
      "      Training_acc: 0.737785016286645\n",
      "-------------------\n",
      "\n",
      "Epoch: 47\n",
      "      Training_acc: 0.7019543973941368\n",
      "-------------------\n",
      "\n",
      "Epoch: 48\n",
      "      Training_acc: 0.7084690553745928\n",
      "-------------------\n",
      "\n",
      "Epoch: 49\n",
      "      Training_acc: 0.7084690553745928\n",
      "-------------------\n",
      "\n",
      "Epoch: 50\n",
      "      Training_acc: 0.7166123778501629\n",
      "-------------------\n",
      "\n",
      "Epoch: 51\n",
      "      Training_acc: 0.7214983713355049\n",
      "-------------------\n",
      "\n",
      "Epoch: 52\n",
      "      Training_acc: 0.7198697068403909\n",
      "-------------------\n",
      "\n",
      "Epoch: 53\n",
      "      Training_acc: 0.7100977198697068\n",
      "-------------------\n",
      "\n",
      "Epoch: 54\n",
      "      Training_acc: 0.7068403908794788\n",
      "-------------------\n",
      "\n",
      "Epoch: 55\n",
      "      Training_acc: 0.7166123778501629\n",
      "-------------------\n",
      "\n",
      "Epoch: 56\n",
      "      Training_acc: 0.7133550488599348\n",
      "-------------------\n",
      "\n",
      "Epoch: 57\n",
      "      Training_acc: 0.7166123778501629\n",
      "-------------------\n",
      "\n",
      "Epoch: 58\n",
      "      Training_acc: 0.7133550488599348\n",
      "-------------------\n",
      "\n",
      "Epoch: 59\n",
      "      Training_acc: 0.7345276872964169\n",
      "-------------------\n",
      "\n",
      "Epoch: 60\n",
      "      Training_acc: 0.7133550488599348\n",
      "-------------------\n",
      "\n",
      "Epoch: 61\n",
      "      Training_acc: 0.7166123778501629\n",
      "-------------------\n",
      "\n",
      "Epoch: 62\n",
      "      Training_acc: 0.7214983713355049\n",
      "-------------------\n",
      "\n",
      "Epoch: 63\n",
      "      Training_acc: 0.7198697068403909\n",
      "-------------------\n",
      "\n",
      "Epoch: 64\n",
      "      Training_acc: 0.7247557003257329\n",
      "-------------------\n",
      "\n",
      "Epoch: 65\n",
      "      Training_acc: 0.7214983713355049\n",
      "-------------------\n",
      "\n",
      "Epoch: 66\n",
      "      Training_acc: 0.7182410423452769\n",
      "-------------------\n",
      "\n",
      "Epoch: 67\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 68\n",
      "      Training_acc: 0.7345276872964169\n",
      "-------------------\n",
      "\n",
      "Epoch: 69\n",
      "      Training_acc: 0.737785016286645\n",
      "-------------------\n",
      "\n",
      "Epoch: 70\n",
      "      Training_acc: 0.7312703583061889\n",
      "-------------------\n",
      "\n",
      "Epoch: 71\n",
      "      Training_acc: 0.7296416938110749\n",
      "-------------------\n",
      "\n",
      "Epoch: 72\n",
      "      Training_acc: 0.7328990228013029\n",
      "-------------------\n",
      "\n",
      "Epoch: 73\n",
      "      Training_acc: 0.7068403908794788\n",
      "-------------------\n",
      "\n",
      "Epoch: 74\n",
      "      Training_acc: 0.7231270358306189\n",
      "-------------------\n",
      "\n",
      "Epoch: 75\n",
      "      Training_acc: 0.7231270358306189\n",
      "-------------------\n",
      "\n",
      "Epoch: 76\n",
      "      Training_acc: 0.7280130293159609\n",
      "-------------------\n",
      "\n",
      "Epoch: 77\n",
      "      Training_acc: 0.7214983713355049\n",
      "-------------------\n",
      "\n",
      "Epoch: 78\n",
      "      Training_acc: 0.747557003257329\n",
      "-------------------\n",
      "\n",
      "Epoch: 79\n",
      "      Training_acc: 0.7312703583061889\n",
      "-------------------\n",
      "\n",
      "Epoch: 80\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 81\n",
      "      Training_acc: 0.7361563517915309\n",
      "-------------------\n",
      "\n",
      "Epoch: 82\n",
      "      Training_acc: 0.7198697068403909\n",
      "-------------------\n",
      "\n",
      "Epoch: 83\n",
      "      Training_acc: 0.7296416938110749\n",
      "-------------------\n",
      "\n",
      "Epoch: 84\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 85\n",
      "      Training_acc: 0.7247557003257329\n",
      "-------------------\n",
      "\n",
      "Epoch: 86\n",
      "      Training_acc: 0.7361563517915309\n",
      "-------------------\n",
      "\n",
      "Epoch: 87\n",
      "      Training_acc: 0.7345276872964169\n",
      "-------------------\n",
      "\n",
      "Epoch: 88\n",
      "      Training_acc: 0.7296416938110749\n",
      "-------------------\n",
      "\n",
      "Epoch: 89\n",
      "      Training_acc: 0.7198697068403909\n",
      "-------------------\n",
      "\n",
      "Epoch: 90\n",
      "      Training_acc: 0.754071661237785\n",
      "-------------------\n",
      "\n",
      "Epoch: 91\n",
      "      Training_acc: 0.7182410423452769\n",
      "-------------------\n",
      "\n",
      "Epoch: 92\n",
      "      Training_acc: 0.754071661237785\n",
      "-------------------\n",
      "\n",
      "Epoch: 93\n",
      "      Training_acc: 0.7263843648208469\n",
      "-------------------\n",
      "\n",
      "Epoch: 94\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 95\n",
      "      Training_acc: 0.749185667752443\n",
      "-------------------\n",
      "\n",
      "Epoch: 96\n",
      "      Training_acc: 0.7214983713355049\n",
      "-------------------\n",
      "\n",
      "Epoch: 97\n",
      "      Training_acc: 0.737785016286645\n",
      "-------------------\n",
      "\n",
      "Epoch: 98\n",
      "      Training_acc: 0.7280130293159609\n",
      "-------------------\n",
      "\n",
      "Epoch: 99\n",
      "      Training_acc: 0.7280130293159609\n",
      "-------------------\n",
      "\n",
      "Epoch: 100\n",
      "      Training_acc: 0.737785016286645\n",
      "-------------------\n",
      "\n",
      "Epoch: 101\n",
      "      Training_acc: 0.7312703583061889\n",
      "-------------------\n",
      "\n",
      "Epoch: 102\n",
      "      Training_acc: 0.739413680781759\n",
      "-------------------\n",
      "\n",
      "Epoch: 103\n",
      "      Training_acc: 0.7296416938110749\n",
      "-------------------\n",
      "\n",
      "Epoch: 104\n",
      "      Training_acc: 0.737785016286645\n",
      "-------------------\n",
      "\n",
      "Epoch: 105\n",
      "      Training_acc: 0.7231270358306189\n",
      "-------------------\n",
      "\n",
      "Epoch: 106\n",
      "      Training_acc: 0.7345276872964169\n",
      "-------------------\n",
      "\n",
      "Epoch: 107\n",
      "      Training_acc: 0.739413680781759\n",
      "-------------------\n",
      "\n",
      "Epoch: 108\n",
      "      Training_acc: 0.7361563517915309\n",
      "-------------------\n",
      "\n",
      "Epoch: 109\n",
      "      Training_acc: 0.739413680781759\n",
      "-------------------\n",
      "\n",
      "Epoch: 110\n",
      "      Training_acc: 0.737785016286645\n",
      "-------------------\n",
      "\n",
      "Epoch: 111\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 112\n",
      "      Training_acc: 0.742671009771987\n",
      "-------------------\n",
      "\n",
      "Epoch: 113\n",
      "      Training_acc: 0.742671009771987\n",
      "-------------------\n",
      "\n",
      "Epoch: 114\n",
      "      Training_acc: 0.737785016286645\n",
      "-------------------\n",
      "\n",
      "Epoch: 115\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 116\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 117\n",
      "      Training_acc: 0.7280130293159609\n",
      "-------------------\n",
      "\n",
      "Epoch: 118\n",
      "      Training_acc: 0.7328990228013029\n",
      "-------------------\n",
      "\n",
      "Epoch: 119\n",
      "      Training_acc: 0.739413680781759\n",
      "-------------------\n",
      "\n",
      "Epoch: 120\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 121\n",
      "      Training_acc: 0.7345276872964169\n",
      "-------------------\n",
      "\n",
      "Epoch: 122\n",
      "      Training_acc: 0.739413680781759\n",
      "-------------------\n",
      "\n",
      "Epoch: 123\n",
      "      Training_acc: 0.7231270358306189\n",
      "-------------------\n",
      "\n",
      "Epoch: 124\n",
      "      Training_acc: 0.7296416938110749\n",
      "-------------------\n",
      "\n",
      "Epoch: 125\n",
      "      Training_acc: 0.749185667752443\n",
      "-------------------\n",
      "\n",
      "Epoch: 126\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 127\n",
      "      Training_acc: 0.7280130293159609\n",
      "-------------------\n",
      "\n",
      "Epoch: 128\n",
      "      Training_acc: 0.757328990228013\n",
      "-------------------\n",
      "\n",
      "Epoch: 129\n",
      "      Training_acc: 0.737785016286645\n",
      "-------------------\n",
      "\n",
      "Epoch: 130\n",
      "      Training_acc: 0.7312703583061889\n",
      "-------------------\n",
      "\n",
      "Epoch: 131\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 132\n",
      "      Training_acc: 0.7296416938110749\n",
      "-------------------\n",
      "\n",
      "Epoch: 133\n",
      "      Training_acc: 0.7296416938110749\n",
      "-------------------\n",
      "\n",
      "Epoch: 134\n",
      "      Training_acc: 0.7361563517915309\n",
      "-------------------\n",
      "\n",
      "Epoch: 135\n",
      "      Training_acc: 0.7296416938110749\n",
      "-------------------\n",
      "\n",
      "Epoch: 136\n",
      "      Training_acc: 0.749185667752443\n",
      "-------------------\n",
      "\n",
      "Epoch: 137\n",
      "      Training_acc: 0.747557003257329\n",
      "-------------------\n",
      "\n",
      "Epoch: 138\n",
      "      Training_acc: 0.7361563517915309\n",
      "-------------------\n",
      "\n",
      "Epoch: 139\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 140\n",
      "      Training_acc: 0.7361563517915309\n",
      "-------------------\n",
      "\n",
      "Epoch: 141\n",
      "      Training_acc: 0.7361563517915309\n",
      "-------------------\n",
      "\n",
      "Epoch: 142\n",
      "      Training_acc: 0.739413680781759\n",
      "-------------------\n",
      "\n",
      "Epoch: 143\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 144\n",
      "      Training_acc: 0.7328990228013029\n",
      "-------------------\n",
      "\n",
      "Epoch: 145\n",
      "      Training_acc: 0.754071661237785\n",
      "-------------------\n",
      "\n",
      "Epoch: 146\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 147\n",
      "      Training_acc: 0.7312703583061889\n",
      "-------------------\n",
      "\n",
      "Epoch: 148\n",
      "      Training_acc: 0.7361563517915309\n",
      "-------------------\n",
      "\n",
      "Epoch: 149\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 150\n",
      "      Training_acc: 0.7296416938110749\n",
      "-------------------\n",
      "\n",
      "Epoch: 151\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 152\n",
      "      Training_acc: 0.747557003257329\n",
      "-------------------\n",
      "\n",
      "Epoch: 153\n",
      "      Training_acc: 0.7312703583061889\n",
      "-------------------\n",
      "\n",
      "Epoch: 154\n",
      "      Training_acc: 0.737785016286645\n",
      "-------------------\n",
      "\n",
      "Epoch: 155\n",
      "      Training_acc: 0.745928338762215\n",
      "-------------------\n",
      "\n",
      "Epoch: 156\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 157\n",
      "      Training_acc: 0.742671009771987\n",
      "-------------------\n",
      "\n",
      "Epoch: 158\n",
      "      Training_acc: 0.742671009771987\n",
      "-------------------\n",
      "\n",
      "Epoch: 159\n",
      "      Training_acc: 0.745928338762215\n",
      "-------------------\n",
      "\n",
      "Epoch: 160\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 161\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 162\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 163\n",
      "      Training_acc: 0.745928338762215\n",
      "-------------------\n",
      "\n",
      "Epoch: 164\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 165\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 166\n",
      "      Training_acc: 0.760586319218241\n",
      "-------------------\n",
      "\n",
      "Epoch: 167\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 168\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 169\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 170\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 171\n",
      "      Training_acc: 0.745928338762215\n",
      "-------------------\n",
      "\n",
      "Epoch: 172\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 173\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 174\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 175\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 176\n",
      "      Training_acc: 0.757328990228013\n",
      "-------------------\n",
      "\n",
      "Epoch: 177\n",
      "      Training_acc: 0.744299674267101\n",
      "-------------------\n",
      "\n",
      "Epoch: 178\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 179\n",
      "      Training_acc: 0.754071661237785\n",
      "-------------------\n",
      "\n",
      "Epoch: 180\n",
      "      Training_acc: 0.762214983713355\n",
      "-------------------\n",
      "\n",
      "Epoch: 181\n",
      "      Training_acc: 0.755700325732899\n",
      "-------------------\n",
      "\n",
      "Epoch: 182\n",
      "      Training_acc: 0.739413680781759\n",
      "-------------------\n",
      "\n",
      "Epoch: 183\n",
      "      Training_acc: 0.754071661237785\n",
      "-------------------\n",
      "\n",
      "Epoch: 184\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 185\n",
      "      Training_acc: 0.742671009771987\n",
      "-------------------\n",
      "\n",
      "Epoch: 186\n",
      "      Training_acc: 0.742671009771987\n",
      "-------------------\n",
      "\n",
      "Epoch: 187\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 188\n",
      "      Training_acc: 0.760586319218241\n",
      "-------------------\n",
      "\n",
      "Epoch: 189\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 190\n",
      "      Training_acc: 0.749185667752443\n",
      "-------------------\n",
      "\n",
      "Epoch: 191\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 192\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 193\n",
      "      Training_acc: 0.745928338762215\n",
      "-------------------\n",
      "\n",
      "Epoch: 194\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 195\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 196\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 197\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 198\n",
      "      Training_acc: 0.755700325732899\n",
      "-------------------\n",
      "\n",
      "Epoch: 199\n",
      "      Training_acc: 0.7345276872964169\n",
      "-------------------\n",
      "\n",
      "Epoch: 200\n",
      "      Training_acc: 0.760586319218241\n",
      "-------------------\n",
      "\n",
      "Epoch: 201\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 202\n",
      "      Training_acc: 0.747557003257329\n",
      "-------------------\n",
      "\n",
      "Epoch: 203\n",
      "      Training_acc: 0.755700325732899\n",
      "-------------------\n",
      "\n",
      "Epoch: 204\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 205\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 206\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 207\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 208\n",
      "      Training_acc: 0.755700325732899\n",
      "-------------------\n",
      "\n",
      "Epoch: 209\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 210\n",
      "      Training_acc: 0.760586319218241\n",
      "-------------------\n",
      "\n",
      "Epoch: 211\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 212\n",
      "      Training_acc: 0.762214983713355\n",
      "-------------------\n",
      "\n",
      "Epoch: 213\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 214\n",
      "      Training_acc: 0.741042345276873\n",
      "-------------------\n",
      "\n",
      "Epoch: 215\n",
      "      Training_acc: 0.754071661237785\n",
      "-------------------\n",
      "\n",
      "Epoch: 216\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 217\n",
      "      Training_acc: 0.754071661237785\n",
      "-------------------\n",
      "\n",
      "Epoch: 218\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 219\n",
      "      Training_acc: 0.755700325732899\n",
      "-------------------\n",
      "\n",
      "Epoch: 220\n",
      "      Training_acc: 0.760586319218241\n",
      "-------------------\n",
      "\n",
      "Epoch: 221\n",
      "      Training_acc: 0.755700325732899\n",
      "-------------------\n",
      "\n",
      "Epoch: 222\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 223\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 224\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 225\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 226\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 227\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 228\n",
      "      Training_acc: 0.757328990228013\n",
      "-------------------\n",
      "\n",
      "Epoch: 229\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 230\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 231\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 232\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 233\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 234\n",
      "      Training_acc: 0.757328990228013\n",
      "-------------------\n",
      "\n",
      "Epoch: 235\n",
      "      Training_acc: 0.762214983713355\n",
      "-------------------\n",
      "\n",
      "Epoch: 236\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 237\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 238\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 239\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 240\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 241\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 242\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 243\n",
      "      Training_acc: 0.762214983713355\n",
      "-------------------\n",
      "\n",
      "Epoch: 244\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 245\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 246\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 247\n",
      "      Training_acc: 0.754071661237785\n",
      "-------------------\n",
      "\n",
      "Epoch: 248\n",
      "      Training_acc: 0.749185667752443\n",
      "-------------------\n",
      "\n",
      "Epoch: 249\n",
      "      Training_acc: 0.760586319218241\n",
      "-------------------\n",
      "\n",
      "Epoch: 250\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 251\n",
      "      Training_acc: 0.749185667752443\n",
      "-------------------\n",
      "\n",
      "Epoch: 252\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 253\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 254\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 255\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 256\n",
      "      Training_acc: 0.757328990228013\n",
      "-------------------\n",
      "\n",
      "Epoch: 257\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 258\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 259\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 260\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 261\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 262\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 263\n",
      "      Training_acc: 0.760586319218241\n",
      "-------------------\n",
      "\n",
      "Epoch: 264\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 265\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 266\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 267\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 268\n",
      "      Training_acc: 0.755700325732899\n",
      "-------------------\n",
      "\n",
      "Epoch: 269\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 270\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 271\n",
      "      Training_acc: 0.760586319218241\n",
      "-------------------\n",
      "\n",
      "Epoch: 272\n",
      "      Training_acc: 0.750814332247557\n",
      "-------------------\n",
      "\n",
      "Epoch: 273\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 274\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 275\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 276\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 277\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 278\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 279\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 280\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 281\n",
      "      Training_acc: 0.762214983713355\n",
      "-------------------\n",
      "\n",
      "Epoch: 282\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 283\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 284\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 285\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 286\n",
      "      Training_acc: 0.760586319218241\n",
      "-------------------\n",
      "\n",
      "Epoch: 287\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 288\n",
      "      Training_acc: 0.755700325732899\n",
      "-------------------\n",
      "\n",
      "Epoch: 289\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 290\n",
      "      Training_acc: 0.757328990228013\n",
      "-------------------\n",
      "\n",
      "Epoch: 291\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 292\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 293\n",
      "      Training_acc: 0.754071661237785\n",
      "-------------------\n",
      "\n",
      "Epoch: 294\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 295\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 296\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 297\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 298\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 299\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 300\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 301\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 302\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 303\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 304\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 305\n",
      "      Training_acc: 0.762214983713355\n",
      "-------------------\n",
      "\n",
      "Epoch: 306\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 307\n",
      "      Training_acc: 0.762214983713355\n",
      "-------------------\n",
      "\n",
      "Epoch: 308\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 309\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 310\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 311\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 312\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 313\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 314\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 315\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 316\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 317\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 318\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 319\n",
      "      Training_acc: 0.762214983713355\n",
      "-------------------\n",
      "\n",
      "Epoch: 320\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 321\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 322\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 323\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 324\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 325\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 326\n",
      "      Training_acc: 0.752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 327\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 328\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 329\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 330\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 331\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 332\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 333\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 334\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 335\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 336\n",
      "      Training_acc: 0.7931596091205212\n",
      "-------------------\n",
      "\n",
      "Epoch: 337\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 338\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 339\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 340\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 341\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 342\n",
      "      Training_acc: 0.7931596091205212\n",
      "-------------------\n",
      "\n",
      "Epoch: 343\n",
      "      Training_acc: 0.758957654723127\n",
      "-------------------\n",
      "\n",
      "Epoch: 344\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 345\n",
      "      Training_acc: 0.7654723127035831\n",
      "-------------------\n",
      "\n",
      "Epoch: 346\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 347\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 348\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 349\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 350\n",
      "      Training_acc: 0.757328990228013\n",
      "-------------------\n",
      "\n",
      "Epoch: 351\n",
      "      Training_acc: 0.7915309446254072\n",
      "-------------------\n",
      "\n",
      "Epoch: 352\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 353\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 354\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 355\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 356\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 357\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 358\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 359\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 360\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 361\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 362\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 363\n",
      "      Training_acc: 0.7931596091205212\n",
      "-------------------\n",
      "\n",
      "Epoch: 364\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 365\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 366\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 367\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 368\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 369\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 370\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 371\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 372\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 373\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 374\n",
      "      Training_acc: 0.7882736156351792\n",
      "-------------------\n",
      "\n",
      "Epoch: 375\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 376\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 377\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 378\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 379\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 380\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 381\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 382\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 383\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 384\n",
      "      Training_acc: 0.7931596091205212\n",
      "-------------------\n",
      "\n",
      "Epoch: 385\n",
      "      Training_acc: 0.7638436482084691\n",
      "-------------------\n",
      "\n",
      "Epoch: 386\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 387\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 388\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 389\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 390\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 391\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 392\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 393\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 394\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 395\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 396\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 397\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 398\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 399\n",
      "      Training_acc: 0.7882736156351792\n",
      "-------------------\n",
      "\n",
      "Epoch: 400\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 401\n",
      "      Training_acc: 0.7703583061889251\n",
      "-------------------\n",
      "\n",
      "Epoch: 402\n",
      "      Training_acc: 0.7931596091205212\n",
      "-------------------\n",
      "\n",
      "Epoch: 403\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 404\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 405\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 406\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 407\n",
      "      Training_acc: 0.7687296416938111\n",
      "-------------------\n",
      "\n",
      "Epoch: 408\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 409\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 410\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 411\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 412\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 413\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 414\n",
      "      Training_acc: 0.8029315960912052\n",
      "-------------------\n",
      "\n",
      "Epoch: 415\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 416\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 417\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 418\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 419\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 420\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 421\n",
      "      Training_acc: 0.7996742671009772\n",
      "-------------------\n",
      "\n",
      "Epoch: 422\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 423\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 424\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 425\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 426\n",
      "      Training_acc: 0.7882736156351792\n",
      "-------------------\n",
      "\n",
      "Epoch: 427\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 428\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 429\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 430\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 431\n",
      "      Training_acc: 0.7752442996742671\n",
      "-------------------\n",
      "\n",
      "Epoch: 432\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 433\n",
      "      Training_acc: 0.7996742671009772\n",
      "-------------------\n",
      "\n",
      "Epoch: 434\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 435\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 436\n",
      "      Training_acc: 0.7931596091205212\n",
      "-------------------\n",
      "\n",
      "Epoch: 437\n",
      "      Training_acc: 0.7817589576547231\n",
      "-------------------\n",
      "\n",
      "Epoch: 438\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 439\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 440\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 441\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 442\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 443\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 444\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 445\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 446\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 447\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 448\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 449\n",
      "      Training_acc: 0.7964169381107492\n",
      "-------------------\n",
      "\n",
      "Epoch: 450\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 451\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 452\n",
      "      Training_acc: 0.7980456026058632\n",
      "-------------------\n",
      "\n",
      "Epoch: 453\n",
      "      Training_acc: 0.7980456026058632\n",
      "-------------------\n",
      "\n",
      "Epoch: 454\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 455\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 456\n",
      "      Training_acc: 0.7996742671009772\n",
      "-------------------\n",
      "\n",
      "Epoch: 457\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 458\n",
      "      Training_acc: 0.7915309446254072\n",
      "-------------------\n",
      "\n",
      "Epoch: 459\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 460\n",
      "      Training_acc: 0.7719869706840391\n",
      "-------------------\n",
      "\n",
      "Epoch: 461\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 462\n",
      "      Training_acc: 0.7996742671009772\n",
      "-------------------\n",
      "\n",
      "Epoch: 463\n",
      "      Training_acc: 0.8078175895765473\n",
      "-------------------\n",
      "\n",
      "Epoch: 464\n",
      "      Training_acc: 0.7736156351791531\n",
      "-------------------\n",
      "\n",
      "Epoch: 465\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 466\n",
      "      Training_acc: 0.7882736156351792\n",
      "-------------------\n",
      "\n",
      "Epoch: 467\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 468\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 469\n",
      "      Training_acc: 0.7671009771986971\n",
      "-------------------\n",
      "\n",
      "Epoch: 470\n",
      "      Training_acc: 0.8013029315960912\n",
      "-------------------\n",
      "\n",
      "Epoch: 471\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 472\n",
      "      Training_acc: 0.7931596091205212\n",
      "-------------------\n",
      "\n",
      "Epoch: 473\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 474\n",
      "      Training_acc: 0.7882736156351792\n",
      "-------------------\n",
      "\n",
      "Epoch: 475\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 476\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 477\n",
      "      Training_acc: 0.7964169381107492\n",
      "-------------------\n",
      "\n",
      "Epoch: 478\n",
      "      Training_acc: 0.7915309446254072\n",
      "-------------------\n",
      "\n",
      "Epoch: 479\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 480\n",
      "      Training_acc: 0.7785016286644951\n",
      "-------------------\n",
      "\n",
      "Epoch: 481\n",
      "      Training_acc: 0.7980456026058632\n",
      "-------------------\n",
      "\n",
      "Epoch: 482\n",
      "      Training_acc: 0.7866449511400652\n",
      "-------------------\n",
      "\n",
      "Epoch: 483\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 484\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 485\n",
      "      Training_acc: 0.7768729641693811\n",
      "-------------------\n",
      "\n",
      "Epoch: 486\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 487\n",
      "      Training_acc: 0.8013029315960912\n",
      "-------------------\n",
      "\n",
      "Epoch: 488\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 489\n",
      "      Training_acc: 0.7947882736156352\n",
      "-------------------\n",
      "\n",
      "Epoch: 490\n",
      "      Training_acc: 0.8078175895765473\n",
      "-------------------\n",
      "\n",
      "Epoch: 491\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 492\n",
      "      Training_acc: 0.7801302931596091\n",
      "-------------------\n",
      "\n",
      "Epoch: 493\n",
      "      Training_acc: 0.7850162866449512\n",
      "-------------------\n",
      "\n",
      "Epoch: 494\n",
      "      Training_acc: 0.7931596091205212\n",
      "-------------------\n",
      "\n",
      "Epoch: 495\n",
      "      Training_acc: 0.7882736156351792\n",
      "-------------------\n",
      "\n",
      "Epoch: 496\n",
      "      Training_acc: 0.7833876221498371\n",
      "-------------------\n",
      "\n",
      "Epoch: 497\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "Epoch: 498\n",
      "      Training_acc: 0.7964169381107492\n",
      "-------------------\n",
      "\n",
      "Epoch: 499\n",
      "      Training_acc: 0.7899022801302932\n",
      "-------------------\n",
      "\n",
      "NOTE: THESE SAVED MODELS ARE BEEING OVERWRITTEN ON NEXT RUN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import train\n",
    "import modelClass\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "model= modelClass.BinaryClassification1HL16N(inputFeatures= inputFeatures, outputFeatures= outputFeatures)\n",
    "num_epochs = 500\n",
    "device = \"cpu\" #\"cuda:0\"\n",
    "doEval = False\n",
    "lr = 0.1\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "train.train(trainloader, model, num_epochs, device, y_train,loss_function, optimizer)\n",
    "#model.predict(torch.FloatTensor([[-0.1,-0.1,-0.1,-0.1,-0.1,-0.1,-0.1,-0.1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rosario/explainable/Bachelor/copyCega.ipynb Cell 3\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(X_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlime\u001b[39;00m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlime_tabular\u001b[39;00m \n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train = np.array(X_train)\n",
    "import lime \n",
    "import lime.lime_tabular \n",
    "import lime.lime_text\n",
    "\n",
    "np.random.seed(1)\n",
    "explainer = LimeTextExplainer(class_names = ['0','1'])\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=features_names, class_names=['0', '1'])\n",
    "\n",
    "#dirPath = utils.createDirPath(seed , modelName, datasetName, num_epochs, batch_size, lr)\n",
    "\n",
    "#print(\"evaluating ...\")\n",
    "#loaderList = [trainloader,evalloader,testloader]\n",
    "#nameList = [\"train\",\"eval\", \"test\"]\n",
    "#yList = [y_train, y_eval,y_test]\n",
    "#print(explainer)\n",
    "#eval.doALLeval(model, modelsDirPath,dirPath, loaderList, device,optimizer, loss_function, num_epochs, nameList, yList, inputFeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6950587034225464\n",
      "-0.7928368449211121\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "#featureListAll  = utils.unpackingFeatureList(inputFeatures , grads)\n",
    "\n",
    "data = utils.loadData(\"./NEWtest/Results/seedNum_0_BinaryClassification0HL16N_KaggleDiabetesALL_Num_Epochs_3batchSize_4_0.1_2023-03-19_145845/\")\n",
    "#for i in data:\n",
    "#   print(i)\n",
    "gradientsPerFeature = data[\"evalGradientsPerFeature\"]\n",
    "print(np.max(gradientsPerFeature))\n",
    "print(np.min(gradientsPerFeature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of cores: 12\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import torch \n",
    "import numpy as np\n",
    "from Myhelper_func import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import shap\n",
    "def evalModel(dataloader):\n",
    "       \n",
    "\n",
    "    predsList = []\n",
    "    probsList = []\n",
    "    lableList_model = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "            for inputs,labels in dataloader:\n",
    "            \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)  \n",
    "\n",
    "                outputs = model(inputs)\n",
    "                #print(outputs)\n",
    "                __, preds = torch.max(outputs, 1)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "                predsList.extend(preds.cpu())\n",
    "                probsList.extend(probs.cpu())\n",
    "                lableList_model.extend(labels.cpu())\n",
    "\n",
    "    return predsList , probsList , lableList_model\n",
    "\n",
    "\n",
    "\n",
    "predsList_eval,probsList_eval, lableList_model_eval = evalModel(evalloader)\n",
    "\n",
    "predsList_test,probsList_test, lableList_model_test = evalModel(testloader)\n",
    "\n",
    "X_train = np.array(X_train , dtype=\"float64\")\n",
    "df_X_train = pd.DataFrame(X_train, columns=features_names.to_list())\n",
    "\n",
    "X_eval = np.array(X_eval , dtype=\"float64\")\n",
    "df_X_eval = pd.DataFrame(X_eval, columns=features_names.to_list())\n",
    "\n",
    "X_test = np.array(X_test , dtype=\"float64\")\n",
    "df_X_test = pd.DataFrame(X_test, columns=features_names.to_list())\n",
    "\n",
    "#print(df_X_train)\n",
    "#JustData_df_X_train = df_X_train[1:]\n",
    "\n",
    "#print(type(df_X_train.iloc[0][\"mean radius\"]))\n",
    "from multiprocessing import Pool, cpu_count, Queue\n",
    "\n",
    "num_cores = cpu_count()\n",
    "print(f'num of cores: {num_cores}')\n",
    "#print(len(X_train))\n",
    "if len(intervals_dict) == 0:\n",
    "    compute_intervals(intervals_dict, df_X_train, 5)\n",
    "#print(len(intervals_dict))        \n",
    "\n",
    "\n",
    "#itemset = set()\n",
    "\n",
    "p = Pool(num_cores)\n",
    "\n",
    "\n",
    "#gradientsPerFeature=  gradientsPerFeature[:,-len(predsList_eval):]\n",
    "\n",
    "gradientsPerFeaturePerEpoch = [] \n",
    "for i in range(len(range(0,len(gradientsPerFeature[0]),len(predsList_eval)))-1):\n",
    "\n",
    "    gradientsPerFeaturePerEpoch.append(gradientsPerFeature[:,i *len(predsList_eval) :(i+1) *len(predsList_eval)]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [02:40<00:00,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in gradientsPerFeaturePerEpoch:\\n    for indx in tqdm(range(len(predsList_eval))):\\n        # since we have two inputs we pass a list of inputs to the explainer\\n        #explainer = shap.GradientExplainer(model, X_train)\\n        #we explain the model's predictions on the first three samples of the test set\\n        #shap_values = explainer.shap_values([X_test, X_test])\\n        pos_queue.put(pos_label)\\n        neg_queue.put(neg_label)\\n\\n\\n        exp = [item[indx] for item in gradientsPerFeature] #normalize featureListALL ?\\n        instance_features = df_X_train.iloc[[indx]].to_dict(orient='records')[0]\\n\\n        feature_vals = [instance_features[name] for name in features_names] #put here grads# feature values ?? \\n\\n        zipped = zip(exp, feature_vals,\\n                     features_names, [shap_threshold]*len(features_names))\\n\\n        p.map(get_relevant_features, zipped) # if statisfies threshold get one hot encoded one lese zero\\n        append_to_encoded_vals(pos_queue, itemset, encoded_vals) #\\n        append_to_encoded_vals(neg_queue, itemset, encoded_vals) #\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#print(gradientsPerFeature)\n",
    "pos_label = '1'\n",
    "neg_label = '0'\n",
    "\n",
    "\n",
    "#print(itemset)\n",
    "#print(len(itemset))\n",
    "\n",
    "\n",
    "shap_threshold = 0.001\n",
    "itemset = set()\n",
    "encoded_vals = []\n",
    "summed_values = {}\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "for feature in df_X_train.columns.to_list():\n",
    "    if feature in intervals_dict:\n",
    "        intervals = intervals_dict[feature]\n",
    "        for interval in intervals:\n",
    "            if interval != interval: continue\n",
    "            left = interval.left\n",
    "            right = interval.right\n",
    "            name = f'{left}<{feature}<={right}'\n",
    "            itemset.add(name)\n",
    "    else:\n",
    "        itemset.add(feature)\n",
    "\n",
    "itemset.add(pos_label)\n",
    "itemset.add(neg_label)\n",
    "\n",
    "ohe_dfList = []\n",
    "\n",
    "\n",
    "def batch_predict(data):\n",
    "    \n",
    "\n",
    "    batch = torch.FloatTensor(data)\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    outputs = model(batch)\n",
    "\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "    return probs.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "for indx in tqdm(range(len(predsList_eval))):\n",
    "        # since we have two inputs we pass a list of inputs to the explainer\n",
    "        #explainer = shap.GradientExplainer(model, X_train)\n",
    "        #we explain the model's predictions on the first three samples of the test set\n",
    "        #shap_values = explainer.shap_values([X_test, X_test])\n",
    "        #print(X_eval[indx])\n",
    "        #print(\"joo\")\n",
    "        #print(batch_predict(X_eval, indx))\n",
    "     \n",
    "        pos_queue.put(pos_label)\n",
    "        neg_queue.put(neg_label)\n",
    "\n",
    "        ### lime \n",
    "        \"\"\"\n",
    "        expLime = explainer.explain_instance(X_eval[indx], batch_predict, num_features=num_features)\n",
    "        lime_names = [clean_name(name) for name, val in expLime.as_list()]\n",
    "        lime_vals = [val for name, val in expLime.as_list()]\n",
    "        \n",
    "        instance_features = df_X_train.iloc[[indx]].to_dict(orient='records')[0]\n",
    "        feature_vals = [instance_features[name] for name in lime_names] #put here grads# feature values ?? \n",
    "        \"\"\"\n",
    "        exp = explainer.explain_instance(X_eval[indx], batch_predict, num_features=num_features)\n",
    "        #lime_names = [clean_name(name) for name, val in exp.as_list()]\n",
    "        lime_vals = [val for name, val in exp.as_list()]\n",
    "\n",
    "        instance_features = df_X_eval.iloc[[indx]].to_dict(orient='records')[0]\n",
    "        #print(lime_names)\n",
    "        feature_vals = [instance_features[name] for name in features_names] #put here grads# feature values ?? \n",
    "\n",
    "        zipped = zip(lime_vals, feature_vals,\n",
    "                 features_names, [shap_threshold]*len(features_names))\n",
    "        ## lime end\n",
    "        #exp = [item[indx] for item in gradientsPerFeaturePerEpoch[i]] #normalize featureListALL ?\n",
    "        #instance_features = df_X_train.iloc[[indx]].to_dict(orient='records')[0]\n",
    "\n",
    "        #feature_vals = [instance_features[name] for name in features_names] #put here grads# feature values ?? \n",
    "\n",
    "        ## GRADS AS LOCAL EXPLAINATION \n",
    "        #zipped = zip(exp, feature_vals,\n",
    "        #             features_names, [shap_threshold]*len(features_names))\n",
    "\n",
    "\n",
    "        p.map(get_relevant_features, zipped) # if statisfies threshold get one hot encoded one lese zero\n",
    "        append_to_encoded_vals(pos_queue, itemset, encoded_vals) #\n",
    "        append_to_encoded_vals(neg_queue, itemset, encoded_vals) #\n",
    "\n",
    "\n",
    "ohe_df = pd.DataFrame(encoded_vals)\n",
    "ohe_dfList.append(ohe_df)\n",
    "\"\"\"\n",
    "for i in gradientsPerFeaturePerEpoch:\n",
    "    for indx in tqdm(range(len(predsList_eval))):\n",
    "        # since we have two inputs we pass a list of inputs to the explainer\n",
    "        #explainer = shap.GradientExplainer(model, X_train)\n",
    "        #we explain the model's predictions on the first three samples of the test set\n",
    "        #shap_values = explainer.shap_values([X_test, X_test])\n",
    "        pos_queue.put(pos_label)\n",
    "        neg_queue.put(neg_label)\n",
    "\n",
    "\n",
    "        exp = [item[indx] for item in gradientsPerFeature] #normalize featureListALL ?\n",
    "        instance_features = df_X_train.iloc[[indx]].to_dict(orient='records')[0]\n",
    "\n",
    "        feature_vals = [instance_features[name] for name in features_names] #put here grads# feature values ?? \n",
    "\n",
    "        zipped = zip(exp, feature_vals,\n",
    "                     features_names, [shap_threshold]*len(features_names))\n",
    "\n",
    "        p.map(get_relevant_features, zipped) # if statisfies threshold get one hot encoded one lese zero\n",
    "        append_to_encoded_vals(pos_queue, itemset, encoded_vals) #\n",
    "        append_to_encoded_vals(neg_queue, itemset, encoded_vals) #\n",
    "\n",
    "\"\"\"\n",
    "#\n",
    "# print(ohe_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.071&lt;BMI&lt;=0.329</th>\n",
       "      <th>-0.863&lt;Glucose&lt;=-0.206</th>\n",
       "      <th>-1.126&lt;Pregnancies&lt;=-0.903</th>\n",
       "      <th>1.015&lt;BloodPressure&lt;=1.601</th>\n",
       "      <th>1.001&lt;Insulin&lt;=1.713</th>\n",
       "      <th>-0.206&lt;Glucose&lt;=0.449</th>\n",
       "      <th>-0.445&lt;BMI&lt;=-0.187</th>\n",
       "      <th>-1.283&lt;DiabetesPedigreeFunction&lt;=-1.073</th>\n",
       "      <th>-0.32&lt;SkinThickness&lt;=-0.00671</th>\n",
       "      <th>...</th>\n",
       "      <th>-0.187&lt;BMI&lt;=0.071</th>\n",
       "      <th>0.945&lt;Age&lt;=1.354</th>\n",
       "      <th>1.713&lt;Insulin&lt;=2.425</th>\n",
       "      <th>-0.652&lt;DiabetesPedigreeFunction&lt;=-0.442</th>\n",
       "      <th>1</th>\n",
       "      <th>-1.073&lt;DiabetesPedigreeFunction&lt;=-0.863</th>\n",
       "      <th>-0.903&lt;Pregnancies&lt;=-0.681</th>\n",
       "      <th>-0.157&lt;BloodPressure&lt;=0.429</th>\n",
       "      <th>0.449&lt;Glucose&lt;=1.103</th>\n",
       "      <th>-0.694&lt;Age&lt;=-0.283</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  0.071<BMI<=0.329  -0.863<Glucose<=-0.206  -1.126<Pregnancies<=-0.903  \\\n",
       "0    0                 0                       0                           0   \n",
       "1    1                 0                       0                           0   \n",
       "2    0                 0                       0                           0   \n",
       "3    1                 0                       0                           0   \n",
       "4    0                 0                       0                           0   \n",
       "..  ..               ...                     ...                         ...   \n",
       "149  1                 0                       0                           0   \n",
       "150  0                 0                       0                           0   \n",
       "151  1                 0                       0                           0   \n",
       "152  0                 0                       0                           0   \n",
       "153  1                 0                       0                           1   \n",
       "\n",
       "     1.015<BloodPressure<=1.601  1.001<Insulin<=1.713  -0.206<Glucose<=0.449  \\\n",
       "0                             0                     0                      0   \n",
       "1                             0                     0                      0   \n",
       "2                             0                     0                      0   \n",
       "3                             0                     1                      0   \n",
       "4                             0                     0                      0   \n",
       "..                          ...                   ...                    ...   \n",
       "149                           0                     0                      0   \n",
       "150                           0                     0                      0   \n",
       "151                           0                     0                      0   \n",
       "152                           0                     1                      0   \n",
       "153                           0                     0                      0   \n",
       "\n",
       "     -0.445<BMI<=-0.187  -1.283<DiabetesPedigreeFunction<=-1.073  \\\n",
       "0                     0                                        0   \n",
       "1                     1                                        0   \n",
       "2                     0                                        0   \n",
       "3                     1                                        0   \n",
       "4                     0                                        0   \n",
       "..                  ...                                      ...   \n",
       "149                   0                                        0   \n",
       "150                   0                                        0   \n",
       "151                   0                                        0   \n",
       "152                   0                                        0   \n",
       "153                   1                                        0   \n",
       "\n",
       "     -0.32<SkinThickness<=-0.00671  ...  -0.187<BMI<=0.071  0.945<Age<=1.354  \\\n",
       "0                                0  ...                  0                 0   \n",
       "1                                0  ...                  0                 0   \n",
       "2                                0  ...                  0                 0   \n",
       "3                                0  ...                  0                 0   \n",
       "4                                0  ...                  0                 0   \n",
       "..                             ...  ...                ...               ...   \n",
       "149                              0  ...                  0                 0   \n",
       "150                              1  ...                  1                 0   \n",
       "151                              0  ...                  0                 0   \n",
       "152                              0  ...                  0                 0   \n",
       "153                              0  ...                  0                 0   \n",
       "\n",
       "     1.713<Insulin<=2.425  -0.652<DiabetesPedigreeFunction<=-0.442  1  \\\n",
       "0                       0                                        0  1   \n",
       "1                       1                                        0  0   \n",
       "2                       0                                        0  1   \n",
       "3                       0                                        0  0   \n",
       "4                       0                                        0  1   \n",
       "..                    ...                                      ... ..   \n",
       "149                     0                                        0  0   \n",
       "150                     0                                        0  1   \n",
       "151                     0                                        0  0   \n",
       "152                     0                                        0  1   \n",
       "153                     0                                        0  0   \n",
       "\n",
       "     -1.073<DiabetesPedigreeFunction<=-0.863  -0.903<Pregnancies<=-0.681  \\\n",
       "0                                          1                           1   \n",
       "1                                          0                           0   \n",
       "2                                          0                           1   \n",
       "3                                          0                           0   \n",
       "4                                          0                           0   \n",
       "..                                       ...                         ...   \n",
       "149                                        0                           0   \n",
       "150                                        0                           0   \n",
       "151                                        0                           1   \n",
       "152                                        1                           0   \n",
       "153                                        0                           0   \n",
       "\n",
       "     -0.157<BloodPressure<=0.429  0.449<Glucose<=1.103  -0.694<Age<=-0.283  \n",
       "0                              1                     1                   1  \n",
       "1                              0                     0                   0  \n",
       "2                              1                     0                   0  \n",
       "3                              0                     0                   1  \n",
       "4                              0                     0                   0  \n",
       "..                           ...                   ...                 ...  \n",
       "149                            0                     0                   0  \n",
       "150                            1                     0                   1  \n",
       "151                            0                     0                   0  \n",
       "152                            0                     0                   0  \n",
       "153                            0                     0                   1  \n",
       "\n",
       "[154 rows x 42 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_df\n",
    "ohe_dfList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "77\n",
      "['(0,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,)', '(0,1.001<Insulin<=1.713,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,-0.694<Age<=-0.283,)', '(0,-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,)', '(0,-0.445<BMI<=-0.187,-0.652<DiabetesPedigreeFunction<=-0.442,-0.157<BloodPressure<=0.429,)', '(0,-0.445<BMI<=-0.187,-0.283<Age<=0.126,-0.903<Pregnancies<=-0.681,)', '(0,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,1.757<Glucose<=2.411,-0.187<BMI<=0.071,)', '(0,-1.126<Pregnancies<=-0.903,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,0.289<Insulin<=1.001,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-1.283<DiabetesPedigreeFunction<=-1.073,-0.947<SkinThickness<=-0.632,1.103<Glucose<=1.757,-0.283<Age<=0.126,)', '(0,1.103<Glucose<=1.757,-0.187<BMI<=0.071,-0.903<Pregnancies<=-0.681,)', '(0,-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,-0.694<Age<=-0.283,)', '(0,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,)', '(0,-0.746<BloodPressure<=-0.157,1.757<Glucose<=2.411,-0.652<DiabetesPedigreeFunction<=-0.442,)', '(0,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,-0.863<DiabetesPedigreeFunction<=-0.652,0.429<BloodPressure<=1.015,)', '(0,-0.445<BMI<=-0.187,-0.681<Pregnancies<=-0.458,0.429<BloodPressure<=1.015,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.947<SkinThickness<=-0.632,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,-0.903<Pregnancies<=-0.681,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-0.694<Age<=-0.283,)', '(0,-1.126<Pregnancies<=-0.903,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,1.713<Insulin<=2.425,-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,)', '(0,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,-0.187<BMI<=0.071,-0.903<Pregnancies<=-0.681,)', '(0,-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,-0.694<Age<=-0.283,)', '(0,-1.126<Pregnancies<=-0.903,1.015<BloodPressure<=1.601,0.126<Age<=0.536,1.103<Glucose<=1.757,-1.494<DiabetesPedigreeFunction<=-1.283,0.289<Insulin<=1.001,)', '(0,-1.126<Pregnancies<=-0.903,1.757<Glucose<=2.411,-0.187<BMI<=0.071,)', '(0,-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,1.103<Glucose<=1.757,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.445<BMI<=-0.187,1.713<Insulin<=2.425,-0.694<Age<=-0.283,)', '(0,-0.445<BMI<=-0.187,-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,0.429<BloodPressure<=1.015,)', '(0,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,1.103<Glucose<=1.757,)', '(0,1.015<BloodPressure<=1.601,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,-0.903<Pregnancies<=-0.681,)', '(0,1.001<Insulin<=1.713,-0.632<SkinThickness<=-0.32,1.757<Glucose<=2.411,)', '(0,-1.073<DiabetesPedigreeFunction<=-0.863,0.449<Glucose<=1.103,-0.694<Age<=-0.283,)', '(0,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,)', '(0,-1.126<Pregnancies<=-0.903,1.015<BloodPressure<=1.601,1.757<Glucose<=2.411,-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,)', '(0,-1.126<Pregnancies<=-0.903,1.757<Glucose<=2.411,0.289<Insulin<=1.001,-0.187<BMI<=0.071,)', '(0,-1.283<DiabetesPedigreeFunction<=-1.073,-1.35<Pregnancies<=-1.126,0.429<BloodPressure<=1.015,)', '(0,1.001<Insulin<=1.713,1.103<Glucose<=1.757,-0.157<BloodPressure<=0.429,)', '(0,1.015<BloodPressure<=1.601,-1.283<DiabetesPedigreeFunction<=-1.073,-1.35<Pregnancies<=-1.126,0.306<SkinThickness<=0.619,)', '(0,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,1.757<Glucose<=2.411,-0.903<Pregnancies<=-0.681,)', '(0,-0.32<SkinThickness<=-0.00671,1.103<Glucose<=1.757,-0.283<Age<=0.126,-1.073<DiabetesPedigreeFunction<=-0.863,-0.903<Pregnancies<=-0.681,)', '(0,-0.704<BMI<=-0.445,1.713<Insulin<=2.425,)', '(0,-1.126<Pregnancies<=-0.903,0.126<Age<=0.536,1.601<BloodPressure<=2.188,0.449<Glucose<=1.103,)', '(0,-0.445<BMI<=-0.187,)', '(0,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,-0.903<Pregnancies<=-0.681,)', '(0,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,)', '(0,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,-0.187<BMI<=0.071,)', '(0,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,-0.694<Age<=-0.283,)', '(0,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.903<Pregnancies<=-0.681,)', '(0,-0.445<BMI<=-0.187,-0.283<Age<=0.126,-0.157<BloodPressure<=0.429,)', '(0,1.103<Glucose<=1.757,-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,)', '(0,-1.126<Pregnancies<=-0.903,-0.283<Age<=0.126,1.757<Glucose<=2.411,)', '(0,-0.632<SkinThickness<=-0.32,1.103<Glucose<=1.757,-0.694<Age<=-0.283,)', '(0,-0.746<BloodPressure<=-0.157,-0.694<Age<=-0.283,)', '(0,-0.283<Age<=0.126,1.713<Insulin<=2.425,-0.157<BloodPressure<=0.429,)', '(0,-0.283<Age<=0.126,1.757<Glucose<=2.411,-0.187<BMI<=0.071,)', '(0,-0.445<BMI<=-0.187,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,0.536<Age<=0.945,)', '(0,1.757<Glucose<=2.411,0.289<Insulin<=1.001,)', '(0,-0.632<SkinThickness<=-0.32,1.757<Glucose<=2.411,-0.704<BMI<=-0.445,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-0.694<Age<=-0.283,)', '(0,-0.445<BMI<=-0.187,0.289<Insulin<=1.001,-1.073<DiabetesPedigreeFunction<=-0.863,-0.903<Pregnancies<=-0.681,-0.694<Age<=-0.283,)', '(0,-1.283<DiabetesPedigreeFunction<=-1.073,-0.32<SkinThickness<=-0.00671,1.103<Glucose<=1.757,)', '(0,1.001<Insulin<=1.713,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,-0.694<Age<=-0.283,)', '(0,-0.32<SkinThickness<=-0.00671,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.632<SkinThickness<=-0.32,0.126<Age<=0.536,1.757<Glucose<=2.411,-0.652<DiabetesPedigreeFunction<=-0.442,)', '(0,1.015<BloodPressure<=1.601,1.103<Glucose<=1.757,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,0.536<Age<=0.945,-0.903<Pregnancies<=-0.681,)', '(0,-0.445<BMI<=-0.187,-0.746<BloodPressure<=-0.157,-0.652<DiabetesPedigreeFunction<=-0.442,)', '(0,-1.126<Pregnancies<=-0.903,-0.947<SkinThickness<=-0.632,1.103<Glucose<=1.757,-0.283<Age<=0.126,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-0.863<DiabetesPedigreeFunction<=-0.652,0.429<BloodPressure<=1.015,)', '(0,1.713<Insulin<=2.425,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.445<BMI<=-0.187,-0.863<DiabetesPedigreeFunction<=-0.652,1.713<Insulin<=2.425,-0.694<Age<=-0.283,)', '(0,-0.445<BMI<=-0.187,1.103<Glucose<=1.757,)', '(0,0.126<Age<=0.536,-0.903<Pregnancies<=-0.681,)', '(0,-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,)', '(0,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,1.757<Glucose<=2.411,)', '(0,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,)', '(0,-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,0.429<BloodPressure<=1.015,)', '(0,-0.863<DiabetesPedigreeFunction<=-0.652,-0.903<Pregnancies<=-0.681,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-0.694<Age<=-0.283,)']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for i in range(len(ohe_dfList[0][\"1\"])):\n",
    "#    df = ohe_dfList[0].iloc[[i]] == 1\n",
    "#    print(df.drop(df == True))\n",
    "#    print(\"done\")\n",
    "\n",
    "# split into pos and negative rules \n",
    "\n",
    "# drop the 0 /  1 column \n",
    "\n",
    "\n",
    "\n",
    "def ohe_df_to_decisionRules(ohe_df):\n",
    "    ohe_df.drop([\"1\",\"0\"], axis=1)\n",
    "    df_neg = ohe_df.iloc[range(1, len(ohe_df), 2)]\n",
    "    df_pos = ohe_df.iloc[range(0, len(ohe_df), 2)]\n",
    "    print(len(df_pos))\n",
    "    def createDecisionRules(df):\n",
    "        rules = []\n",
    "        for i in range(len(df)):\n",
    "            colums = df.iloc[[i]].columns.tolist() \n",
    "            list = df.iloc[[i]].values.tolist()[0]\n",
    "            tempString = \"(\" \n",
    "            for j ,item in enumerate(list):\n",
    "                if item == 1:\n",
    "                    tempString = tempString + str(colums[j]) +\",\"\n",
    "            tempString = tempString + \")\"\n",
    "            rules.append(tempString)\n",
    "        return rules\n",
    "\n",
    "    decisionRules_pos =createDecisionRules(df_pos)\n",
    "    decisionRules_neg =createDecisionRules(df_neg)    \n",
    "\n",
    "    return decisionRules_pos , decisionRules_neg\n",
    "\n",
    "limeDecisionRules_pos , limeDecisionRules_neg = ohe_df_to_decisionRules(ohe_df)\n",
    "print(len(limeDecisionRules_pos))\n",
    "print(limeDecisionRules_neg)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77,)\n",
      "(154,)\n",
      "['(1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,0.449<Glucose<=1.103,-0.694<Age<=-0.283,)', '(1.103<Glucose<=1.757,-0.863<DiabetesPedigreeFunction<=-0.652,1,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,)', '(-0.947<SkinThickness<=-0.632,1,)', '(-0.206<Glucose<=0.449,-0.632<SkinThickness<=-0.32,-0.681<Pregnancies<=-0.458,1.713<Insulin<=2.425,1,-0.694<Age<=-0.283,)', '(-0.32<SkinThickness<=-0.00671,-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,1,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(-0.32<SkinThickness<=-0.00671,-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,1,-0.903<Pregnancies<=-0.681,)', '(-0.445<BMI<=-0.187,-0.947<SkinThickness<=-0.632,-0.283<Age<=0.126,1,)', '(0.429<BloodPressure<=1.015,0.289<Insulin<=1.001,1,)', '(1.015<BloodPressure<=1.601,-0.32<SkinThickness<=-0.00671,0.126<Age<=0.536,-1.138<Insulin<=-0.423,1,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(-0.445<BMI<=-0.187,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,0.449<Glucose<=1.103,)', '(-1.126<Pregnancies<=-0.903,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.157<BloodPressure<=0.429,0.449<Glucose<=1.103,-0.694<Age<=-0.283,)', '(0.071<BMI<=0.329,-0.632<SkinThickness<=-0.32,-0.681<Pregnancies<=-0.458,0.126<Age<=0.536,-1.138<Insulin<=-0.423,1,)', '(-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,1.757<Glucose<=2.411,1,)', '(-0.32<SkinThickness<=-0.00671,-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,1,)', '(0.126<Age<=0.536,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,1,)', '(-0.32<SkinThickness<=-0.00671,1.713<Insulin<=2.425,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.157<BloodPressure<=0.429,0.449<Glucose<=1.103,)', '(1.001<Insulin<=1.713,-0.32<SkinThickness<=-0.00671,1.103<Glucose<=1.757,-0.704<BMI<=-0.445,0.429<BloodPressure<=1.015,1,-0.694<Age<=-0.283,)', '(-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,1,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,0.449<Glucose<=1.103,)', '(0.126<Age<=0.536,0.429<BloodPressure<=1.015,1,)', '(1.103<Glucose<=1.757,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.157<BloodPressure<=0.429,)', '(-0.445<BMI<=-0.187,-0.32<SkinThickness<=-0.00671,1,)', '(-1.283<DiabetesPedigreeFunction<=-1.073,-0.32<SkinThickness<=-0.00671,0.289<Insulin<=1.001,1,-0.157<BloodPressure<=0.429,-0.694<Age<=-0.283,)', '(0.071<BMI<=0.329,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,1.757<Glucose<=2.411,1,)', '(1.001<Insulin<=1.713,0.429<BloodPressure<=1.015,1,-0.694<Age<=-0.283,)', '(-1.126<Pregnancies<=-0.903,-0.32<SkinThickness<=-0.00671,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.157<BloodPressure<=0.429,0.449<Glucose<=1.103,)', '(-0.947<SkinThickness<=-0.632,-0.283<Age<=0.126,1.757<Glucose<=2.411,1,)', '(0.126<Age<=0.536,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,1,)', '(1.001<Insulin<=1.713,-0.632<SkinThickness<=-0.32,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.157<BloodPressure<=0.429,-0.694<Age<=-0.283,)', '(0.071<BMI<=0.329,-0.947<SkinThickness<=-0.632,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,1,)', '(-0.704<BMI<=-0.445,1,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,)', '(-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-0.32<SkinThickness<=-0.00671,0.429<BloodPressure<=1.015,1.713<Insulin<=2.425,1,)', '(-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,1,)', '(-0.445<BMI<=-0.187,-0.32<SkinThickness<=-0.00671,-0.423<Insulin<=0.289,1,)', '(-1.283<DiabetesPedigreeFunction<=-1.073,-0.632<SkinThickness<=-0.32,0.429<BloodPressure<=1.015,1,-0.694<Age<=-0.283,)', '(-0.445<BMI<=-0.187,-0.32<SkinThickness<=-0.00671,1.757<Glucose<=2.411,0.289<Insulin<=1.001,1,)', '(-1.126<Pregnancies<=-0.903,-0.632<SkinThickness<=-0.32,-0.283<Age<=0.126,-0.704<BMI<=-0.445,1,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(1.103<Glucose<=1.757,-0.423<Insulin<=0.289,-0.187<BMI<=0.071,1,-0.694<Age<=-0.283,)', '(0.071<BMI<=0.329,-0.863<DiabetesPedigreeFunction<=-0.652,0.429<BloodPressure<=1.015,1,)', '(1.015<BloodPressure<=1.601,-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,1,)', '(-0.632<SkinThickness<=-0.32,-0.746<BloodPressure<=-0.157,-0.863<DiabetesPedigreeFunction<=-0.652,1,-0.903<Pregnancies<=-0.681,0.449<Glucose<=1.103,)', '(0.071<BMI<=0.329,-0.32<SkinThickness<=-0.00671,-1.138<Insulin<=-0.423,1,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,0.449<Glucose<=1.103,)', '(0.071<BMI<=0.329,1.015<BloodPressure<=1.601,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,1,)', '(-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,1,)', '(1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,1,)', '(-0.32<SkinThickness<=-0.00671,-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,1,-0.157<BloodPressure<=0.429,)', '(-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,1,)', '(1.001<Insulin<=1.713,-1.283<DiabetesPedigreeFunction<=-1.073,-0.632<SkinThickness<=-0.32,1.103<Glucose<=1.757,1,-0.903<Pregnancies<=-0.681,)', '(-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,-0.445<BMI<=-0.187,-0.32<SkinThickness<=-0.00671,0.429<BloodPressure<=1.015,1,)', '(0.071<BMI<=0.329,-0.00671<SkinThickness<=0.306,-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,1,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,-0.445<BMI<=-0.187,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.157<BloodPressure<=0.429,)', '(-0.206<Glucose<=0.449,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,-0.681<Pregnancies<=-0.458,1.713<Insulin<=2.425,-0.652<DiabetesPedigreeFunction<=-0.442,1,)', '(-0.632<SkinThickness<=-0.32,1.103<Glucose<=1.757,-0.704<BMI<=-0.445,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.903<Pregnancies<=-0.681,)', '(-0.632<SkinThickness<=-0.32,-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,-0.652<DiabetesPedigreeFunction<=-0.442,1,)', '(-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,1,)', '(-0.445<BMI<=-0.187,-1.283<DiabetesPedigreeFunction<=-1.073,-0.632<SkinThickness<=-0.32,-0.283<Age<=0.126,1,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,)', '(0.126<Age<=0.536,-1.138<Insulin<=-0.423,-0.746<BloodPressure<=-0.157,-0.458<Pregnancies<=-0.235,-0.652<DiabetesPedigreeFunction<=-0.442,1,)', '(1.001<Insulin<=1.713,-1.283<DiabetesPedigreeFunction<=-1.073,-0.32<SkinThickness<=-0.00671,1.103<Glucose<=1.757,0.429<BloodPressure<=1.015,1,)', '(-0.632<SkinThickness<=-0.32,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,1,)', '(-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,-0.445<BMI<=-0.187,1,-0.157<BloodPressure<=0.429,-0.694<Age<=-0.283,)', '(-1.126<Pregnancies<=-0.903,-0.632<SkinThickness<=-0.32,1.103<Glucose<=1.757,1,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,1,)', '(-1.138<Insulin<=-0.423,-0.746<BloodPressure<=-0.157,-0.704<BMI<=-0.445,-0.458<Pregnancies<=-0.235,1,)', '(-0.445<BMI<=-0.187,-0.947<SkinThickness<=-0.632,1,)', '(-0.632<SkinThickness<=-0.32,-0.681<Pregnancies<=-0.458,1.713<Insulin<=2.425,1,0.449<Glucose<=1.103,-0.694<Age<=-0.283,)', '(1.001<Insulin<=1.713,-0.704<BMI<=-0.445,1,-0.157<BloodPressure<=0.429,)', '(0.126<Age<=0.536,-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,-0.187<BMI<=0.071,1,)', '(-0.632<SkinThickness<=-0.32,-0.704<BMI<=-0.445,1,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,0.449<Glucose<=1.103,-0.694<Age<=-0.283,)', '(-0.632<SkinThickness<=-0.32,1,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,0.449<Glucose<=1.103,)', '(-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,-0.632<SkinThickness<=-0.32,0.429<BloodPressure<=1.015,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,)', '(-0.445<BMI<=-0.187,-0.32<SkinThickness<=-0.00671,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,1,)', '(-0.445<BMI<=-0.187,1.103<Glucose<=1.757,1,-1.073<DiabetesPedigreeFunction<=-0.863,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,-0.694<Age<=-0.283,)', '(-0.681<Pregnancies<=-0.458,-0.863<DiabetesPedigreeFunction<=-0.652,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,1,)', '(-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,1,)', '(0.071<BMI<=0.329,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,1,)', '(-0.32<SkinThickness<=-0.00671,-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,-0.187<BMI<=0.071,1,-0.157<BloodPressure<=0.429,-0.694<Age<=-0.283,)', '(1.001<Insulin<=1.713,-0.632<SkinThickness<=-0.32,1.103<Glucose<=1.757,0.429<BloodPressure<=1.015,1,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,)', '(0,1.001<Insulin<=1.713,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,-0.694<Age<=-0.283,)', '(0,-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,)', '(0,-0.445<BMI<=-0.187,-0.652<DiabetesPedigreeFunction<=-0.442,-0.157<BloodPressure<=0.429,)', '(0,-0.445<BMI<=-0.187,-0.283<Age<=0.126,-0.903<Pregnancies<=-0.681,)', '(0,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,1.757<Glucose<=2.411,-0.187<BMI<=0.071,)', '(0,-1.126<Pregnancies<=-0.903,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,0.289<Insulin<=1.001,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-1.283<DiabetesPedigreeFunction<=-1.073,-0.947<SkinThickness<=-0.632,1.103<Glucose<=1.757,-0.283<Age<=0.126,)', '(0,1.103<Glucose<=1.757,-0.187<BMI<=0.071,-0.903<Pregnancies<=-0.681,)', '(0,-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,-0.694<Age<=-0.283,)', '(0,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,)', '(0,-0.746<BloodPressure<=-0.157,1.757<Glucose<=2.411,-0.652<DiabetesPedigreeFunction<=-0.442,)', '(0,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,-0.863<DiabetesPedigreeFunction<=-0.652,0.429<BloodPressure<=1.015,)', '(0,-0.445<BMI<=-0.187,-0.681<Pregnancies<=-0.458,0.429<BloodPressure<=1.015,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.947<SkinThickness<=-0.632,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,-0.903<Pregnancies<=-0.681,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-0.694<Age<=-0.283,)', '(0,-1.126<Pregnancies<=-0.903,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,1.713<Insulin<=2.425,-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,)', '(0,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,-0.187<BMI<=0.071,-0.903<Pregnancies<=-0.681,)', '(0,-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,-0.694<Age<=-0.283,)', '(0,-1.126<Pregnancies<=-0.903,1.015<BloodPressure<=1.601,0.126<Age<=0.536,1.103<Glucose<=1.757,-1.494<DiabetesPedigreeFunction<=-1.283,0.289<Insulin<=1.001,)', '(0,-1.126<Pregnancies<=-0.903,1.757<Glucose<=2.411,-0.187<BMI<=0.071,)', '(0,-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-0.632<SkinThickness<=-0.32,1.103<Glucose<=1.757,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.445<BMI<=-0.187,1.713<Insulin<=2.425,-0.694<Age<=-0.283,)', '(0,-0.445<BMI<=-0.187,-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,0.429<BloodPressure<=1.015,)', '(0,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,1.103<Glucose<=1.757,)', '(0,1.015<BloodPressure<=1.601,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,-0.903<Pregnancies<=-0.681,)', '(0,1.001<Insulin<=1.713,-0.632<SkinThickness<=-0.32,1.757<Glucose<=2.411,)', '(0,-1.073<DiabetesPedigreeFunction<=-0.863,0.449<Glucose<=1.103,-0.694<Age<=-0.283,)', '(0,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,)', '(0,-1.126<Pregnancies<=-0.903,1.015<BloodPressure<=1.601,1.757<Glucose<=2.411,-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,)', '(0,-1.126<Pregnancies<=-0.903,1.757<Glucose<=2.411,0.289<Insulin<=1.001,-0.187<BMI<=0.071,)', '(0,-1.283<DiabetesPedigreeFunction<=-1.073,-1.35<Pregnancies<=-1.126,0.429<BloodPressure<=1.015,)', '(0,1.001<Insulin<=1.713,1.103<Glucose<=1.757,-0.157<BloodPressure<=0.429,)', '(0,1.015<BloodPressure<=1.601,-1.283<DiabetesPedigreeFunction<=-1.073,-1.35<Pregnancies<=-1.126,0.306<SkinThickness<=0.619,)', '(0,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,1.757<Glucose<=2.411,-0.903<Pregnancies<=-0.681,)', '(0,-0.32<SkinThickness<=-0.00671,1.103<Glucose<=1.757,-0.283<Age<=0.126,-1.073<DiabetesPedigreeFunction<=-0.863,-0.903<Pregnancies<=-0.681,)', '(0,-0.704<BMI<=-0.445,1.713<Insulin<=2.425,)', '(0,-1.126<Pregnancies<=-0.903,0.126<Age<=0.536,1.601<BloodPressure<=2.188,0.449<Glucose<=1.103,)', '(0,-0.445<BMI<=-0.187,)', '(0,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,-0.903<Pregnancies<=-0.681,)', '(0,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,)', '(0,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,-0.187<BMI<=0.071,)', '(0,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,-0.694<Age<=-0.283,)', '(0,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.903<Pregnancies<=-0.681,)', '(0,-0.445<BMI<=-0.187,-0.283<Age<=0.126,-0.157<BloodPressure<=0.429,)', '(0,1.103<Glucose<=1.757,-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,)', '(0,-1.126<Pregnancies<=-0.903,-0.283<Age<=0.126,1.757<Glucose<=2.411,)', '(0,-0.632<SkinThickness<=-0.32,1.103<Glucose<=1.757,-0.694<Age<=-0.283,)', '(0,-0.746<BloodPressure<=-0.157,-0.694<Age<=-0.283,)', '(0,-0.283<Age<=0.126,1.713<Insulin<=2.425,-0.157<BloodPressure<=0.429,)', '(0,-0.283<Age<=0.126,1.757<Glucose<=2.411,-0.187<BMI<=0.071,)', '(0,-0.445<BMI<=-0.187,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,0.536<Age<=0.945,)', '(0,1.757<Glucose<=2.411,0.289<Insulin<=1.001,)', '(0,-0.632<SkinThickness<=-0.32,1.757<Glucose<=2.411,-0.704<BMI<=-0.445,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-0.694<Age<=-0.283,)', '(0,-0.445<BMI<=-0.187,0.289<Insulin<=1.001,-1.073<DiabetesPedigreeFunction<=-0.863,-0.903<Pregnancies<=-0.681,-0.694<Age<=-0.283,)', '(0,-1.283<DiabetesPedigreeFunction<=-1.073,-0.32<SkinThickness<=-0.00671,1.103<Glucose<=1.757,)', '(0,1.001<Insulin<=1.713,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,-0.694<Age<=-0.283,)', '(0,-0.32<SkinThickness<=-0.00671,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.632<SkinThickness<=-0.32,0.126<Age<=0.536,1.757<Glucose<=2.411,-0.652<DiabetesPedigreeFunction<=-0.442,)', '(0,1.015<BloodPressure<=1.601,1.103<Glucose<=1.757,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,0.536<Age<=0.945,-0.903<Pregnancies<=-0.681,)', '(0,-0.445<BMI<=-0.187,-0.746<BloodPressure<=-0.157,-0.652<DiabetesPedigreeFunction<=-0.442,)', '(0,-1.126<Pregnancies<=-0.903,-0.947<SkinThickness<=-0.632,1.103<Glucose<=1.757,-0.283<Age<=0.126,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.681<Pregnancies<=-0.458,-0.947<SkinThickness<=-0.632,-0.863<DiabetesPedigreeFunction<=-0.652,0.429<BloodPressure<=1.015,)', '(0,1.713<Insulin<=2.425,-1.073<DiabetesPedigreeFunction<=-0.863,)', '(0,-0.445<BMI<=-0.187,-0.863<DiabetesPedigreeFunction<=-0.652,1.713<Insulin<=2.425,-0.694<Age<=-0.283,)', '(0,-0.445<BMI<=-0.187,1.103<Glucose<=1.757,)', '(0,0.126<Age<=0.536,-0.903<Pregnancies<=-0.681,)', '(0,-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,)', '(0,-0.947<SkinThickness<=-0.632,-1.138<Insulin<=-0.423,-0.283<Age<=0.126,1.757<Glucose<=2.411,)', '(0,-0.863<DiabetesPedigreeFunction<=-0.652,1.757<Glucose<=2.411,0.429<BloodPressure<=1.015,-0.187<BMI<=0.071,)', '(0,-0.681<Pregnancies<=-0.458,-1.138<Insulin<=-0.423,-0.863<DiabetesPedigreeFunction<=-0.652,-0.283<Age<=0.126,0.429<BloodPressure<=1.015,)', '(0,-0.863<DiabetesPedigreeFunction<=-0.652,-0.903<Pregnancies<=-0.681,)', '(0,-1.126<Pregnancies<=-0.903,-0.445<BMI<=-0.187,-0.694<Age<=-0.283,)']\n"
     ]
    }
   ],
   "source": [
    "allRules = []\n",
    "\n",
    "allRules.extend(limeDecisionRules_pos)\n",
    "allRules.extend(limeDecisionRules_neg)\n",
    "print(np.shape(limeDecisionRules_pos))\n",
    "print(np.shape(allRules))\n",
    "\n",
    "import json\n",
    "\n",
    "def limeRulesToGLOCALX_Json(allRules):\n",
    "    \n",
    "    with open(\"allRules.json\", \"w\") as f:\n",
    "        json.dump(allRules, f)\n",
    "\n",
    "limeRulesToGLOCALX_Json(allRules)\n",
    "with open(\"allRules.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "#input_str = \"(0.903<Pregnancies<=-0.8104,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)\"\n",
    "\n",
    "pattern =  r\"([(-+]?\\d+\\.\\d+)<(\\w+)<=([-+]?\\d+\\.\\d+)\" \n",
    "\n",
    "#    {\"22\": [30.0, 91.9], \"23\": [-Infinity, 553.3], \"label\": 0},\n",
    "\n",
    "output_list = []\n",
    "\n",
    "ruleSets = [[limeDecisionRules_pos,1] ,[limeDecisionRules_neg,0]]\n",
    "for i in range(len(ruleSets)):\n",
    "    for rules in ruleSets[i][0]:\n",
    "        input_str  =rules\n",
    "        classLable = ruleSets[i][1]\n",
    "\n",
    "        matches = re.findall(pattern, input_str)\n",
    "        #print(matches)\n",
    "\n",
    "        #for k in split_string:\n",
    "            #k = \"0.903<Pregnancies<=-0.8104\"\n",
    "        \n",
    "        output_dict = {}\n",
    "        for match in matches:\n",
    "            #print(match)\n",
    "            lower_bound = match[0]\n",
    "\n",
    "            variable_name = match[1]\n",
    "            variable_name = 0\n",
    "            #print(variable_name)\n",
    "\n",
    "            upper_bound = match[2]\n",
    "            output_dict[variable_name] = [lower_bound, upper_bound]\n",
    "\n",
    "        output_dict[\"label\"] = classLable\n",
    "\n",
    "        output_list.append(output_dict) # \n",
    "\n",
    "with open('testJson.json', 'w') as f:\n",
    "    json.dump(output_list, f)\n",
    "#test = json.dumps(output_list)\n",
    "\n",
    "#print(allRules)\n",
    "#print(result_str)#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9848/2863577165.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import genfromtxt, float as np_float\n",
      "/tmp/ipykernel_9848/2863577165.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import genfromtxt, float as np_float\n",
      "2023-04-19 17:10:37.656613: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 17:10:38.832432: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib\n",
      "2023-04-19 17:10:38.832519: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib\n",
      "2023-04-19 17:10:38.832525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(77, 8)\n",
      "[0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]\n",
      "[[-0.8862209320068359, 0.8651766180992126, 0.21702159941196442, -0.3759712874889374, 2.0097906589508057, -0.38562464714050293, -0.8751333355903625, -0.5690387487411499], [-0.8181026577949524, 1.4360368251800537, -0.09308703243732452, -0.5676426887512207, 1.699678897857666, -0.4239577651023865, -0.8498319387435913, -0.3830932676792145], [-0.6720006465911865, 1.9586060047149658, 0.9867895245552063, -0.755777895450592, -0.755777895450592, -0.12409720569849014, -0.7532477974891663, 0.11550579965114594], [-0.5882543921470642, 0.3357672691345215, -0.14045926928520203, -0.49585220217704773, 2.3543992042541504, -0.4269059896469116, -0.5925973057746887, -0.446097195148468], [-0.7558092474937439, 1.951029658317566, 0.9724032878875732, -0.04786678031086922, -0.9015620946884155, -0.23526331782341003, -0.8934208154678345, -0.08951045572757721], [-0.8361142873764038, 1.984812617301941, 0.9400249719619751, -0.3137204051017761, -0.8361142873764038, -0.12304665893316269, -0.8155580759048462, -0.0002840700326487422], [-0.9585675001144409, 1.8166049718856812, 0.989957869052887, -0.6338132619857788, 0.4290187358856201, -0.3946760296821594, -0.9985122680664062, -0.2500128149986267], [-1.082759976387024, 1.5659536123275757, 0.925605297088623, -0.7043723464012146, 0.7218580842018127, -0.25030717253685, -1.1992741823196411, 0.023296253755688667], [-0.7542646527290344, 1.3358463048934937, 1.5349044799804688, -0.22344282269477844, -1.052851915359497, -0.004478816874325275, -1.0435625314712524, 0.2078499048948288], [-0.845526397228241, 1.017075777053833, 0.1826300024986267, -0.517708420753479, 1.9409265518188477, -0.4074423611164093, -0.8820483088493347, -0.4879067838191986], [-0.9085181951522827, 0.912915825843811, 0.25654321908950806, -0.43264803290367126, 1.9631121158599854, -0.3604470193386078, -0.9162634015083313, -0.5146946310997009], [-0.4837607145309448, 2.2530815601348877, -0.6110556721687317, -0.6110556721687317, -0.6110556721687317, 0.2800091803073883, -0.591643214225769, 0.37548041343688965], [-0.6031425595283508, 2.1706366539001465, 0.7390086054801941, -0.3883983790874481, -0.764200747013092, -0.21302397549152374, -0.749329686164856, -0.19154953956604004], [-0.6731482148170471, 1.9717707633972168, 0.8980907797813416, -0.22796383500099182, -0.9350214004516602, -0.21487018465995789, -0.9313290119171143, 0.11247128248214722], [-0.7846468687057495, 1.938226580619812, 0.8092302680015564, -0.7846468687057495, -0.7846468687057495, 0.018932854756712914, -0.7789354920387268, 0.36648663878440857], [-1.0369843244552612, 1.0585734844207764, 0.24030807614326477, -0.21871887147426605, 1.7970082759857178, -0.2087400108575821, -1.0335315465927124, -0.5979150533676147], [-1.032989740371704, 1.166680932044983, 0.5207459330558777, -0.22993537783622742, 1.6205812692642212, -0.4499024450778961, -1.068463921546936, -0.5267163515090942], [-0.8553365468978882, 0.962733805179596, 0.08866153657436371, -0.4183003902435303, 1.9941390752792358, -0.38333749771118164, -0.8828523755073547, -0.5057076215744019], [-0.6832832098007202, 2.003325939178467, 0.6978609561920166, -0.7778821587562561, -0.7778821587562561, -0.1402854472398758, -0.7737576365470886, 0.451903760433197], [-0.9249532222747803, 1.6524381637573242, 0.32078588008880615, -0.5813010931015015, 1.3087859153747559, -0.3278576135635376, -0.952509880065918, -0.4953880310058594], [-1.094321608543396, 1.5079655647277832, 1.0285968780517578, -0.13558420538902283, 0.34378448128700256, -0.2965151369571686, -1.4922661781311035, 0.1383407562971115], [-1.0884709358215332, 1.894152045249939, 0.3107842206954956, -0.05744083970785141, 0.7526542544364929, -0.10899233818054199, -1.074496865272522, -0.6281896829605103], [-0.6488099694252014, 2.1337199211120605, 0.6509243249893188, -0.7220344543457031, -0.7220344543457031, 0.1621510088443756, -0.7176775932312012, -0.136238694190979], [-1.045017957687378, 1.2959617376327515, 0.6841147541999817, -0.45977306365966797, 1.4289718866348267, -0.32410264015197754, -1.0671775341033936, -0.512977123260498], [-0.9147667288780212, 0.637382447719574, 0.19015304744243622, -0.2965377867221832, 2.1106088161468506, -0.31232237815856934, -0.9075189232826233, -0.5069987177848816], [-0.5784093141555786, 2.098081111907959, 0.776992917060852, -0.7328222393989563, -0.7328222393989563, -0.2112497240304947, -0.7276408076286316, 0.10787029564380646], [-0.6752997040748596, 1.928307294845581, 0.8562338352203369, -0.8065739870071411, -0.8065739870071411, -0.06706209480762482, -0.8039266467094421, 0.37489473819732666], [-0.9958245158195496, 1.3168530464172363, 0.42736169695854187, -0.4843669533729553, 1.58370041847229, -0.37762799859046936, -0.9857288002967834, -0.4843669533729553], [-0.6906447410583496, 1.7730525732040405, 1.2317856550216675, -0.783966600894928, -0.783966600894928, 0.1268547922372818, -0.7797297835350037, -0.09338480979204178], [-0.878109872341156, 1.874415397644043, 0.15207499265670776, -0.3308241665363312, 1.0856800079345703, -0.48052290081977844, -0.9953094720840454, -0.4274040162563324], [-1.0066859722137451, 0.950436532497406, 0.48784393072128296, -0.29500508308410645, 1.8044536113739014, -0.3590563237667084, -1.0378931760787964, -0.5440933704376221], [-0.61504065990448, 2.16355037689209, 0.7201264500617981, -0.6872118711471558, -0.6872118711471558, -0.09360376000404358, -0.6727234721183777, -0.127885103225708], [-1.0777075290679932, 1.7713452577590942, 1.098340630531311, -0.1803680807352066, 0.20100119709968567, -0.3822694718837738, -1.0705064535140991, -0.35983598232269287], [-1.1053599119186401, 1.8052208423614502, 0.9353921413421631, -0.3358960449695587, 0.333202987909317, -0.13851183652877808, -1.1246968507766724, -0.36935099959373474], [-1.147592544555664, 1.7892502546310425, 0.6596953272819519, -0.10275425761938095, 0.6596953272819519, -0.19876646995544434, -1.1331907510757446, -0.5263373255729675], [-0.9849770665168762, 1.679744839668274, 0.22134974598884583, -0.4808405041694641, 1.283637523651123, -0.47543904185295105, -0.996698260307312, -0.24677707254886627], [-1.2479355335235596, 1.1484676599502563, 1.3881078958511353, 0.42954668402671814, 0.25837501883506775, -0.14559006690979004, -1.2677229642868042, -0.5632489323616028], [-0.6873106360435486, 1.8897370100021362, 1.0118416547775269, -0.8005874752998352, -0.8005874752998352, 0.10562711209058762, -0.7960280776023865, 0.07730790227651596], [-0.8019004464149475, 1.6628313064575195, 1.3239306211471558, -0.2781449854373932, -0.9559462070465088, -0.06248094514012337, -0.9182358384132385, 0.029946494847536087], [-0.7426145672798157, 0.9603205323219299, -0.16550877690315247, -0.43987053632736206, 2.067228317260742, -0.4521695077419281, -0.8064367771148682, -0.4209490418434143], [-0.9510974884033203, 0.9622892737388611, 1.6550672054290771, -0.12636183202266693, -1.0500657558441162, 0.08477054536342621, -1.0420494079589844, 0.46744784712791443], [-0.7791593670845032, 0.6618200540542603, 0.2931974530220032, -0.5948480367660522, 2.069288492202759, -0.4004833698272705, -1.0068341493606567, -0.24298095703125], [-0.6983715295791626, 1.9086560010910034, 1.0546298027038574, -0.7433202862739563, -0.7433202862739563, 0.22083048522472382, -0.727745532989502, -0.2713584005832672], [-0.5848408937454224, 2.2127251625061035, 0.5031014680862427, -0.7091771364212036, -0.7091771364212036, 0.03528629243373871, -0.7070478796958923, -0.04086969420313835], [-0.5898943543434143, 2.0772593021392822, 0.7756883502006531, -0.7605921626091003, -0.7605921626091003, -0.05859728902578354, -0.7548311352729797, 0.07155977934598923], [-0.646771252155304, 2.304208278656006, 0.3938373625278473, -0.21189002692699432, -0.6933656334877014, -0.1513172686100006, -0.6896225214004517, -0.3050788640975952], [-0.6913343071937561, 2.1268014907836914, 0.7453622817993164, -0.6913343071937561, -0.6913343071937561, 0.0021481169387698174, -0.6891793012619019, -0.11112989485263824], [-0.89175945520401, 1.4215595722198486, 0.255792498588562, -0.6185328364372253, 1.5308502912521362, -0.43456020951271057, -1.0819798707962036, -0.18137015402317047], [-1.0641909837722778, 1.2963840961456299, 0.5794687271118164, -0.25984686613082886, 1.4712414741516113, -0.3542698919773102, -1.059225082397461, -0.6095616817474365], [-0.9504030346870422, 1.7699122428894043, 1.0048235654830933, 0.15472504496574402, -0.9787396192550659, 0.10088542103767395, -0.972562313079834, -0.12864112854003906], [-0.9448752403259277, 1.4448713064193726, 0.324016809463501, -0.45846661925315857, 1.508315920829773, -0.3273477554321289, -1.0246039628982544, -0.5219112038612366], [-0.5973531007766724, 0.23008310794830322, -0.16157004237174988, -0.38221970200538635, 2.3814172744750977, -0.41145575046539307, -0.6215197443962097, -0.43738213181495667], [-0.8702309131622314, 1.1445077657699585, -0.07925944030284882, -0.48220717906951904, 1.8608592748641968, -0.5448879599571228, -0.9644461274147034, -0.06433545053005219], [-0.6037174463272095, 2.2679481506347656, 0.5258044004440308, -0.6228618621826172, -0.6228618621826172, -0.12319203466176987, -0.6194350123405457, -0.20168423652648926], [-0.6275558471679688, 1.8535621166229248, 0.8992859721183777, -0.818411111831665, -0.818411111831665, -0.25538814067840576, -0.7983713150024414, 0.565289318561554], [-0.8436493277549744, 1.8956775665283203, 0.18828889727592468, -0.3933490216732025, 0.9763144850730896, -0.3952252268791199, -1.1472830772399902, -0.28077393770217896], [-0.38783517479896545, 2.398678779602051, -0.4557989239692688, -0.4557989239692688, -0.4557989239692688, -0.4557989239692688, -0.4454684257507324, 0.2578204870223999], [-1.0576465129852295, 1.1089808940887451, 0.43929606676101685, -0.11220909655094147, 1.6604862213134766, -0.32099318504333496, -1.093592882156372, -0.6243210434913635], [-0.8778314590454102, 2.1300013065338135, 0.4731442928314209, -0.5719501376152039, 0.3711838722229004, -0.3348921835422516, -0.8980960845947266, -0.29155898094177246], [-1.04866361618042, 1.4851242303848267, 0.21823027729988098, -0.17637601494789124, 1.40204918384552, -0.394447922706604, -1.1849274635314941, -0.3009885251522064], [-0.922896146774292, 1.3710097074508667, 0.15878304839134216, -0.5872026085853577, 1.6507543325424194, -0.3298375606536865, -0.958554208278656, -0.3820565640926361], [-0.66171795129776, 2.0203137397766113, 0.867878258228302, -0.2216971069574356, -0.9131584167480469, -0.12950223684310913, -0.9080458283424377, -0.0540701299905777], [-0.3935167193412781, 2.3672642707824707, -0.4702050983905792, -0.4702050983905792, -0.4702050983905792, -0.4702050983905792, -0.46629399061203003, 0.3733668923377991], [-0.697712242603302, 1.6751691102981567, 1.1039198637008667, -0.8295390009880066, -0.8295390009880066, -0.218741774559021, -0.824112057685852, 0.6205551624298096], [-0.6321198344230652, 0.4738045632839203, -0.2428344488143921, -0.4640193283557892, 2.3140628337860107, -0.34546419978141785, -0.6482574343681335, -0.4551719129085541], [-0.9639230370521545, 1.4784067869186401, 0.36825689673423767, -0.6813393831253052, 1.4178532361984253, -0.4613278806209564, -1.021570086479187, -0.13635669648647308], [-0.6753479242324829, 1.9539620876312256, 0.9278898239135742, -0.7822304368019104, -0.7822304368019104, -0.04260345548391342, -0.7791522145271301, 0.1797122210264206], [-0.8182935118675232, 0.7322834730148315, 0.058800533413887024, -0.3640840947628021, 2.1262364387512207, -0.5050456523895264, -0.8658131957054138, -0.3640840947628021], [-0.8580658435821533, 0.9891661405563354, -0.00721959350630641, -0.37666597962379456, 1.9967472553253174, -0.321808785200119, -0.8551661968231201, -0.5669868588447571], [-0.9861690998077393, 1.3679614067077637, 0.5119138956069946, -0.4025003910064697, 1.4846950769424438, -0.3986092507839203, -1.0386019945144653, -0.5386897325515747], [-0.7521339058876038, 2.1225180625915527, 0.5839719176292419, -0.16505712270736694, -0.8735980987548828, -0.19339878857135773, -0.860905110836029, 0.13860328495502472], [-0.8704261183738708, 1.1519955396652222, 0.1738308072090149, -0.3945622146129608, 1.8393546342849731, -0.4157117009162903, -0.9312971234321594, -0.5531835556030273], [-0.6264846920967102, 2.092083215713501, 0.820495069026947, -0.7141804099082947, -0.7141804099082947, 0.0378105603158474, -0.7075374722480774, -0.1880059689283371], [-0.6352759599685669, 1.9689069986343384, 0.9561691880226135, -0.7799527645111084, -0.7799527645111084, 0.05090560019016266, -0.7655677795410156, -0.015232393518090248], [-0.6757904291152954, 1.9876073598861694, 0.8661767244338989, -0.7692430019378662, -0.7692430019378662, 0.2704166769981384, -0.7481227517127991, -0.16180139780044556], [-0.7087281346321106, 2.266737699508667, 0.4183422029018402, -0.07756874710321426, -0.7237557768821716, -0.07907148450613022, -0.7028072476387024, -0.3931484520435333], [-1.0100905895233154, 1.262033462524414, 0.5046587586402893, -0.44205960631370544, 1.577606201171875, -0.37052974104881287, -1.0374822616577148, -0.4841359555721283]]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]\n",
      "1\n",
      "[{{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['0.157', '0.429']}\n",
      "1}, {{0: ['0.947', '-0.632']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['1.073', '-0.863']}\n",
      "1}, {{0: ['0.903', '-0.681']}\n",
      "1}, {{0: ['0.283', '0.126']}\n",
      "1}, {{0: ['0.289', '1.001']}\n",
      "1}, {{0: ['1.073', '-0.863']}\n",
      "1}, {{0: ['0.449', '1.103']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['1.138', '-0.423']}\n",
      "1}, {{0: ['1.757', '2.411']}\n",
      "1}, {{0: ['1.757', '2.411']}\n",
      "1}, {{0: ['1.757', '2.411']}\n",
      "1}, {{0: ['0.449', '1.103']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['0.449', '1.103']}\n",
      "1}, {{0: ['0.429', '1.015']}\n",
      "1}, {{0: ['0.157', '0.429']}\n",
      "1}, {{0: ['0.32', '-0.00671']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['1.757', '2.411']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['0.449', '1.103']}\n",
      "1}, {{0: ['1.757', '2.411']}\n",
      "1}, {{0: ['1.757', '2.411']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['1.757', '2.411']}\n",
      "1}, {{0: ['0.157', '0.429']}\n",
      "1}, {{0: ['1.713', '2.425']}\n",
      "1}, {{0: ['0.187', '0.071']}\n",
      "1}, {{0: ['0.423', '0.289']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['0.289', '1.001']}\n",
      "1}, {{0: ['1.073', '-0.863']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['0.429', '1.015']}\n",
      "1}, {{0: ['0.187', '0.071']}\n",
      "1}, {{0: ['0.449', '1.103']}\n",
      "1}, {{0: ['1.073', '-0.863']}\n",
      "1}, {{0: ['0.449', '1.103']}\n",
      "1}, {{0: ['0.283', '0.126']}\n",
      "1}, {{0: ['0.187', '0.071']}\n",
      "1}, {{0: ['0.429', '1.015']}\n",
      "1}, {{0: ['0.157', '0.429']}\n",
      "1}, {{0: ['0.187', '0.071']}\n",
      "1}, {{0: ['0.903', '-0.681']}\n",
      "1}, {{0: ['0.429', '1.015']}\n",
      "1}, {{0: ['1.073', '-0.863']}\n",
      "1}, {{0: ['0.157', '0.429']}\n",
      "1}, {{0: ['0.652', '-0.442']}\n",
      "1}, {{0: ['0.903', '-0.681']}\n",
      "1}, {{0: ['0.652', '-0.442']}\n",
      "1}, {{0: ['0.429', '1.015']}\n",
      "1}, {{0: ['0.157', '0.429']}\n",
      "1}, {{0: ['0.652', '-0.442']}\n",
      "1}, {{0: ['0.429', '1.015']}\n",
      "1}, {{0: ['0.429', '1.015']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['1.073', '-0.863']}\n",
      "1}, {{0: ['0.187', '0.071']}\n",
      "1}, {{0: ['0.458', '-0.235']}\n",
      "1}, {{0: ['0.947', '-0.632']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['0.157', '0.429']}\n",
      "1}, {{0: ['0.187', '0.071']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['0.449', '1.103']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['0.429', '1.015']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['0.187', '0.071']}\n",
      "1}, {{0: ['1.138', '-0.423']}\n",
      "1}, {{0: ['1.757', '2.411']}\n",
      "1}, {{0: ['0.694', '-0.283']}\n",
      "1}, {{0: ['1.073', '-0.863']}\n",
      "1}, {{0: ['1.713', '2.425']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.187', '0.071']}\n",
      "0}, {{0: ['0.157', '0.429']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['0.187', '0.071']}\n",
      "0}, {{0: ['1.073', '-0.863']}\n",
      "0}, {{0: ['0.283', '0.126']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['1.713', '2.425']}\n",
      "0}, {{0: ['0.652', '-0.442']}\n",
      "0}, {{0: ['0.429', '1.015']}\n",
      "0}, {{0: ['1.073', '-0.863']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['1.073', '-0.863']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.289', '1.001']}\n",
      "0}, {{0: ['0.187', '0.071']}\n",
      "0}, {{0: ['0.429', '1.015']}\n",
      "0}, {{0: ['1.073', '-0.863']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.429', '1.015']}\n",
      "0}, {{0: ['0.187', '0.071']}\n",
      "0}, {{0: ['1.103', '1.757']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['1.757', '2.411']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.283', '0.126']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.187', '0.071']}\n",
      "0}, {{0: ['0.429', '1.015']}\n",
      "0}, {{0: ['0.157', '0.429']}\n",
      "0}, {{0: ['0.306', '0.619']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['1.713', '2.425']}\n",
      "0}, {{0: ['0.449', '1.103']}\n",
      "0}, {{0: ['0.445', '-0.187']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['1.757', '2.411']}\n",
      "0}, {{0: ['0.187', '0.071']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['0.157', '0.429']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['1.757', '2.411']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.157', '0.429']}\n",
      "0}, {{0: ['0.187', '0.071']}\n",
      "0}, {{0: ['0.536', '0.945']}\n",
      "0}, {{0: ['0.289', '1.001']}\n",
      "0}, {{0: ['0.704', '-0.445']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['1.103', '1.757']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['1.073', '-0.863']}\n",
      "0}, {{0: ['0.652', '-0.442']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['0.652', '-0.442']}\n",
      "0}, {{0: ['1.073', '-0.863']}\n",
      "0}, {{0: ['0.429', '1.015']}\n",
      "0}, {{0: ['1.073', '-0.863']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}, {{0: ['1.103', '1.757']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['1.713', '2.425']}\n",
      "0}, {{0: ['1.757', '2.411']}\n",
      "0}, {{0: ['0.187', '0.071']}\n",
      "0}, {{0: ['0.429', '1.015']}\n",
      "0}, {{0: ['0.903', '-0.681']}\n",
      "0}, {{0: ['0.694', '-0.283']}\n",
      "0}]\n",
      "2\n",
      "154\n",
      "(0, 1)\n",
      "here\n",
      "0\n",
      "1\n",
      "begin\n",
      "0\n",
      "1\n",
      "[[-0.8862209320068359, 0.8651766180992126, 0.21702159941196442, -0.3759712874889374, 2.0097906589508057, -0.38562464714050293, -0.8751333355903625, -0.5690387487411499], [-0.8181026577949524, 1.4360368251800537, -0.09308703243732452, -0.5676426887512207, 1.699678897857666, -0.4239577651023865, -0.8498319387435913, -0.3830932676792145], [-0.6720006465911865, 1.9586060047149658, 0.9867895245552063, -0.755777895450592, -0.755777895450592, -0.12409720569849014, -0.7532477974891663, 0.11550579965114594], [-0.5882543921470642, 0.3357672691345215, -0.14045926928520203, -0.49585220217704773, 2.3543992042541504, -0.4269059896469116, -0.5925973057746887, -0.446097195148468], [-0.7558092474937439, 1.951029658317566, 0.9724032878875732, -0.04786678031086922, -0.9015620946884155, -0.23526331782341003, -0.8934208154678345, -0.08951045572757721], [-0.8361142873764038, 1.984812617301941, 0.9400249719619751, -0.3137204051017761, -0.8361142873764038, -0.12304665893316269, -0.8155580759048462, -0.0002840700326487422], [-0.9585675001144409, 1.8166049718856812, 0.989957869052887, -0.6338132619857788, 0.4290187358856201, -0.3946760296821594, -0.9985122680664062, -0.2500128149986267], [-1.082759976387024, 1.5659536123275757, 0.925605297088623, -0.7043723464012146, 0.7218580842018127, -0.25030717253685, -1.1992741823196411, 0.023296253755688667], [-0.7542646527290344, 1.3358463048934937, 1.5349044799804688, -0.22344282269477844, -1.052851915359497, -0.004478816874325275, -1.0435625314712524, 0.2078499048948288], [-0.845526397228241, 1.017075777053833, 0.1826300024986267, -0.517708420753479, 1.9409265518188477, -0.4074423611164093, -0.8820483088493347, -0.4879067838191986], [-0.9085181951522827, 0.912915825843811, 0.25654321908950806, -0.43264803290367126, 1.9631121158599854, -0.3604470193386078, -0.9162634015083313, -0.5146946310997009], [-0.4837607145309448, 2.2530815601348877, -0.6110556721687317, -0.6110556721687317, -0.6110556721687317, 0.2800091803073883, -0.591643214225769, 0.37548041343688965], [-0.6031425595283508, 2.1706366539001465, 0.7390086054801941, -0.3883983790874481, -0.764200747013092, -0.21302397549152374, -0.749329686164856, -0.19154953956604004], [-0.6731482148170471, 1.9717707633972168, 0.8980907797813416, -0.22796383500099182, -0.9350214004516602, -0.21487018465995789, -0.9313290119171143, 0.11247128248214722], [-0.7846468687057495, 1.938226580619812, 0.8092302680015564, -0.7846468687057495, -0.7846468687057495, 0.018932854756712914, -0.7789354920387268, 0.36648663878440857], [-1.0369843244552612, 1.0585734844207764, 0.24030807614326477, -0.21871887147426605, 1.7970082759857178, -0.2087400108575821, -1.0335315465927124, -0.5979150533676147], [-1.032989740371704, 1.166680932044983, 0.5207459330558777, -0.22993537783622742, 1.6205812692642212, -0.4499024450778961, -1.068463921546936, -0.5267163515090942], [-0.8553365468978882, 0.962733805179596, 0.08866153657436371, -0.4183003902435303, 1.9941390752792358, -0.38333749771118164, -0.8828523755073547, -0.5057076215744019], [-0.6832832098007202, 2.003325939178467, 0.6978609561920166, -0.7778821587562561, -0.7778821587562561, -0.1402854472398758, -0.7737576365470886, 0.451903760433197], [-0.9249532222747803, 1.6524381637573242, 0.32078588008880615, -0.5813010931015015, 1.3087859153747559, -0.3278576135635376, -0.952509880065918, -0.4953880310058594], [-1.094321608543396, 1.5079655647277832, 1.0285968780517578, -0.13558420538902283, 0.34378448128700256, -0.2965151369571686, -1.4922661781311035, 0.1383407562971115], [-1.0884709358215332, 1.894152045249939, 0.3107842206954956, -0.05744083970785141, 0.7526542544364929, -0.10899233818054199, -1.074496865272522, -0.6281896829605103], [-0.6488099694252014, 2.1337199211120605, 0.6509243249893188, -0.7220344543457031, -0.7220344543457031, 0.1621510088443756, -0.7176775932312012, -0.136238694190979], [-1.045017957687378, 1.2959617376327515, 0.6841147541999817, -0.45977306365966797, 1.4289718866348267, -0.32410264015197754, -1.0671775341033936, -0.512977123260498], [-0.9147667288780212, 0.637382447719574, 0.19015304744243622, -0.2965377867221832, 2.1106088161468506, -0.31232237815856934, -0.9075189232826233, -0.5069987177848816], [-0.5784093141555786, 2.098081111907959, 0.776992917060852, -0.7328222393989563, -0.7328222393989563, -0.2112497240304947, -0.7276408076286316, 0.10787029564380646], [-0.6752997040748596, 1.928307294845581, 0.8562338352203369, -0.8065739870071411, -0.8065739870071411, -0.06706209480762482, -0.8039266467094421, 0.37489473819732666], [-0.9958245158195496, 1.3168530464172363, 0.42736169695854187, -0.4843669533729553, 1.58370041847229, -0.37762799859046936, -0.9857288002967834, -0.4843669533729553], [-0.6906447410583496, 1.7730525732040405, 1.2317856550216675, -0.783966600894928, -0.783966600894928, 0.1268547922372818, -0.7797297835350037, -0.09338480979204178], [-0.878109872341156, 1.874415397644043, 0.15207499265670776, -0.3308241665363312, 1.0856800079345703, -0.48052290081977844, -0.9953094720840454, -0.4274040162563324], [-1.0066859722137451, 0.950436532497406, 0.48784393072128296, -0.29500508308410645, 1.8044536113739014, -0.3590563237667084, -1.0378931760787964, -0.5440933704376221], [-0.61504065990448, 2.16355037689209, 0.7201264500617981, -0.6872118711471558, -0.6872118711471558, -0.09360376000404358, -0.6727234721183777, -0.127885103225708], [-1.0777075290679932, 1.7713452577590942, 1.098340630531311, -0.1803680807352066, 0.20100119709968567, -0.3822694718837738, -1.0705064535140991, -0.35983598232269287], [-1.1053599119186401, 1.8052208423614502, 0.9353921413421631, -0.3358960449695587, 0.333202987909317, -0.13851183652877808, -1.1246968507766724, -0.36935099959373474], [-1.147592544555664, 1.7892502546310425, 0.6596953272819519, -0.10275425761938095, 0.6596953272819519, -0.19876646995544434, -1.1331907510757446, -0.5263373255729675], [-0.9849770665168762, 1.679744839668274, 0.22134974598884583, -0.4808405041694641, 1.283637523651123, -0.47543904185295105, -0.996698260307312, -0.24677707254886627], [-1.2479355335235596, 1.1484676599502563, 1.3881078958511353, 0.42954668402671814, 0.25837501883506775, -0.14559006690979004, -1.2677229642868042, -0.5632489323616028], [-0.6873106360435486, 1.8897370100021362, 1.0118416547775269, -0.8005874752998352, -0.8005874752998352, 0.10562711209058762, -0.7960280776023865, 0.07730790227651596], [-0.8019004464149475, 1.6628313064575195, 1.3239306211471558, -0.2781449854373932, -0.9559462070465088, -0.06248094514012337, -0.9182358384132385, 0.029946494847536087], [-0.7426145672798157, 0.9603205323219299, -0.16550877690315247, -0.43987053632736206, 2.067228317260742, -0.4521695077419281, -0.8064367771148682, -0.4209490418434143], [-0.9510974884033203, 0.9622892737388611, 1.6550672054290771, -0.12636183202266693, -1.0500657558441162, 0.08477054536342621, -1.0420494079589844, 0.46744784712791443], [-0.7791593670845032, 0.6618200540542603, 0.2931974530220032, -0.5948480367660522, 2.069288492202759, -0.4004833698272705, -1.0068341493606567, -0.24298095703125], [-0.6983715295791626, 1.9086560010910034, 1.0546298027038574, -0.7433202862739563, -0.7433202862739563, 0.22083048522472382, -0.727745532989502, -0.2713584005832672], [-0.5848408937454224, 2.2127251625061035, 0.5031014680862427, -0.7091771364212036, -0.7091771364212036, 0.03528629243373871, -0.7070478796958923, -0.04086969420313835], [-0.5898943543434143, 2.0772593021392822, 0.7756883502006531, -0.7605921626091003, -0.7605921626091003, -0.05859728902578354, -0.7548311352729797, 0.07155977934598923], [-0.646771252155304, 2.304208278656006, 0.3938373625278473, -0.21189002692699432, -0.6933656334877014, -0.1513172686100006, -0.6896225214004517, -0.3050788640975952], [-0.6913343071937561, 2.1268014907836914, 0.7453622817993164, -0.6913343071937561, -0.6913343071937561, 0.0021481169387698174, -0.6891793012619019, -0.11112989485263824], [-0.89175945520401, 1.4215595722198486, 0.255792498588562, -0.6185328364372253, 1.5308502912521362, -0.43456020951271057, -1.0819798707962036, -0.18137015402317047], [-1.0641909837722778, 1.2963840961456299, 0.5794687271118164, -0.25984686613082886, 1.4712414741516113, -0.3542698919773102, -1.059225082397461, -0.6095616817474365], [-0.9504030346870422, 1.7699122428894043, 1.0048235654830933, 0.15472504496574402, -0.9787396192550659, 0.10088542103767395, -0.972562313079834, -0.12864112854003906], [-0.9448752403259277, 1.4448713064193726, 0.324016809463501, -0.45846661925315857, 1.508315920829773, -0.3273477554321289, -1.0246039628982544, -0.5219112038612366], [-0.5973531007766724, 0.23008310794830322, -0.16157004237174988, -0.38221970200538635, 2.3814172744750977, -0.41145575046539307, -0.6215197443962097, -0.43738213181495667], [-0.8702309131622314, 1.1445077657699585, -0.07925944030284882, -0.48220717906951904, 1.8608592748641968, -0.5448879599571228, -0.9644461274147034, -0.06433545053005219], [-0.6037174463272095, 2.2679481506347656, 0.5258044004440308, -0.6228618621826172, -0.6228618621826172, -0.12319203466176987, -0.6194350123405457, -0.20168423652648926], [-0.6275558471679688, 1.8535621166229248, 0.8992859721183777, -0.818411111831665, -0.818411111831665, -0.25538814067840576, -0.7983713150024414, 0.565289318561554], [-0.8436493277549744, 1.8956775665283203, 0.18828889727592468, -0.3933490216732025, 0.9763144850730896, -0.3952252268791199, -1.1472830772399902, -0.28077393770217896], [-0.38783517479896545, 2.398678779602051, -0.4557989239692688, -0.4557989239692688, -0.4557989239692688, -0.4557989239692688, -0.4454684257507324, 0.2578204870223999], [-1.0576465129852295, 1.1089808940887451, 0.43929606676101685, -0.11220909655094147, 1.6604862213134766, -0.32099318504333496, -1.093592882156372, -0.6243210434913635], [-0.8778314590454102, 2.1300013065338135, 0.4731442928314209, -0.5719501376152039, 0.3711838722229004, -0.3348921835422516, -0.8980960845947266, -0.29155898094177246], [-1.04866361618042, 1.4851242303848267, 0.21823027729988098, -0.17637601494789124, 1.40204918384552, -0.394447922706604, -1.1849274635314941, -0.3009885251522064], [-0.922896146774292, 1.3710097074508667, 0.15878304839134216, -0.5872026085853577, 1.6507543325424194, -0.3298375606536865, -0.958554208278656, -0.3820565640926361], [-0.66171795129776, 2.0203137397766113, 0.867878258228302, -0.2216971069574356, -0.9131584167480469, -0.12950223684310913, -0.9080458283424377, -0.0540701299905777], [-0.3935167193412781, 2.3672642707824707, -0.4702050983905792, -0.4702050983905792, -0.4702050983905792, -0.4702050983905792, -0.46629399061203003, 0.3733668923377991], [-0.697712242603302, 1.6751691102981567, 1.1039198637008667, -0.8295390009880066, -0.8295390009880066, -0.218741774559021, -0.824112057685852, 0.6205551624298096], [-0.6321198344230652, 0.4738045632839203, -0.2428344488143921, -0.4640193283557892, 2.3140628337860107, -0.34546419978141785, -0.6482574343681335, -0.4551719129085541], [-0.9639230370521545, 1.4784067869186401, 0.36825689673423767, -0.6813393831253052, 1.4178532361984253, -0.4613278806209564, -1.021570086479187, -0.13635669648647308], [-0.6753479242324829, 1.9539620876312256, 0.9278898239135742, -0.7822304368019104, -0.7822304368019104, -0.04260345548391342, -0.7791522145271301, 0.1797122210264206], [-0.8182935118675232, 0.7322834730148315, 0.058800533413887024, -0.3640840947628021, 2.1262364387512207, -0.5050456523895264, -0.8658131957054138, -0.3640840947628021], [-0.8580658435821533, 0.9891661405563354, -0.00721959350630641, -0.37666597962379456, 1.9967472553253174, -0.321808785200119, -0.8551661968231201, -0.5669868588447571], [-0.9861690998077393, 1.3679614067077637, 0.5119138956069946, -0.4025003910064697, 1.4846950769424438, -0.3986092507839203, -1.0386019945144653, -0.5386897325515747], [-0.7521339058876038, 2.1225180625915527, 0.5839719176292419, -0.16505712270736694, -0.8735980987548828, -0.19339878857135773, -0.860905110836029, 0.13860328495502472], [-0.8704261183738708, 1.1519955396652222, 0.1738308072090149, -0.3945622146129608, 1.8393546342849731, -0.4157117009162903, -0.9312971234321594, -0.5531835556030273], [-0.6264846920967102, 2.092083215713501, 0.820495069026947, -0.7141804099082947, -0.7141804099082947, 0.0378105603158474, -0.7075374722480774, -0.1880059689283371], [-0.6352759599685669, 1.9689069986343384, 0.9561691880226135, -0.7799527645111084, -0.7799527645111084, 0.05090560019016266, -0.7655677795410156, -0.015232393518090248], [-0.6757904291152954, 1.9876073598861694, 0.8661767244338989, -0.7692430019378662, -0.7692430019378662, 0.2704166769981384, -0.7481227517127991, -0.16180139780044556], [-0.7087281346321106, 2.266737699508667, 0.4183422029018402, -0.07756874710321426, -0.7237557768821716, -0.07907148450613022, -0.7028072476387024, -0.3931484520435333], [-1.0100905895233154, 1.262033462524414, 0.5046587586402893, -0.44205960631370544, 1.577606201171875, -0.37052974104881287, -1.0374822616577148, -0.4841359555721283]]\n",
      "[]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rosario/explainable/Bachelor/copyCega.ipynb Cell 11\u001b[0m in \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m glocalx \u001b[39m=\u001b[39m GLocalX(oracle\u001b[39m=\u001b[39mblack_box)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# Fit the model, use batch_size=128 for larger datasets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X13sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# need this format {\"1\": [32, Infinity], \"2\": [0.5, Infinity], \"label\": 1}\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m#glocalx = glocalx.fit(local_explanations, tr_set, batch_size=2, name='black_box_explanations')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m glocalx \u001b[39m=\u001b[39m glocalx\u001b[39m.\u001b[39;49mfit(local_explanations, tr_set, batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mblack_box_explanations\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Retrieve global explanations by fidelity\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n",
      "File \u001b[0;32m~/explainable/Bachelor/otherCode/glocalx/glocalx.py:418\u001b[0m, in \u001b[0;36mGLocalX.fit\u001b[0;34m(self, rules, tr_set, batch_size, global_direction, intersecting, strict_join, strict_cut, fidelity_weight, complexity_weight, callbacks, callback_step, name, pickle_this)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m    417\u001b[0m     \u001b[39mprint\u001b[39m(distances)\n\u001b[0;32m--> 418\u001b[0m     distances\u001b[39m.\u001b[39mappend(i,j, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluator\u001b[39m.\u001b[39;49mdistance(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboundary[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboundary[j],x))\n\u001b[1;32m    420\u001b[0m \u001b[39m#distances = [(tupel[0],tupel[1] , self.evaluator.distance(self.boundary[tupel[0]], self.boundary[tupel[1]], x))\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m#             for tupel in candidates_indices]\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m#distances = [i,j , self.evaluator.distance(self.boundary[i], self.boundary[j], x))\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[39m#             for i,j in candidates_indices]\u001b[39;00m\n\u001b[1;32m    426\u001b[0m logger\u001b[39m.\u001b[39mdebug(full_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m|  sorting candidates queue\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/explainable/Bachelor/otherCode/glocalx/evaluators.py:392\u001b[0m, in \u001b[0;36mMemEvaluator.distance\u001b[0;34m(self, A, B, x, ids)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[39mreturn\u001b[39;00m diff\n\u001b[1;32m    391\u001b[0m \u001b[39m# New distance to compute\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m coverage_A, coverage_B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoverage(A, x, ids\u001b[39m=\u001b[39;49mids)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoverage(B, x, ids\u001b[39m=\u001b[39mids)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    393\u001b[0m diff \u001b[39m=\u001b[39m hamming(coverage_A, coverage_B)\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtuple\u001b[39m(A) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistances:\n",
      "File \u001b[0;32m~/explainable/Bachelor/otherCode/glocalx/evaluators.py:361\u001b[0m, in \u001b[0;36mMemEvaluator.coverage\u001b[0;34m(self, rules, patterns, targets, ids)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[39mfor\u001b[39;00m rule \u001b[39min\u001b[39;00m rules_:\n\u001b[1;32m    360\u001b[0m         \u001b[39mif\u001b[39;00m rule \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoverages:\n\u001b[0;32m--> 361\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoverages[rule] \u001b[39m=\u001b[39m coverage_matrix(rule, patterns, targets)\n\u001b[1;32m    362\u001b[0m     cov \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoverages[rule] \u001b[39mfor\u001b[39;00m rule \u001b[39min\u001b[39;00m rules_])\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/explainable/Bachelor/otherCode/glocalx/evaluators.py:94\u001b[0m, in \u001b[0;36mcoverage_matrix\u001b[0;34m(rules, patterns, targets, ids)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     coverage_matrix_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((\u001b[39mlen\u001b[39m(patterns)), \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 94\u001b[0m     hit_columns \u001b[39m=\u001b[39m [premises_from(rules, targets \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)]\n\u001b[1;32m     95\u001b[0m     coverage_matrix_[hit_columns] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     97\u001b[0m coverage_matrix_ \u001b[39m=\u001b[39m coverage_matrix_[:, ids] \u001b[39mif\u001b[39;00m ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m coverage_matrix_\n",
      "File \u001b[0;32m~/explainable/Bachelor/otherCode/glocalx/evaluators.py:75\u001b[0m, in \u001b[0;36mcoverage_matrix.<locals>.premises_from\u001b[0;34m(rule, pure)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpremises_from\u001b[39m(rule, pure\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pure:\n\u001b[0;32m---> 75\u001b[0m         premises \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlogical_and\u001b[39m.\u001b[39mreduce([[(patterns[:, feature] \u001b[39m>\u001b[39m lower) \u001b[39m&\u001b[39m (patterns[:, feature] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m upper)]\n\u001b[1;32m     76\u001b[0m                                           \u001b[39mfor\u001b[39;00m feature, (lower, upper) \u001b[39min\u001b[39;00m rule])\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     77\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         premises \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlogical_and\u001b[39m.\u001b[39mreduce([(patterns[:, feature] \u001b[39m>\u001b[39m lower) \u001b[39m&\u001b[39m (patterns[:, feature] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m upper)\n\u001b[1;32m     79\u001b[0m                                           \u001b[39m&\u001b[39m (targets \u001b[39m==\u001b[39m rule\u001b[39m.\u001b[39mconsequence)\n\u001b[1;32m     80\u001b[0m                                           \u001b[39mfor\u001b[39;00m feature, (lower, upper) \u001b[39min\u001b[39;00m rule])\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/explainable/Bachelor/otherCode/glocalx/evaluators.py:75\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpremises_from\u001b[39m(rule, pure\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pure:\n\u001b[0;32m---> 75\u001b[0m         premises \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlogical_and\u001b[39m.\u001b[39mreduce([[(patterns[:, feature] \u001b[39m>\u001b[39m lower) \u001b[39m&\u001b[39m (patterns[:, feature] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m upper)]\n\u001b[1;32m     76\u001b[0m                                           \u001b[39mfor\u001b[39;00m feature, (lower, upper) \u001b[39min\u001b[39;00m rule])\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     77\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         premises \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlogical_and\u001b[39m.\u001b[39mreduce([(patterns[:, feature] \u001b[39m>\u001b[39m lower) \u001b[39m&\u001b[39m (patterns[:, feature] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m upper)\n\u001b[1;32m     79\u001b[0m                                           \u001b[39m&\u001b[39m (targets \u001b[39m==\u001b[39m rule\u001b[39m.\u001b[39mconsequence)\n\u001b[1;32m     80\u001b[0m                                           \u001b[39mfor\u001b[39;00m feature, (lower, upper) \u001b[39min\u001b[39;00m rule])\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#from tensorflow.keras.models import load_model\n",
    "from numpy import genfromtxt, float as np_float\n",
    "import logzero\n",
    "\n",
    "from otherCode.glocalx.glocalx import GLocalX, shut_up_tensorflow\n",
    "\n",
    "from otherCode.glocalx.models import Rule\n",
    "\n",
    "# Set log profile: INFO for normal logging, DEBUG for verbosity\n",
    "logzero.loglevel(logzero.logging.INFO)\n",
    "shut_up_tensorflow()\n",
    "\n",
    "# Load black box: optional! Use black_box = None to use the dataset labels\n",
    "black_box = model\n",
    "# Load data and header\n",
    "#data = genfromtxt('data/dummy/dummy_dataset.csv', delimiter=',', names=True)\n",
    "features_names = features_names\n",
    "tr_set = X_eval\n",
    "\n",
    "print(type(X_eval))\n",
    "print(np.shape(X_eval))\n",
    "XY_EVAL = []\n",
    "\n",
    "for i in range(len(X_eval)):  \n",
    "    x = X_eval[i].tolist()\n",
    "    #print(x)\n",
    "    a = y_eval[i].item()\n",
    "    x.append(a)\n",
    "    #print(x)\n",
    "    XY_EVAL.append(x)\n",
    "\n",
    "tr_set = XY_EVAL\n",
    "\n",
    "#print(tr_set[:, :-1])\n",
    "print([sublist[-1] for sublist in tr_set])\n",
    "\n",
    "# Load local explanations\n",
    "#allRulesJson= Rule.from_json('/home/rosario/explainable/Bachelor/allRules.json', names=features_names)\n",
    "f = open(\"/home/rosario/explainable/Bachelor/allRules.json\") # allRulesJson #Rule.from_json('data/dummy/dummy_rules.json\")\n",
    "#print(type(json.load(f)[0]))\n",
    "features_names1 = range(0,len(features_names))\n",
    "#print(features_names1)\n",
    "local_explanations = Rule.from_json('/home/rosario/explainable/Bachelor/testJson.json', names=features_names1)# allRulesJson #Rule.from_json('data/dummy/dummy_rules.json', names=features_names)\n",
    "\n",
    "# Create a GLocalX instance for `black_box`\n",
    "glocalx = GLocalX(oracle=black_box)\n",
    "# Fit the model, use batch_size=128 for larger datasets\n",
    "# need this format {\"1\": [32, Infinity], \"2\": [0.5, Infinity], \"label\": 1}\n",
    "#glocalx = glocalx.fit(local_explanations, tr_set, batch_size=2, name='black_box_explanations')\n",
    "glocalx = glocalx.fit(local_explanations, tr_set, batch_size=4, name='black_box_explanations')\n",
    "\n",
    "# Retrieve global explanations by fidelity\n",
    "alpha = 0.5\n",
    "global_explanations = glocalx.rules(alpha, tr_set)\n",
    "# Retrieve global explanations by fidelity percentile\n",
    "alpha = 95\n",
    "global_explanations = glocalx.rules(alpha, tr_set, is_percentile=True)\n",
    "# Retrieve exactly `alpha` global explanations, `alpha/2` per class\n",
    "alpha = 10\n",
    "global_explanations = glocalx.rules(alpha, tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      support                                           itemsets\n",
      "0    0.077922                               (-0.704<BMI<=-0.445)\n",
      "1    0.207792                      (-0.157<BloodPressure<=0.429)\n",
      "2    0.181818                             (1.713<Insulin<=2.425)\n",
      "3    0.337662                             (1.757<Glucose<=2.411)\n",
      "4    0.168831                    (-0.32<SkinThickness<=-0.00671)\n",
      "..        ...                                                ...\n",
      "681  0.012987  (-0.903<Pregnancies<=-0.681, -1.138<Insulin<=-...\n",
      "682  0.012987  (-1.138<Insulin<=-0.423, -1.073<DiabetesPedigr...\n",
      "683  0.012987  (-0.903<Pregnancies<=-0.681, -1.138<Insulin<=-...\n",
      "684  0.012987  (0.449<Glucose<=1.103, -1.073<DiabetesPedigree...\n",
      "685  0.012987  (-0.903<Pregnancies<=-0.681, -1.073<DiabetesPe...\n",
      "\n",
      "[686 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosario/.local/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpcommon.py:111: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "/home/rosario/.local/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpcommon.py:111: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "/home/rosario/.local/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpcommon.py:111: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-0.458&lt;Pregnancies&lt;=-0.235)</td>\n",
       "      <td>(-0.704&lt;BMI&lt;=-0.445)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.058442</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.111111</td>\n",
       "      <td>0.012228</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1.713&lt;Insulin&lt;=2.425)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(-0.746&lt;BloodPressure&lt;=-0.157)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(-0.206&lt;Glucose&lt;=0.449)</td>\n",
       "      <td>(-0.652&lt;DiabetesPedigreeFunction&lt;=-0.442)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.012397</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.536&lt;Age&lt;=0.945)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>(-0.681&lt;Pregnancies&lt;=-0.458, -0.445&lt;BMI&lt;=-0.187)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>(0.449&lt;Glucose&lt;=1.103, -1.073&lt;DiabetesPedigree...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>(-0.903&lt;Pregnancies&lt;=-0.681, -1.073&lt;DiabetesPe...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>(0.449&lt;Glucose&lt;=1.103, -0.903&lt;Pregnancies&lt;=-0....</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>(0.449&lt;Glucose&lt;=1.103, 1)</td>\n",
       "      <td>(-0.903&lt;Pregnancies&lt;=-0.681)</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.175325</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.277778</td>\n",
       "      <td>0.029853</td>\n",
       "      <td>3.298701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           antecedents  \\\n",
       "0                         (-0.458<Pregnancies<=-0.235)   \n",
       "1                               (1.713<Insulin<=2.425)   \n",
       "2                       (-0.746<BloodPressure<=-0.157)   \n",
       "3                              (-0.206<Glucose<=0.449)   \n",
       "4                                   (0.536<Age<=0.945)   \n",
       "..                                                 ...   \n",
       "263   (-0.681<Pregnancies<=-0.458, -0.445<BMI<=-0.187)   \n",
       "264  (0.449<Glucose<=1.103, -1.073<DiabetesPedigree...   \n",
       "265  (-0.903<Pregnancies<=-0.681, -1.073<DiabetesPe...   \n",
       "266  (0.449<Glucose<=1.103, -0.903<Pregnancies<=-0....   \n",
       "267                          (0.449<Glucose<=1.103, 1)   \n",
       "\n",
       "                                   consequents  antecedent support  \\\n",
       "0                         (-0.704<BMI<=-0.445)            0.012987   \n",
       "1                                          (0)            0.103896   \n",
       "2                                          (0)            0.038961   \n",
       "3    (-0.652<DiabetesPedigreeFunction<=-0.442)            0.012987   \n",
       "4                                          (0)            0.012987   \n",
       "..                                         ...                 ...   \n",
       "263                                        (1)            0.025974   \n",
       "264                                        (1)            0.025974   \n",
       "265                                        (1)            0.051948   \n",
       "266                                        (1)            0.038961   \n",
       "267               (-0.903<Pregnancies<=-0.681)            0.051948   \n",
       "\n",
       "     consequent support   support  confidence       lift  leverage  conviction  \n",
       "0              0.058442  0.012987    1.000000  17.111111  0.012228         inf  \n",
       "1              0.500000  0.090909    0.875000   1.750000  0.038961    4.000000  \n",
       "2              0.500000  0.032468    0.833333   1.666667  0.012987    3.000000  \n",
       "3              0.045455  0.012987    1.000000  22.000000  0.012397         inf  \n",
       "4              0.500000  0.012987    1.000000   2.000000  0.006494         inf  \n",
       "..                  ...       ...         ...        ...       ...         ...  \n",
       "263            0.500000  0.019481    0.750000   1.500000  0.006494    2.000000  \n",
       "264            0.500000  0.019481    0.750000   1.500000  0.006494    2.000000  \n",
       "265            0.500000  0.038961    0.750000   1.500000  0.012987    2.000000  \n",
       "266            0.500000  0.038961    1.000000   2.000000  0.019481         inf  \n",
       "267            0.175325  0.038961    0.750000   4.277778  0.029853    3.298701  \n",
       "\n",
       "[268 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_df = ohe_dfList[0]     \n",
    "#Get frequent itemsets from a one-hot DataFrame\n",
    "                                        #10\n",
    "#print(10/len(predsList))\n",
    "freq_items = apriori(ohe_df, min_support=(1/len(predsList_eval)), use_colnames=True, max_len=3)\n",
    "#print(freq_items)\n",
    "all_rules = association_rules(freq_items, metohric=\"confidence\", min_threshold=0.7, support_only=False)\n",
    "#print(all_rules)   \n",
    "#print(ohe_df.loc[ohe_df[pos_label] == 1])\n",
    "\n",
    "                                               #10\n",
    "freq_items = apriori(ohe_df.loc[ohe_df[pos_label] == 1], min_support=(1/len(predsList_eval)), use_colnames=True, max_len=3)\n",
    "pos_rules = association_rules(freq_items, metric=\"confidence\", min_threshold=0.1, support_only=False) #0.6\n",
    "                                                            #10\n",
    "freq_items = apriori(ohe_df.loc[ohe_df[neg_label] == 1], min_support=(1/len(predsList_eval)), use_colnames=True, max_len=3)\n",
    "neg_rules = association_rules(freq_items, metric=\"confidence\", min_threshold=0.1, support_only=False) #0.6\n",
    "\n",
    "print(freq_items)\n",
    "all_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-0.947&lt;SkinThickness&lt;=-0.632, -0.681&lt;Pregnanc...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, -1.073&lt;DiabetesP...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(-0.947&lt;SkinThickness&lt;=-0.632, -0.187&lt;BMI&lt;=0.071)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(-0.283&lt;Age&lt;=0.126, -0.947&lt;SkinThickness&lt;=-0.632)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.429&lt;BloodPressure&lt;=1.015, -0.32&lt;SkinThickne...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(-0.283&lt;Age&lt;=0.126, -1.073&lt;DiabetesPedigreeFun...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(-0.903&lt;Pregnancies&lt;=-0.681, -0.694&lt;Age&lt;=-0.283)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, -0.694&lt;Age&lt;=-0.283)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(-0.632&lt;SkinThickness&lt;=-0.32, -0.681&lt;Pregnanci...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(-1.138&lt;Insulin&lt;=-0.423, 0.071&lt;BMI&lt;=0.329)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(-0.903&lt;Pregnancies&lt;=-0.681, -1.138&lt;Insulin&lt;=-...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(-0.32&lt;SkinThickness&lt;=-0.00671, -0.187&lt;BMI&lt;=0....</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(-0.903&lt;Pregnancies&lt;=-0.681, -0.187&lt;BMI&lt;=0.071)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(1.757&lt;Glucose&lt;=2.411, 0.071&lt;BMI&lt;=0.329)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(-0.903&lt;Pregnancies&lt;=-0.681, -0.704&lt;BMI&lt;=-0.445)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(-1.073&lt;DiabetesPedigreeFunction&lt;=-0.863, -0.1...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(-0.704&lt;BMI&lt;=-0.445, -1.073&lt;DiabetesPedigreeFu...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(1.103&lt;Glucose&lt;=1.757, -1.138&lt;Insulin&lt;=-0.423)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(1.713&lt;Insulin&lt;=2.425, -0.704&lt;BMI&lt;=-0.445)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(0.071&lt;BMI&lt;=0.329, -0.947&lt;SkinThickness&lt;=-0.632)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(-1.283&lt;DiabetesPedigreeFunction&lt;=-1.073, -0.3...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(-1.283&lt;DiabetesPedigreeFunction&lt;=-1.073, 0.28...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(-0.423&lt;Insulin&lt;=0.289)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(-0.458&lt;Pregnancies&lt;=-0.235)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(-1.283&lt;DiabetesPedigreeFunction&lt;=-1.073, -0.6...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(-0.632&lt;SkinThickness&lt;=-0.32, -0.704&lt;BMI&lt;=-0.445)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(0.126&lt;Age&lt;=0.536, -0.632&lt;SkinThickness&lt;=-0.32)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(1.015&lt;BloodPressure&lt;=1.601, -1.138&lt;Insulin&lt;=-...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(0.126&lt;Age&lt;=0.536, -1.138&lt;Insulin&lt;=-0.423)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(-1.283&lt;DiabetesPedigreeFunction&lt;=-1.073, -0.1...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(-0.283&lt;Age&lt;=0.126, -0.187&lt;BMI&lt;=0.071)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(0.289&lt;Insulin&lt;=1.001, -0.187&lt;BMI&lt;=0.071)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, -0.187&lt;BMI&lt;=0.071)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(0.289&lt;Insulin&lt;=1.001, 0.429&lt;BloodPressure&lt;=1....</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(0.071&lt;BMI&lt;=0.329, -0.283&lt;Age&lt;=0.126)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(-1.283&lt;DiabetesPedigreeFunction&lt;=-1.073, -0.4...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(0.126&lt;Age&lt;=0.536, -0.947&lt;SkinThickness&lt;=-0.632)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, 1.757&lt;Glucose&lt;=2...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(-0.32&lt;SkinThickness&lt;=-0.00671, -0.681&lt;Pregnan...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, -0.863&lt;DiabetesP...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, -0.681&lt;Pregnanci...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(-0.903&lt;Pregnancies&lt;=-0.681, 1.001&lt;Insulin&lt;=1....</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          antecedents consequents  \\\n",
       "0   (-0.947<SkinThickness<=-0.632, -0.681<Pregnanc...         (1)   \n",
       "1   (-0.157<BloodPressure<=0.429, -1.073<DiabetesP...         (1)   \n",
       "2   (-0.947<SkinThickness<=-0.632, -0.187<BMI<=0.071)         (1)   \n",
       "3   (-0.283<Age<=0.126, -0.947<SkinThickness<=-0.632)         (1)   \n",
       "4   (0.429<BloodPressure<=1.015, -0.32<SkinThickne...         (1)   \n",
       "5   (-0.283<Age<=0.126, -1.073<DiabetesPedigreeFun...         (1)   \n",
       "6    (-0.903<Pregnancies<=-0.681, -0.694<Age<=-0.283)         (1)   \n",
       "7   (-0.157<BloodPressure<=0.429, -0.694<Age<=-0.283)         (1)   \n",
       "8   (-0.632<SkinThickness<=-0.32, -0.681<Pregnanci...         (1)   \n",
       "9          (-1.138<Insulin<=-0.423, 0.071<BMI<=0.329)         (1)   \n",
       "10  (-0.903<Pregnancies<=-0.681, -1.138<Insulin<=-...         (1)   \n",
       "11  (-0.32<SkinThickness<=-0.00671, -0.187<BMI<=0....         (1)   \n",
       "12    (-0.903<Pregnancies<=-0.681, -0.187<BMI<=0.071)         (1)   \n",
       "13           (1.757<Glucose<=2.411, 0.071<BMI<=0.329)         (1)   \n",
       "14   (-0.903<Pregnancies<=-0.681, -0.704<BMI<=-0.445)         (1)   \n",
       "15  (-1.073<DiabetesPedigreeFunction<=-0.863, -0.1...         (1)   \n",
       "16  (-0.704<BMI<=-0.445, -1.073<DiabetesPedigreeFu...         (1)   \n",
       "17     (1.103<Glucose<=1.757, -1.138<Insulin<=-0.423)         (1)   \n",
       "18         (1.713<Insulin<=2.425, -0.704<BMI<=-0.445)         (1)   \n",
       "19   (0.071<BMI<=0.329, -0.947<SkinThickness<=-0.632)         (1)   \n",
       "20  (-1.283<DiabetesPedigreeFunction<=-1.073, -0.3...         (1)   \n",
       "21  (-1.283<DiabetesPedigreeFunction<=-1.073, 0.28...         (1)   \n",
       "22                            (-0.423<Insulin<=0.289)         (1)   \n",
       "23                       (-0.458<Pregnancies<=-0.235)         (1)   \n",
       "24  (-1.283<DiabetesPedigreeFunction<=-1.073, -0.6...         (1)   \n",
       "25  (-0.632<SkinThickness<=-0.32, -0.704<BMI<=-0.445)         (1)   \n",
       "26    (0.126<Age<=0.536, -0.632<SkinThickness<=-0.32)         (1)   \n",
       "28  (1.015<BloodPressure<=1.601, -1.138<Insulin<=-...         (1)   \n",
       "29         (0.126<Age<=0.536, -1.138<Insulin<=-0.423)         (1)   \n",
       "30  (-1.283<DiabetesPedigreeFunction<=-1.073, -0.1...         (1)   \n",
       "31             (-0.283<Age<=0.126, -0.187<BMI<=0.071)         (1)   \n",
       "32          (0.289<Insulin<=1.001, -0.187<BMI<=0.071)         (1)   \n",
       "33   (-0.157<BloodPressure<=0.429, -0.187<BMI<=0.071)         (1)   \n",
       "34  (0.289<Insulin<=1.001, 0.429<BloodPressure<=1....         (1)   \n",
       "35              (0.071<BMI<=0.329, -0.283<Age<=0.126)         (1)   \n",
       "36  (-1.283<DiabetesPedigreeFunction<=-1.073, -0.4...         (1)   \n",
       "37   (0.126<Age<=0.536, -0.947<SkinThickness<=-0.632)         (1)   \n",
       "39  (-0.157<BloodPressure<=0.429, 1.757<Glucose<=2...         (1)   \n",
       "40  (-0.32<SkinThickness<=-0.00671, -0.681<Pregnan...         (1)   \n",
       "42  (-0.157<BloodPressure<=0.429, -0.863<DiabetesP...         (1)   \n",
       "43  (-0.157<BloodPressure<=0.429, -0.681<Pregnanci...         (1)   \n",
       "44  (-0.903<Pregnancies<=-0.681, 1.001<Insulin<=1....         (1)   \n",
       "\n",
       "    antecedent support  consequent support   support  confidence  lift  \\\n",
       "0             0.051948                 0.5  0.051948         1.0   2.0   \n",
       "1             0.045455                 0.5  0.045455         1.0   2.0   \n",
       "2             0.045455                 0.5  0.045455         1.0   2.0   \n",
       "3             0.038961                 0.5  0.038961         1.0   2.0   \n",
       "4             0.032468                 0.5  0.032468         1.0   2.0   \n",
       "5             0.025974                 0.5  0.025974         1.0   2.0   \n",
       "6             0.025974                 0.5  0.025974         1.0   2.0   \n",
       "7             0.025974                 0.5  0.025974         1.0   2.0   \n",
       "8             0.025974                 0.5  0.025974         1.0   2.0   \n",
       "9             0.025974                 0.5  0.025974         1.0   2.0   \n",
       "10            0.025974                 0.5  0.025974         1.0   2.0   \n",
       "11            0.025974                 0.5  0.025974         1.0   2.0   \n",
       "12            0.025974                 0.5  0.025974         1.0   2.0   \n",
       "13            0.025974                 0.5  0.025974         1.0   2.0   \n",
       "14            0.025974                 0.5  0.025974         1.0   2.0   \n",
       "15            0.019481                 0.5  0.019481         1.0   2.0   \n",
       "16            0.019481                 0.5  0.019481         1.0   2.0   \n",
       "17            0.019481                 0.5  0.019481         1.0   2.0   \n",
       "18            0.019481                 0.5  0.019481         1.0   2.0   \n",
       "19            0.019481                 0.5  0.019481         1.0   2.0   \n",
       "20            0.019481                 0.5  0.019481         1.0   2.0   \n",
       "21            0.019481                 0.5  0.019481         1.0   2.0   \n",
       "22            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "23            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "24            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "25            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "26            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "28            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "29            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "30            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "31            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "32            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "33            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "34            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "35            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "36            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "37            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "39            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "40            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "42            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "43            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "44            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "\n",
       "    leverage  conviction  \n",
       "0   0.025974         inf  \n",
       "1   0.022727         inf  \n",
       "2   0.022727         inf  \n",
       "3   0.019481         inf  \n",
       "4   0.016234         inf  \n",
       "5   0.012987         inf  \n",
       "6   0.012987         inf  \n",
       "7   0.012987         inf  \n",
       "8   0.012987         inf  \n",
       "9   0.012987         inf  \n",
       "10  0.012987         inf  \n",
       "11  0.012987         inf  \n",
       "12  0.012987         inf  \n",
       "13  0.012987         inf  \n",
       "14  0.012987         inf  \n",
       "15  0.009740         inf  \n",
       "16  0.009740         inf  \n",
       "17  0.009740         inf  \n",
       "18  0.009740         inf  \n",
       "19  0.009740         inf  \n",
       "20  0.009740         inf  \n",
       "21  0.009740         inf  \n",
       "22  0.006494         inf  \n",
       "23  0.006494         inf  \n",
       "24  0.006494         inf  \n",
       "25  0.006494         inf  \n",
       "26  0.006494         inf  \n",
       "28  0.006494         inf  \n",
       "29  0.006494         inf  \n",
       "30  0.006494         inf  \n",
       "31  0.006494         inf  \n",
       "32  0.006494         inf  \n",
       "33  0.006494         inf  \n",
       "34  0.006494         inf  \n",
       "35  0.006494         inf  \n",
       "36  0.006494         inf  \n",
       "37  0.006494         inf  \n",
       "39  0.006494         inf  \n",
       "40  0.006494         inf  \n",
       "42  0.006494         inf  \n",
       "43  0.006494         inf  \n",
       "44  0.006494         inf  "
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = all_rules[all_rules['consequents'] == {pos_label}]\n",
    "\n",
    "positive = positive[positive['confidence'] == 1]\n",
    "\n",
    "positive = positive.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "seen = set()\n",
    "dropped = set()\n",
    "indexes_to_drop = []\n",
    "\n",
    "positive = positive.reset_index(drop=True)\n",
    "for i in positive.index:\n",
    "    new_rule = positive.loc[[i]]['antecedents'].values[0]\n",
    "    \n",
    "    for seen_rule in seen:\n",
    "        if seen_rule.issubset(new_rule):#new_rule.issubset(seen_rule) or seen_rule.issubset(new_rule):\n",
    "            indexes_to_drop.append(i)\n",
    "            break\n",
    "    else:\n",
    "        seen.add(new_rule)\n",
    "\n",
    "positive.drop(positive.index[indexes_to_drop], inplace=True )\n",
    "positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1.103&lt;Glucose&lt;=1.757, -1.126&lt;Pregnancies&lt;=-0....</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1.713&lt;Insulin&lt;=2.425, 0.449&lt;Glucose&lt;=1.103)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.429&lt;BloodPressure&lt;=1.015, -1.126&lt;Pregnancie...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1.103&lt;Glucose&lt;=1.757, -0.632&lt;SkinThickness&lt;=-...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1.757&lt;Glucose&lt;=2.411, -1.126&lt;Pregnancies&lt;=-0....</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(-0.746&lt;BloodPressure&lt;=-0.157, -0.652&lt;Diabetes...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(-0.903&lt;Pregnancies&lt;=-0.681, 1.015&lt;BloodPressu...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(-1.126&lt;Pregnancies&lt;=-0.903, -0.32&lt;SkinThickne...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(-0.206&lt;Glucose&lt;=0.449)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(-1.35&lt;Pregnancies&lt;=-1.126)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(-1.138&lt;Insulin&lt;=-0.423, -0.652&lt;DiabetesPedigr...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(1.713&lt;Insulin&lt;=2.425, -0.652&lt;DiabetesPedigree...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(-0.746&lt;BloodPressure&lt;=-0.157, -0.694&lt;Age&lt;=-0....</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(-0.746&lt;BloodPressure&lt;=-0.157, -1.138&lt;Insulin&lt;...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(1.713&lt;Insulin&lt;=2.425, -1.126&lt;Pregnancies&lt;=-0....</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(1.103&lt;Glucose&lt;=1.757, -0.704&lt;BMI&lt;=-0.445)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(-1.283&lt;DiabetesPedigreeFunction&lt;=-1.073, 1.10...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(-1.126&lt;Pregnancies&lt;=-0.903, -0.947&lt;SkinThickn...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(1.103&lt;Glucose&lt;=1.757, -0.947&lt;SkinThickness&lt;=-...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(-0.445&lt;BMI&lt;=-0.187, -0.947&lt;SkinThickness&lt;=-0....</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(1.015&lt;BloodPressure&lt;=1.601, -0.32&lt;SkinThickne...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(1.015&lt;BloodPressure&lt;=1.601, 0.126&lt;Age&lt;=0.536)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(1.015&lt;BloodPressure&lt;=1.601, -1.126&lt;Pregnancie...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(1.015&lt;BloodPressure&lt;=1.601, 1.103&lt;Glucose&lt;=1....</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(1.015&lt;BloodPressure&lt;=1.601, -0.445&lt;BMI&lt;=-0.187)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(-0.32&lt;SkinThickness&lt;=-0.00671, -0.283&lt;Age&lt;=0....</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(-0.32&lt;SkinThickness&lt;=-0.00671, 0.449&lt;Glucose&lt;...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(-0.863&lt;DiabetesPedigreeFunction&lt;=-0.652, 0.44...</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(-1.126&lt;Pregnancies&lt;=-0.903, 0.449&lt;Glucose&lt;=1....</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          antecedents consequents  \\\n",
       "0   (1.103<Glucose<=1.757, -1.126<Pregnancies<=-0....         (0)   \n",
       "1        (1.713<Insulin<=2.425, 0.449<Glucose<=1.103)         (0)   \n",
       "2   (0.429<BloodPressure<=1.015, -1.126<Pregnancie...         (0)   \n",
       "3   (1.103<Glucose<=1.757, -0.632<SkinThickness<=-...         (0)   \n",
       "4   (1.757<Glucose<=2.411, -1.126<Pregnancies<=-0....         (0)   \n",
       "5   (-0.746<BloodPressure<=-0.157, -0.652<Diabetes...         (0)   \n",
       "6   (-0.903<Pregnancies<=-0.681, 1.015<BloodPressu...         (0)   \n",
       "7   (-1.126<Pregnancies<=-0.903, -0.32<SkinThickne...         (0)   \n",
       "8                             (-0.206<Glucose<=0.449)         (0)   \n",
       "9                         (-1.35<Pregnancies<=-1.126)         (0)   \n",
       "12  (-1.138<Insulin<=-0.423, -0.652<DiabetesPedigr...         (0)   \n",
       "13  (1.713<Insulin<=2.425, -0.652<DiabetesPedigree...         (0)   \n",
       "14  (-0.746<BloodPressure<=-0.157, -0.694<Age<=-0....         (0)   \n",
       "15  (-0.746<BloodPressure<=-0.157, -1.138<Insulin<...         (0)   \n",
       "16  (1.713<Insulin<=2.425, -1.126<Pregnancies<=-0....         (0)   \n",
       "17         (1.103<Glucose<=1.757, -0.704<BMI<=-0.445)         (0)   \n",
       "18  (-1.283<DiabetesPedigreeFunction<=-1.073, 1.10...         (0)   \n",
       "19  (-1.126<Pregnancies<=-0.903, -0.947<SkinThickn...         (0)   \n",
       "20  (1.103<Glucose<=1.757, -0.947<SkinThickness<=-...         (0)   \n",
       "21  (-0.445<BMI<=-0.187, -0.947<SkinThickness<=-0....         (0)   \n",
       "22  (1.015<BloodPressure<=1.601, -0.32<SkinThickne...         (0)   \n",
       "23     (1.015<BloodPressure<=1.601, 0.126<Age<=0.536)         (0)   \n",
       "24  (1.015<BloodPressure<=1.601, -1.126<Pregnancie...         (0)   \n",
       "25  (1.015<BloodPressure<=1.601, 1.103<Glucose<=1....         (0)   \n",
       "26   (1.015<BloodPressure<=1.601, -0.445<BMI<=-0.187)         (0)   \n",
       "27  (-0.32<SkinThickness<=-0.00671, -0.283<Age<=0....         (0)   \n",
       "28  (-0.32<SkinThickness<=-0.00671, 0.449<Glucose<...         (0)   \n",
       "29  (-0.863<DiabetesPedigreeFunction<=-0.652, 0.44...         (0)   \n",
       "30  (-1.126<Pregnancies<=-0.903, 0.449<Glucose<=1....         (0)   \n",
       "\n",
       "    antecedent support  consequent support   support  confidence  lift  \\\n",
       "0             0.051948                 0.5  0.051948         1.0   2.0   \n",
       "1             0.032468                 0.5  0.032468         1.0   2.0   \n",
       "2             0.032468                 0.5  0.032468         1.0   2.0   \n",
       "3             0.025974                 0.5  0.025974         1.0   2.0   \n",
       "4             0.025974                 0.5  0.025974         1.0   2.0   \n",
       "5             0.019481                 0.5  0.019481         1.0   2.0   \n",
       "6             0.019481                 0.5  0.019481         1.0   2.0   \n",
       "7             0.019481                 0.5  0.019481         1.0   2.0   \n",
       "8             0.012987                 0.5  0.012987         1.0   2.0   \n",
       "9             0.012987                 0.5  0.012987         1.0   2.0   \n",
       "12            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "13            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "14            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "15            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "16            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "17            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "18            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "19            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "20            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "21            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "22            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "23            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "24            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "25            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "26            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "27            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "28            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "29            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "30            0.012987                 0.5  0.012987         1.0   2.0   \n",
       "\n",
       "    leverage  conviction  \n",
       "0   0.025974         inf  \n",
       "1   0.016234         inf  \n",
       "2   0.016234         inf  \n",
       "3   0.012987         inf  \n",
       "4   0.012987         inf  \n",
       "5   0.009740         inf  \n",
       "6   0.009740         inf  \n",
       "7   0.009740         inf  \n",
       "8   0.006494         inf  \n",
       "9   0.006494         inf  \n",
       "12  0.006494         inf  \n",
       "13  0.006494         inf  \n",
       "14  0.006494         inf  \n",
       "15  0.006494         inf  \n",
       "16  0.006494         inf  \n",
       "17  0.006494         inf  \n",
       "18  0.006494         inf  \n",
       "19  0.006494         inf  \n",
       "20  0.006494         inf  \n",
       "21  0.006494         inf  \n",
       "22  0.006494         inf  \n",
       "23  0.006494         inf  \n",
       "24  0.006494         inf  \n",
       "25  0.006494         inf  \n",
       "26  0.006494         inf  \n",
       "27  0.006494         inf  \n",
       "28  0.006494         inf  \n",
       "29  0.006494         inf  \n",
       "30  0.006494         inf  "
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = all_rules[all_rules['consequents'] == {neg_label}]\n",
    "negative = negative[negative['confidence'] == 1]\n",
    "negative = negative.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "\n",
    "seen = set()\n",
    "dropped = set()\n",
    "indexes_to_drop = []\n",
    "\n",
    "negative = negative.reset_index(drop=True)\n",
    "print(len(negative))\n",
    "for i in negative.index:\n",
    "    new_rule = negative.loc[[i]]['antecedents'].values[0]\n",
    "    \n",
    "    for seen_rule in seen:\n",
    "        if seen_rule.issubset(new_rule):#new_rule.issubset(seen_rule) or seen_rule.issubset(new_rule):\n",
    "            indexes_to_drop.append(i)\n",
    "            break\n",
    "    else:\n",
    "        seen.add(new_rule)\n",
    "\n",
    "negative.drop(negative.index[indexes_to_drop], inplace=True )\n",
    "print(len(negative))\n",
    "negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4447/4608923.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  both = positive.append(negative, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemset</th>\n",
       "      <th>label</th>\n",
       "      <th>num-items</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>antecedent support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-0.947&lt;SkinThickness&lt;=-0.632, -0.681&lt;Pregnanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.051948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(1.103&lt;Glucose&lt;=1.757, -1.126&lt;Pregnancies&lt;=-0....</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.051948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, -1.073&lt;DiabetesP...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(-0.947&lt;SkinThickness&lt;=-0.632, -0.187&lt;BMI&lt;=0.071)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(-0.283&lt;Age&lt;=0.126, -0.947&lt;SkinThickness&lt;=-0.632)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.038961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>(-1.126&lt;Pregnancies&lt;=-0.903, 0.449&lt;Glucose&lt;=1....</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(-0.423&lt;Insulin&lt;=0.289)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(-0.458&lt;Pregnancies&lt;=-0.235)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>(-0.206&lt;Glucose&lt;=0.449)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>(-1.35&lt;Pregnancies&lt;=-1.126)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              itemset label  num-items  \\\n",
       "0   (-0.947<SkinThickness<=-0.632, -0.681<Pregnanc...     1          2   \n",
       "42  (1.103<Glucose<=1.757, -1.126<Pregnancies<=-0....     0          2   \n",
       "1   (-0.157<BloodPressure<=0.429, -1.073<DiabetesP...     1          2   \n",
       "2   (-0.947<SkinThickness<=-0.632, -0.187<BMI<=0.071)     1          2   \n",
       "3   (-0.283<Age<=0.126, -0.947<SkinThickness<=-0.632)     1          2   \n",
       "..                                                ...   ...        ...   \n",
       "70  (-1.126<Pregnancies<=-0.903, 0.449<Glucose<=1....     0          2   \n",
       "22                            (-0.423<Insulin<=0.289)     1          1   \n",
       "23                       (-0.458<Pregnancies<=-0.235)     1          1   \n",
       "50                            (-0.206<Glucose<=0.449)     0          1   \n",
       "51                        (-1.35<Pregnancies<=-1.126)     0          1   \n",
       "\n",
       "     support  confidence  antecedent support  \n",
       "0   0.051948         1.0            0.051948  \n",
       "42  0.051948         1.0            0.051948  \n",
       "1   0.045455         1.0            0.045455  \n",
       "2   0.045455         1.0            0.045455  \n",
       "3   0.038961         1.0            0.038961  \n",
       "..       ...         ...                 ...  \n",
       "70  0.012987         1.0            0.012987  \n",
       "22  0.012987         1.0            0.012987  \n",
       "23  0.012987         1.0            0.012987  \n",
       "50  0.012987         1.0            0.012987  \n",
       "51  0.012987         1.0            0.012987  \n",
       "\n",
       "[71 rows x 6 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive['num-items'] = positive['antecedents'].map(lambda x: len(x))\n",
    "negative['num-items'] = negative['antecedents'].map(lambda x: len(x))\n",
    "positive['consequents'] = positive['consequents'].map(lambda x: pos_label)\n",
    "negative['consequents'] = negative['consequents'].map(lambda x: neg_label)\n",
    "\n",
    "both = positive.append(negative, ignore_index=True)\n",
    "\n",
    "discr_rules = both[['antecedents', 'consequents', 'num-items', 'support', 'confidence', 'antecedent support']].sort_values(\n",
    "    ['support', 'confidence', 'num-items'], ascending=[False, False, False])\n",
    "\n",
    "discr_rules = discr_rules.rename(columns={\"antecedents\": \"itemset\", \"consequents\": \"label\"})\n",
    "\n",
    "discr_rules#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "                                            antecedents  \\\n",
      "0                                  (-0.694<Age<=-0.283)   \n",
      "1             (-1.073<DiabetesPedigreeFunction<=-0.863)   \n",
      "2                         (-0.632<SkinThickness<=-0.32)   \n",
      "3             (-1.073<DiabetesPedigreeFunction<=-0.863)   \n",
      "4                              (-1.138<Insulin<=-0.423)   \n",
      "...                                                 ...   \n",
      "2707  (-0.157<BloodPressure<=0.429, -0.445<BMI<=-0.187)   \n",
      "2708                            (1, -0.445<BMI<=-0.187)   \n",
      "2709                      (-0.157<BloodPressure<=0.429)   \n",
      "2710                                                (1)   \n",
      "2711                               (-0.445<BMI<=-0.187)   \n",
      "\n",
      "                                            consequents  antecedent support  \\\n",
      "0             (-1.073<DiabetesPedigreeFunction<=-0.863)            0.129870   \n",
      "1                                  (-0.694<Age<=-0.283)            0.298701   \n",
      "2             (-1.073<DiabetesPedigreeFunction<=-0.863)            0.168831   \n",
      "3                         (-0.632<SkinThickness<=-0.32)            0.298701   \n",
      "4             (-1.073<DiabetesPedigreeFunction<=-0.863)            0.311688   \n",
      "...                                                 ...                 ...   \n",
      "2707                                                (1)            0.116883   \n",
      "2708                      (-0.157<BloodPressure<=0.429)            0.259740   \n",
      "2709                            (1, -0.445<BMI<=-0.187)            0.246753   \n",
      "2710  (-0.157<BloodPressure<=0.429, -0.445<BMI<=-0.187)            1.000000   \n",
      "2711                   (-0.157<BloodPressure<=0.429, 1)            0.259740   \n",
      "\n",
      "      consequent support   support  confidence      lift  leverage  conviction  \n",
      "0               0.298701  0.051948    0.400000  1.339130  0.013156    1.168831  \n",
      "1               0.129870  0.051948    0.173913  1.339130  0.013156    1.053315  \n",
      "2               0.298701  0.038961    0.230769  0.772575 -0.011469    0.911688  \n",
      "3               0.168831  0.038961    0.130435  0.772575 -0.011469    0.955844  \n",
      "4               0.298701  0.077922    0.250000  0.836957 -0.015180    0.935065  \n",
      "...                  ...       ...         ...       ...       ...         ...  \n",
      "2707            1.000000  0.116883    1.000000  1.000000  0.000000         inf  \n",
      "2708            0.246753  0.116883    0.450000  1.823684  0.052791    1.369540  \n",
      "2709            0.259740  0.116883    0.473684  1.823684  0.052791    1.406494  \n",
      "2710            0.116883  0.116883    0.116883  1.000000  0.000000    1.000000  \n",
      "2711            0.246753  0.116883    0.450000  1.823684  0.052791    1.369540  \n",
      "\n",
      "[2712 rows x 9 columns]\n",
      "   antecedents                                        consequents  \\\n",
      "0          (1)                           (-1.138<Insulin<=-0.423)   \n",
      "1          (1)          (-1.073<DiabetesPedigreeFunction<=-0.863)   \n",
      "2          (1)                       (0.429<BloodPressure<=1.015)   \n",
      "3          (1)                             (1.757<Glucose<=2.411)   \n",
      "4          (1)                               (-0.445<BMI<=-0.187)   \n",
      "5          (1)                      (-0.157<BloodPressure<=0.429)   \n",
      "6          (1)                       (-0.681<Pregnancies<=-0.458)   \n",
      "7          (1)                     (-0.947<SkinThickness<=-0.632)   \n",
      "8          (1)                       (-0.903<Pregnancies<=-0.681)   \n",
      "9          (1)                                (-0.187<BMI<=0.071)   \n",
      "10         (1)                      (-0.632<SkinThickness<=-0.32)   \n",
      "11         (1)                    (-0.32<SkinThickness<=-0.00671)   \n",
      "12         (1)     (1.757<Glucose<=2.411, -1.138<Insulin<=-0.423)   \n",
      "13         (1)                                (-0.283<Age<=0.126)   \n",
      "14         (1)  (-1.138<Insulin<=-0.423, -0.947<SkinThickness<...   \n",
      "15         (1)                               (-0.694<Age<=-0.283)   \n",
      "16         (1)          (-0.863<DiabetesPedigreeFunction<=-0.652)   \n",
      "17         (1)        (-1.138<Insulin<=-0.423, -0.187<BMI<=0.071)   \n",
      "18         (1)  (-1.138<Insulin<=-0.423, -0.681<Pregnancies<=-...   \n",
      "19         (1)  (1.757<Glucose<=2.411, -0.681<Pregnancies<=-0....   \n",
      "20         (1)  (-0.157<BloodPressure<=0.429, -0.903<Pregnanci...   \n",
      "21         (1)  (-0.157<BloodPressure<=0.429, -0.445<BMI<=-0.187)   \n",
      "22         (1)                             (1.001<Insulin<=1.713)   \n",
      "23         (1)  (0.429<BloodPressure<=1.015, -1.138<Insulin<=-...   \n",
      "24         (1)  (0.429<BloodPressure<=1.015, -0.947<SkinThickn...   \n",
      "25         (1)  (-0.947<SkinThickness<=-0.632, -0.681<Pregnanc...   \n",
      "\n",
      "    antecedent support  consequent support   support  confidence  lift  \\\n",
      "0                  1.0            0.311688  0.311688    0.311688   1.0   \n",
      "1                  1.0            0.298701  0.298701    0.298701   1.0   \n",
      "2                  1.0            0.272727  0.272727    0.272727   1.0   \n",
      "3                  1.0            0.259740  0.259740    0.259740   1.0   \n",
      "4                  1.0            0.259740  0.259740    0.259740   1.0   \n",
      "5                  1.0            0.246753  0.246753    0.246753   1.0   \n",
      "6                  1.0            0.233766  0.233766    0.233766   1.0   \n",
      "7                  1.0            0.220779  0.220779    0.220779   1.0   \n",
      "8                  1.0            0.220779  0.220779    0.220779   1.0   \n",
      "9                  1.0            0.194805  0.194805    0.194805   1.0   \n",
      "10                 1.0            0.168831  0.168831    0.168831   1.0   \n",
      "11                 1.0            0.168831  0.168831    0.168831   1.0   \n",
      "12                 1.0            0.168831  0.168831    0.168831   1.0   \n",
      "13                 1.0            0.155844  0.155844    0.155844   1.0   \n",
      "14                 1.0            0.142857  0.142857    0.142857   1.0   \n",
      "15                 1.0            0.129870  0.129870    0.129870   1.0   \n",
      "16                 1.0            0.129870  0.129870    0.129870   1.0   \n",
      "17                 1.0            0.129870  0.129870    0.129870   1.0   \n",
      "18                 1.0            0.116883  0.116883    0.116883   1.0   \n",
      "19                 1.0            0.116883  0.116883    0.116883   1.0   \n",
      "20                 1.0            0.116883  0.116883    0.116883   1.0   \n",
      "21                 1.0            0.116883  0.116883    0.116883   1.0   \n",
      "22                 1.0            0.103896  0.103896    0.103896   1.0   \n",
      "23                 1.0            0.103896  0.103896    0.103896   1.0   \n",
      "24                 1.0            0.103896  0.103896    0.103896   1.0   \n",
      "25                 1.0            0.103896  0.103896    0.103896   1.0   \n",
      "\n",
      "    leverage  conviction  \n",
      "0        0.0         1.0  \n",
      "1        0.0         1.0  \n",
      "2        0.0         1.0  \n",
      "3        0.0         1.0  \n",
      "4        0.0         1.0  \n",
      "5        0.0         1.0  \n",
      "6        0.0         1.0  \n",
      "7        0.0         1.0  \n",
      "8        0.0         1.0  \n",
      "9        0.0         1.0  \n",
      "10       0.0         1.0  \n",
      "11       0.0         1.0  \n",
      "12       0.0         1.0  \n",
      "13       0.0         1.0  \n",
      "14       0.0         1.0  \n",
      "15       0.0         1.0  \n",
      "16       0.0         1.0  \n",
      "17       0.0         1.0  \n",
      "18       0.0         1.0  \n",
      "19       0.0         1.0  \n",
      "20       0.0         1.0  \n",
      "21       0.0         1.0  \n",
      "22       0.0         1.0  \n",
      "23       0.0         1.0  \n",
      "24       0.0         1.0  \n",
      "25       0.0         1.0  \n",
      "16\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-0.694&lt;Age&lt;=-0.283)</td>\n",
       "      <td>(-1.073&lt;DiabetesPedigreeFunction&lt;=-0.863)</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.339130</td>\n",
       "      <td>0.013156</td>\n",
       "      <td>1.168831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(-1.073&lt;DiabetesPedigreeFunction&lt;=-0.863)</td>\n",
       "      <td>(-0.694&lt;Age&lt;=-0.283)</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>1.339130</td>\n",
       "      <td>0.013156</td>\n",
       "      <td>1.053315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(-0.632&lt;SkinThickness&lt;=-0.32)</td>\n",
       "      <td>(-1.073&lt;DiabetesPedigreeFunction&lt;=-0.863)</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.772575</td>\n",
       "      <td>-0.011469</td>\n",
       "      <td>0.911688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(-1.073&lt;DiabetesPedigreeFunction&lt;=-0.863)</td>\n",
       "      <td>(-0.632&lt;SkinThickness&lt;=-0.32)</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.772575</td>\n",
       "      <td>-0.011469</td>\n",
       "      <td>0.955844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(-1.138&lt;Insulin&lt;=-0.423)</td>\n",
       "      <td>(-1.073&lt;DiabetesPedigreeFunction&lt;=-0.863)</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.836957</td>\n",
       "      <td>-0.015180</td>\n",
       "      <td>0.935065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, -0.445&lt;BMI&lt;=-0.187)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>(1, -0.445&lt;BMI&lt;=-0.187)</td>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429)</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>1.823684</td>\n",
       "      <td>0.052791</td>\n",
       "      <td>1.369540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429)</td>\n",
       "      <td>(1, -0.445&lt;BMI&lt;=-0.187)</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>1.823684</td>\n",
       "      <td>0.052791</td>\n",
       "      <td>1.406494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>(1)</td>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, -0.445&lt;BMI&lt;=-0.187)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>(-0.445&lt;BMI&lt;=-0.187)</td>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429, 1)</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>1.823684</td>\n",
       "      <td>0.052791</td>\n",
       "      <td>1.369540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2712 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            antecedents  \\\n",
       "0                                  (-0.694<Age<=-0.283)   \n",
       "1             (-1.073<DiabetesPedigreeFunction<=-0.863)   \n",
       "2                         (-0.632<SkinThickness<=-0.32)   \n",
       "3             (-1.073<DiabetesPedigreeFunction<=-0.863)   \n",
       "4                              (-1.138<Insulin<=-0.423)   \n",
       "...                                                 ...   \n",
       "2707  (-0.157<BloodPressure<=0.429, -0.445<BMI<=-0.187)   \n",
       "2708                            (1, -0.445<BMI<=-0.187)   \n",
       "2709                      (-0.157<BloodPressure<=0.429)   \n",
       "2710                                                (1)   \n",
       "2711                               (-0.445<BMI<=-0.187)   \n",
       "\n",
       "                                            consequents  antecedent support  \\\n",
       "0             (-1.073<DiabetesPedigreeFunction<=-0.863)            0.129870   \n",
       "1                                  (-0.694<Age<=-0.283)            0.298701   \n",
       "2             (-1.073<DiabetesPedigreeFunction<=-0.863)            0.168831   \n",
       "3                         (-0.632<SkinThickness<=-0.32)            0.298701   \n",
       "4             (-1.073<DiabetesPedigreeFunction<=-0.863)            0.311688   \n",
       "...                                                 ...                 ...   \n",
       "2707                                                (1)            0.116883   \n",
       "2708                      (-0.157<BloodPressure<=0.429)            0.259740   \n",
       "2709                            (1, -0.445<BMI<=-0.187)            0.246753   \n",
       "2710  (-0.157<BloodPressure<=0.429, -0.445<BMI<=-0.187)            1.000000   \n",
       "2711                   (-0.157<BloodPressure<=0.429, 1)            0.259740   \n",
       "\n",
       "      consequent support   support  confidence      lift  leverage  conviction  \n",
       "0               0.298701  0.051948    0.400000  1.339130  0.013156    1.168831  \n",
       "1               0.129870  0.051948    0.173913  1.339130  0.013156    1.053315  \n",
       "2               0.298701  0.038961    0.230769  0.772575 -0.011469    0.911688  \n",
       "3               0.168831  0.038961    0.130435  0.772575 -0.011469    0.955844  \n",
       "4               0.298701  0.077922    0.250000  0.836957 -0.015180    0.935065  \n",
       "...                  ...       ...         ...       ...       ...         ...  \n",
       "2707            1.000000  0.116883    1.000000  1.000000  0.000000         inf  \n",
       "2708            0.246753  0.116883    0.450000  1.823684  0.052791    1.369540  \n",
       "2709            0.259740  0.116883    0.473684  1.823684  0.052791    1.406494  \n",
       "2710            0.116883  0.116883    0.116883  1.000000  0.000000    1.000000  \n",
       "2711            0.246753  0.116883    0.450000  1.823684  0.052791    1.369540  \n",
       "\n",
       "[2712 rows x 9 columns]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_positive = pos_rules[pos_rules['antecedents'] == {pos_label}]\n",
    "print(pos_label)\n",
    "print(pos_rules)\n",
    "rev_positive = rev_positive[rev_positive['confidence'] >= 0.1] # 0.7\n",
    "rev_positive = rev_positive.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "\n",
    "seen = set()\n",
    "dropped = set()\n",
    "indexes_to_drop = []\n",
    "\n",
    "rev_positive = rev_positive.reset_index(drop=True)\n",
    "print(rev_positive)\n",
    "for i in rev_positive.index:\n",
    "    new_rule = rev_positive.loc[[i]]['consequents'].values[0]\n",
    "    \n",
    "    for seen_rule, indx in seen:\n",
    "        if seen_rule.issubset(new_rule):\n",
    "            indexes_to_drop.append(i)\n",
    "            break\n",
    "    else:\n",
    "        seen.add((new_rule, i))\n",
    "\n",
    "rev_positive.drop(rev_positive.index[indexes_to_drop], inplace=True )\n",
    "print(len(rev_positive))\n",
    "\n",
    "\n",
    "\n",
    "rev_negative = neg_rules[neg_rules['antecedents'] == {neg_label}]\n",
    "rev_negative = rev_negative[rev_negative['confidendatace'] >= 0.7]\n",
    "rev_negative = rev_negative.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "\n",
    "seen = set()\n",
    "dropped = set()\n",
    "indexes_to_drop = []\n",
    "\n",
    "rev_negative = rev_negative.reset_index(drop=True)\n",
    "print(len(rev_negative))\n",
    "for i in rev_negative.index:\n",
    "    new_rule = rev_negative.loc[[i]]['consequents'].values[0]\n",
    "    \n",
    "    for seen_rule, indx in seen:\n",
    "        if seen_rule.issubset(new_rule):\n",
    "            indexes_to_drop.append(i)\n",
    "            break\n",
    "    else:\n",
    "        seen.add((new_rule, i))\n",
    "\n",
    "rev_negative.drop(rev_negative.index[indexes_to_drop], inplace=True )\n",
    "print(len(rev_negative))\n",
    "pos_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4447/3026179554.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  rev_both = rev_positive.append(rev_negative, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>itemset</th>\n",
       "      <th>num-items</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>consequent support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>(-1.138&lt;Insulin&lt;=-0.423)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.311688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(-1.073&lt;DiabetesPedigreeFunction&lt;=-0.863)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.298701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>(0.429&lt;BloodPressure&lt;=1.015)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>(1.757&lt;Glucose&lt;=2.411)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.259740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.445&lt;BMI&lt;=-0.187)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.259740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.157&lt;BloodPressure&lt;=0.429)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.246753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.681&lt;Pregnancies&lt;=-0.458)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.233766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.947&lt;SkinThickness&lt;=-0.632)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.220779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.903&lt;Pregnancies&lt;=-0.681)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.220779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.187&lt;BMI&lt;=0.071)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.194805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.632&lt;SkinThickness&lt;=-0.32)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.168831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.32&lt;SkinThickness&lt;=-0.00671)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.168831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.283&lt;Age&lt;=0.126)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>0.155844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.694&lt;Age&lt;=-0.283)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.129870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>(-0.863&lt;DiabetesPedigreeFunction&lt;=-0.652)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.129870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>(1.001&lt;Insulin&lt;=1.713)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.103896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                    itemset num-items   support  \\\n",
       "0      1                   (-1.138<Insulin<=-0.423)         1  0.311688   \n",
       "1      1  (-1.073<DiabetesPedigreeFunction<=-0.863)         1  0.298701   \n",
       "2      1               (0.429<BloodPressure<=1.015)         1  0.272727   \n",
       "3      1                     (1.757<Glucose<=2.411)         1  0.259740   \n",
       "4      1                       (-0.445<BMI<=-0.187)         1  0.259740   \n",
       "5      1              (-0.157<BloodPressure<=0.429)         1  0.246753   \n",
       "6      1               (-0.681<Pregnancies<=-0.458)         1  0.233766   \n",
       "7      1             (-0.947<SkinThickness<=-0.632)         1  0.220779   \n",
       "8      1               (-0.903<Pregnancies<=-0.681)         1  0.220779   \n",
       "9      1                        (-0.187<BMI<=0.071)         1  0.194805   \n",
       "10     1              (-0.632<SkinThickness<=-0.32)         1  0.168831   \n",
       "11     1            (-0.32<SkinThickness<=-0.00671)         1  0.168831   \n",
       "12     1                        (-0.283<Age<=0.126)         1  0.155844   \n",
       "13     1                       (-0.694<Age<=-0.283)         1  0.129870   \n",
       "14     1  (-0.863<DiabetesPedigreeFunction<=-0.652)         1  0.129870   \n",
       "15     1                     (1.001<Insulin<=1.713)         1  0.103896   \n",
       "\n",
       "    confidence  consequent support  \n",
       "0     0.311688            0.311688  \n",
       "1     0.298701            0.298701  \n",
       "2     0.272727            0.272727  \n",
       "3     0.259740            0.259740  \n",
       "4     0.259740            0.259740  \n",
       "5     0.246753            0.246753  \n",
       "6     0.233766            0.233766  \n",
       "7     0.220779            0.220779  \n",
       "8     0.220779            0.220779  \n",
       "9     0.194805            0.194805  \n",
       "10    0.168831            0.168831  \n",
       "11    0.168831            0.168831  \n",
       "12    0.155844            0.155844  \n",
       "13    0.129870            0.129870  \n",
       "14    0.129870            0.129870  \n",
       "15    0.103896            0.103896  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_positive['num-items'] = rev_positive['consequents'].map(lambda x: len(x))\n",
    "rev_negative['num-items'] = rev_negative['consequents'].map(lambda x: len(x))\n",
    "rev_positive['antecedents'] = rev_positive['antecedents'].map(lambda x: pos_label)\n",
    "rev_negative['antecedents'] = rev_negative['antecedents'].map(lambda x: neg_label)\n",
    "\n",
    "rev_both = rev_positive.append(rev_negative, ignore_index=True)\n",
    "\n",
    "chr_rules = rev_both[['antecedents', 'consequents', 'num-items', 'support', \n",
    "                          'confidence', 'consequent support']].sort_values(\n",
    "    ['support', 'confidence', 'num-items'], ascending=[False, False, False])\n",
    "\n",
    "chr_rules = chr_rules.rename(columns={\"antecedents\": \"label\", \"consequents\": \"itemset\"})\n",
    "\n",
    "chr_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(-0.903<Pregnancies<=-0.681,1,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(-0.863<DiabetesPedigreeFunction<=-0.652,1.001<Insulin<=1.713,-0.903<Pregnancies<=-0.681,1,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(0.429<BloodPressure<=1.015,-0.947<SkinThickness<=-0.632,1,)', '(-0.632<SkinThickness<=-0.32,-0.681<Pregnancies<=-0.458,1,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,-0.903<Pregnancies<=-0.681,1,-0.445<BMI<=-0.187,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,0.429<BloodPressure<=1.015,-0.283<Age<=0.126,-0.863<DiabetesPedigreeFunction<=-0.652,-0.903<Pregnancies<=-0.681,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.947<SkinThickness<=-0.632,-0.283<Age<=0.126,0.289<Insulin<=1.001,1,)', '(0.289<Insulin<=1.001,1,-0.445<BMI<=-0.187,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,-0.32<SkinThickness<=-0.00671,1.103<Glucose<=1.757,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,1.713<Insulin<=2.425,-0.903<Pregnancies<=-0.681,1,-0.157<BloodPressure<=0.429,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,0.449<Glucose<=1.103,1,-0.157<BloodPressure<=0.429,)', '(-0.632<SkinThickness<=-0.32,-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,-0.681<Pregnancies<=-0.458,1,)', '(-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,-0.863<DiabetesPedigreeFunction<=-0.652,-0.681<Pregnancies<=-0.458,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,-0.32<SkinThickness<=-0.00671,1,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,-0.947<SkinThickness<=-0.632,0.126<Age<=0.536,-0.903<Pregnancies<=-0.681,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.32<SkinThickness<=-0.00671,1,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,0.429<BloodPressure<=1.015,-0.704<BMI<=-0.445,-0.32<SkinThickness<=-0.00671,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,-0.903<Pregnancies<=-0.681,1,-0.157<BloodPressure<=0.429,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,0.429<BloodPressure<=1.015,-0.947<SkinThickness<=-0.632,-0.863<DiabetesPedigreeFunction<=-0.652,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,1.103<Glucose<=1.757,1,-0.157<BloodPressure<=0.429,)', '(1,)', '(-0.187<BMI<=0.071,-1.283<DiabetesPedigreeFunction<=-1.073,-0.32<SkinThickness<=-0.00671,0.289<Insulin<=1.001,1,-0.157<BloodPressure<=0.429,)', '(-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,-0.283<Age<=0.126,-0.863<DiabetesPedigreeFunction<=-0.652,-0.681<Pregnancies<=-0.458,1,)', '(-0.694<Age<=-0.283,1.001<Insulin<=1.713,1,)', '(-1.126<Pregnancies<=-0.903,1,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(0.429<BloodPressure<=1.015,-0.863<DiabetesPedigreeFunction<=-0.652,-0.681<Pregnancies<=-0.458,1,)', '(-1.138<Insulin<=-0.423,-0.947<SkinThickness<=-0.632,0.126<Age<=0.536,-0.681<Pregnancies<=-0.458,1,)', '(1.001<Insulin<=1.713,1.103<Glucose<=1.757,1,-0.157<BloodPressure<=0.429,)', '(-1.138<Insulin<=-0.423,-0.947<SkinThickness<=-0.632,1.015<BloodPressure<=1.601,1.757<Glucose<=2.411,-0.863<DiabetesPedigreeFunction<=-0.652,1,)', '(-0.704<BMI<=-0.445,-0.903<Pregnancies<=-0.681,1,-0.157<BloodPressure<=0.429,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,1.713<Insulin<=2.425,0.429<BloodPressure<=1.015,1,-0.445<BMI<=-0.187,)', '(-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,-0.283<Age<=0.126,-0.681<Pregnancies<=-0.458,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.423<Insulin<=0.289,-0.32<SkinThickness<=-0.00671,1,)', '(-0.632<SkinThickness<=-0.32,-0.187<BMI<=0.071,-1.283<DiabetesPedigreeFunction<=-1.073,0.289<Insulin<=1.001,1,)', '(0.429<BloodPressure<=1.015,-1.283<DiabetesPedigreeFunction<=-1.073,-0.32<SkinThickness<=-0.00671,0.289<Insulin<=1.001,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.632<SkinThickness<=-0.32,-1.126<Pregnancies<=-0.903,1,)', '(-1.283<DiabetesPedigreeFunction<=-1.073,-0.423<Insulin<=0.289,1,)', '(-1.138<Insulin<=-0.423,0.071<BMI<=0.329,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,1.103<Glucose<=1.757,1,)', '(-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,-0.704<BMI<=-0.445,-0.903<Pregnancies<=-0.681,1,)', '(0.071<BMI<=0.329,0.126<Age<=0.536,0.449<Glucose<=1.103,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.632<SkinThickness<=-0.32,0.449<Glucose<=1.103,1,-0.445<BMI<=-0.187,)', '(-1.138<Insulin<=-0.423,0.071<BMI<=0.329,-0.947<SkinThickness<=-0.632,1.015<BloodPressure<=1.601,1.757<Glucose<=2.411,1,)', '(0.429<BloodPressure<=1.015,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,-0.283<Age<=0.126,-0.681<Pregnancies<=-0.458,1,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,-0.947<SkinThickness<=-0.632,-0.681<Pregnancies<=-0.458,1,)', '(1.757<Glucose<=2.411,-0.32<SkinThickness<=-0.00671,-0.681<Pregnancies<=-0.458,1,-0.157<BloodPressure<=0.429,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,0.429<BloodPressure<=1.015,-0.947<SkinThickness<=-0.632,-0.283<Age<=0.126,-0.863<DiabetesPedigreeFunction<=-0.652,-0.903<Pregnancies<=-0.681,1,)', '(-0.632<SkinThickness<=-0.32,-0.283<Age<=0.126,1.001<Insulin<=1.713,-0.903<Pregnancies<=-0.681,1,-0.445<BMI<=-0.187,)', '(0.429<BloodPressure<=1.015,-0.32<SkinThickness<=-0.00671,1,-0.445<BMI<=-0.187,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-1.138<Insulin<=-0.423,0.071<BMI<=0.329,-0.00671<SkinThickness<=0.306,1.757<Glucose<=2.411,-0.283<Age<=0.126,1,)', '(-0.694<Age<=-0.283,1.103<Glucose<=1.757,1,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(-0.746<BloodPressure<=-0.157,-0.632<SkinThickness<=-0.32,-0.681<Pregnancies<=-0.458,1,-0.445<BMI<=-0.187,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,1.713<Insulin<=2.425,-0.704<BMI<=-0.445,-0.283<Age<=0.126,-0.903<Pregnancies<=-0.681,1,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,0.429<BloodPressure<=1.015,1.757<Glucose<=2.411,-0.681<Pregnancies<=-0.458,1,)', '(-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,-0.681<Pregnancies<=-0.458,1,)', '(-1.283<DiabetesPedigreeFunction<=-1.073,-0.283<Age<=0.126,-0.903<Pregnancies<=-0.681,1,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(-0.632<SkinThickness<=-0.32,1.757<Glucose<=2.411,0.126<Age<=0.536,-0.458<Pregnancies<=-0.235,1,)', '(-0.694<Age<=-0.283,0.429<BloodPressure<=1.015,-1.283<DiabetesPedigreeFunction<=-1.073,-0.32<SkinThickness<=-0.00671,1,-0.445<BMI<=-0.187,)', '(-0.694<Age<=-0.283,0.429<BloodPressure<=1.015,1.757<Glucose<=2.411,0.289<Insulin<=1.001,1,-0.445<BMI<=-0.187,)', '(-0.694<Age<=-0.283,-1.283<DiabetesPedigreeFunction<=-1.073,-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,-0.632<SkinThickness<=-0.32,-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,1.757<Glucose<=2.411,-0.32<SkinThickness<=-0.00671,-0.681<Pregnancies<=-0.458,1,)', '(-0.632<SkinThickness<=-0.32,-0.704<BMI<=-0.445,1.757<Glucose<=2.411,0.126<Age<=0.536,-0.458<Pregnancies<=-0.235,1,)', '(-1.138<Insulin<=-0.423,1.103<Glucose<=1.757,1,-0.445<BMI<=-0.187,)', '(-0.652<DiabetesPedigreeFunction<=-0.442,-0.632<SkinThickness<=-0.32,-0.681<Pregnancies<=-0.458,0.449<Glucose<=1.103,1,-0.445<BMI<=-0.187,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.283<Age<=0.126,1.001<Insulin<=1.713,1,-0.157<BloodPressure<=0.429,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,-0.947<SkinThickness<=-0.632,-0.863<DiabetesPedigreeFunction<=-0.652,-0.681<Pregnancies<=-0.458,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,1.713<Insulin<=2.425,-0.704<BMI<=-0.445,-0.903<Pregnancies<=-0.681,1,)', '(-0.694<Age<=-0.283,-0.903<Pregnancies<=-0.681,1,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,0.429<BloodPressure<=1.015,1.001<Insulin<=1.713,1.103<Glucose<=1.757,1,)', '(-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,1.757<Glucose<=2.411,-0.32<SkinThickness<=-0.00671,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,-0.903<Pregnancies<=-0.681,1,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(-0.187<BMI<=0.071,0.429<BloodPressure<=1.015,-0.947<SkinThickness<=-0.632,-0.681<Pregnancies<=-0.458,1,)', '(-0.187<BMI<=0.071,0.429<BloodPressure<=1.015,-0.947<SkinThickness<=-0.632,-0.681<Pregnancies<=-0.458,1,)', '(-1.138<Insulin<=-0.423,0.429<BloodPressure<=1.015,0.071<BMI<=0.329,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,-0.283<Age<=0.126,1,)', '(-0.187<BMI<=0.071,1.757<Glucose<=2.411,-0.32<SkinThickness<=-0.00671,-0.863<DiabetesPedigreeFunction<=-0.652,-0.903<Pregnancies<=-0.681,1,-0.157<BloodPressure<=0.429,)', '(-0.632<SkinThickness<=-0.32,0.429<BloodPressure<=1.015,1,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,0,0.449<Glucose<=1.103,)', '(-0.694<Age<=-0.283,-0.632<SkinThickness<=-0.32,0,1.103<Glucose<=1.757,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,1.757<Glucose<=2.411,-0.863<DiabetesPedigreeFunction<=-0.652,-0.681<Pregnancies<=-0.458,0,)', '(-0.206<Glucose<=0.449,-0.652<DiabetesPedigreeFunction<=-0.442,1.713<Insulin<=2.425,0,)', '(1.757<Glucose<=2.411,-0.32<SkinThickness<=-0.00671,-0.283<Age<=0.126,0,)', '(1.757<Glucose<=2.411,-0.32<SkinThickness<=-0.00671,0,)', '(0.429<BloodPressure<=1.015,1.757<Glucose<=2.411,-1.126<Pregnancies<=-0.903,0,-0.445<BMI<=-0.187,)', '(0.429<BloodPressure<=1.015,-1.283<DiabetesPedigreeFunction<=-1.073,-0.947<SkinThickness<=-0.632,-1.126<Pregnancies<=-0.903,0,1.103<Glucose<=1.757,)', '(1.015<BloodPressure<=1.601,0.126<Age<=0.536,0,-0.903<Pregnancies<=-0.681,)', '(-0.632<SkinThickness<=-0.32,0,0.449<Glucose<=1.103,-0.445<BMI<=-0.187,)', '(-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,-1.126<Pregnancies<=-0.903,0,-0.445<BMI<=-0.187,)', '(-0.652<DiabetesPedigreeFunction<=-0.442,-0.746<BloodPressure<=-0.157,0.071<BMI<=0.329,0.126<Age<=0.536,0,)', '(-0.632<SkinThickness<=-0.32,0.429<BloodPressure<=1.015,-0.283<Age<=0.126,0,-0.445<BMI<=-0.187,)', '(0.429<BloodPressure<=1.015,-0.283<Age<=0.126,-0.681<Pregnancies<=-0.458,0,-0.445<BMI<=-0.187,)', '(0.429<BloodPressure<=1.015,1.757<Glucose<=2.411,-0.863<DiabetesPedigreeFunction<=-0.652,0,)', '(1.713<Insulin<=2.425,-1.126<Pregnancies<=-0.903,0,0.449<Glucose<=1.103,)', '(-0.694<Age<=-0.283,-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,0,1.103<Glucose<=1.757,)', '(-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,0,0.449<Glucose<=1.103,-0.445<BMI<=-0.187,)', '(1.757<Glucose<=2.411,0,-0.903<Pregnancies<=-0.681,)', '(-0.694<Age<=-0.283,-0.632<SkinThickness<=-0.32,-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,0,-0.445<BMI<=-0.187,)', '(-1.494<DiabetesPedigreeFunction<=-1.283,1.015<BloodPressure<=1.601,-0.32<SkinThickness<=-0.00671,0.126<Age<=0.536,0.289<Insulin<=1.001,-1.126<Pregnancies<=-0.903,0,1.103<Glucose<=1.757,-0.445<BMI<=-0.187,)', '(-0.694<Age<=-0.283,1.757<Glucose<=2.411,-1.126<Pregnancies<=-0.903,0,)', '(0.071<BMI<=0.329,0,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.632<SkinThickness<=-0.32,0.429<BloodPressure<=1.015,-1.126<Pregnancies<=-0.903,0,1.103<Glucose<=1.757,-0.445<BMI<=-0.187,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,1.713<Insulin<=2.425,-0.32<SkinThickness<=-0.00671,0,0.449<Glucose<=1.103,)', '(-1.138<Insulin<=-0.423,-0.947<SkinThickness<=-0.632,1.757<Glucose<=2.411,0,-0.445<BMI<=-0.187,)', '(-0.187<BMI<=0.071,0.429<BloodPressure<=1.015,1.757<Glucose<=2.411,-0.863<DiabetesPedigreeFunction<=-0.652,0,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,-0.632<SkinThickness<=-0.32,-1.126<Pregnancies<=-0.903,0,-0.445<BMI<=-0.187,)', '(0.071<BMI<=0.329,0,-0.903<Pregnancies<=-0.681,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.694<Age<=-0.283,-0.632<SkinThickness<=-0.32,1.757<Glucose<=2.411,1.001<Insulin<=1.713,0,)', '(-0.32<SkinThickness<=-0.00671,-1.126<Pregnancies<=-0.903,0,0.449<Glucose<=1.103,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,0.429<BloodPressure<=1.015,-0.863<DiabetesPedigreeFunction<=-0.652,0,)', '(-0.694<Age<=-0.283,1.015<BloodPressure<=1.601,1.757<Glucose<=2.411,-1.126<Pregnancies<=-0.903,0,-0.445<BMI<=-0.187,)', '(-0.694<Age<=-0.283,0.429<BloodPressure<=1.015,1.757<Glucose<=2.411,-1.126<Pregnancies<=-0.903,0,)', '(1.757<Glucose<=2.411,0,-1.35<Pregnancies<=-1.126,-0.445<BMI<=-0.187,)', '(-0.704<BMI<=-0.445,-0.283<Age<=0.126,1.001<Insulin<=1.713,0,1.103<Glucose<=1.757,-0.157<BloodPressure<=0.429,)', '(0.306<SkinThickness<=0.619,-0.187<BMI<=0.071,1.015<BloodPressure<=1.601,0,1.103<Glucose<=1.757,-1.35<Pregnancies<=-1.126,)', '(0.429<BloodPressure<=1.015,-0.283<Age<=0.126,-0.863<DiabetesPedigreeFunction<=-0.652,0,-0.903<Pregnancies<=-0.681,)', '(1.015<BloodPressure<=1.601,-0.32<SkinThickness<=-0.00671,-0.283<Age<=0.126,0,-0.903<Pregnancies<=-0.681,)', '(-0.746<BloodPressure<=-0.157,-0.694<Age<=-0.283,-0.863<DiabetesPedigreeFunction<=-0.652,0,0.449<Glucose<=1.103,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-1.138<Insulin<=-0.423,1.601<BloodPressure<=2.188,-0.32<SkinThickness<=-0.00671,-1.126<Pregnancies<=-0.903,0,)', '(1.713<Insulin<=2.425,-0.283<Age<=0.126,0,-0.903<Pregnancies<=-0.681,-0.157<BloodPressure<=0.429,)', '(-0.863<DiabetesPedigreeFunction<=-0.652,0,-0.903<Pregnancies<=-0.681,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,-0.863<DiabetesPedigreeFunction<=-0.652,0,)', '(0.429<BloodPressure<=1.015,1.757<Glucose<=2.411,-0.863<DiabetesPedigreeFunction<=-0.652,0,)', '(-1.138<Insulin<=-0.423,-0.187<BMI<=0.071,-0.863<DiabetesPedigreeFunction<=-0.652,0,)', '(1.757<Glucose<=2.411,0,)', '(-1.283<DiabetesPedigreeFunction<=-1.073,0,1.103<Glucose<=1.757,-0.157<BloodPressure<=0.429,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,0,1.103<Glucose<=1.757,)', '(0.429<BloodPressure<=1.015,-1.126<Pregnancies<=-0.903,0,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.632<SkinThickness<=-0.32,-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,0,)', '(-0.206<Glucose<=0.449,-0.652<DiabetesPedigreeFunction<=-0.442,-0.694<Age<=-0.283,1.713<Insulin<=2.425,0,)', '(-0.632<SkinThickness<=-0.32,0,1.103<Glucose<=1.757,-0.157<BloodPressure<=0.429,)', '(-0.652<DiabetesPedigreeFunction<=-0.442,-0.632<SkinThickness<=-0.32,-0.283<Age<=0.126,0,)', '(0.429<BloodPressure<=1.015,-0.947<SkinThickness<=-0.632,-0.863<DiabetesPedigreeFunction<=-0.652,0,-0.445<BMI<=-0.187,)', '(-0.632<SkinThickness<=-0.32,1.757<Glucose<=2.411,0.289<Insulin<=1.001,0,)', '(-0.652<DiabetesPedigreeFunction<=-0.442,-0.746<BloodPressure<=-0.157,-1.138<Insulin<=-0.423,-0.704<BMI<=-0.445,0,)', '(-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,0,1.103<Glucose<=1.757,)', '(-1.073<DiabetesPedigreeFunction<=-0.863,-0.632<SkinThickness<=-0.32,0,-0.903<Pregnancies<=-0.681,)', '(-0.32<SkinThickness<=-0.00671,0,1.103<Glucose<=1.757,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(0,1.103<Glucose<=1.757,-0.445<BMI<=-0.187,-0.157<BloodPressure<=0.429,)', '(0.429<BloodPressure<=1.015,-0.283<Age<=0.126,0,)', '(-0.652<DiabetesPedigreeFunction<=-0.442,-0.746<BloodPressure<=-0.157,-1.138<Insulin<=-0.423,0,)', '(0.536<Age<=0.945,-0.947<SkinThickness<=-0.632,1.015<BloodPressure<=1.601,-0.863<DiabetesPedigreeFunction<=-0.652,0,-0.903<Pregnancies<=-0.681,)', '(-0.746<BloodPressure<=-0.157,-0.694<Age<=-0.283,1.713<Insulin<=2.425,0,)', '(-0.704<BMI<=-0.445,-0.947<SkinThickness<=-0.632,-1.126<Pregnancies<=-0.903,0,1.103<Glucose<=1.757,)', '(0.429<BloodPressure<=1.015,1.757<Glucose<=2.411,0.126<Age<=0.536,0,)', '(-0.632<SkinThickness<=-0.32,0,0.449<Glucose<=1.103,-0.157<BloodPressure<=0.429,)', '(-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,-0.863<DiabetesPedigreeFunction<=-0.652,0,0.449<Glucose<=1.103,)', '(-0.632<SkinThickness<=-0.32,-1.126<Pregnancies<=-0.903,0,-0.445<BMI<=-0.187,)', '(0,-0.903<Pregnancies<=-0.681,-0.445<BMI<=-0.187,)', '(-0.632<SkinThickness<=-0.32,1.713<Insulin<=2.425,0,1.103<Glucose<=1.757,)', '(-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,-0.283<Age<=0.126,-0.863<DiabetesPedigreeFunction<=-0.652,0,)', '(-1.138<Insulin<=-0.423,1.757<Glucose<=2.411,-0.283<Age<=0.126,-0.863<DiabetesPedigreeFunction<=-0.652,0,)', '(-0.863<DiabetesPedigreeFunction<=-0.652,-0.681<Pregnancies<=-0.458,0,)', '(-0.694<Age<=-0.283,-1.138<Insulin<=-0.423,0,)', '(-1.126<Pregnancies<=-0.903,1.001<Insulin<=1.713,0,1.103<Glucose<=1.757,)']\n",
      "[tensor([[-6.4881e-01,  2.1337e+00,  6.5092e-01, -7.2203e-01, -7.2203e-01,\n",
      "          1.6215e-01, -7.1768e-01, -1.3624e-01],\n",
      "        [-1.0777e+00,  1.7713e+00,  1.0983e+00, -1.8037e-01,  2.0100e-01,\n",
      "         -3.8227e-01, -1.0705e+00, -3.5984e-01],\n",
      "        [-6.9133e-01,  2.1268e+00,  7.4536e-01, -6.9133e-01, -6.9133e-01,\n",
      "          2.1481e-03, -6.8918e-01, -1.1113e-01],\n",
      "        [-6.6172e-01,  2.0203e+00,  8.6788e-01, -2.2170e-01, -9.1316e-01,\n",
      "         -1.2950e-01, -9.0805e-01, -5.4070e-02],\n",
      "        [-3.9352e-01,  2.3673e+00, -4.7021e-01, -4.7021e-01, -4.7021e-01,\n",
      "         -4.7021e-01, -4.6629e-01,  3.7337e-01]]), tensor([1, 1, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "       'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
    "print(next(iter(evalloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "       'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
    "    \n",
    "model_predictions = predsList_eval\n",
    "\n",
    "with torch.no_grad():\n",
    "    predsList_eval = []\n",
    "    rulePreds = []\n",
    "    counterRULEDETECTED = 0\n",
    "    counterRULENOTDETECTED = 0\n",
    "    for inputs, lables in evalloader:\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)  \n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "        __, preds = torch.max(outputs, 1)\n",
    "        predsList_eval.extend(preds.cpu())\n",
    "            \n",
    "        for i in inputs:\n",
    "            #if i[7].item() <= -0.283 and -0.694 < i[7].item() and -0.445 < i[5].item() and i[5].item() <= -0.187:\n",
    "            #(0.429<BloodPressure<=1.015)1.757<Glucose<=2.411 -0.632<SkinThickness<=-0.32\n",
    "            #(-0.445<BMI<=-0.187)\n",
    "            if  -0.445< i[5].item() and i[5].item() <=-0.187:\n",
    "                tempRulePreds = 0 # or 0 depending on what rule is usedcounterRULEDETECTED +=1\n",
    "                counterRULEDETECTED +=1\n",
    "            #elif other rule ...\n",
    "        \n",
    "            else:\n",
    "                tempRulePreds = -1 # meaning no matching rule found ### can i do this for coverarge ? \n",
    "                counterRULENOTDETECTED +=1\n",
    "            rulePreds.append(tempRulePreds)\n",
    "            #(-0.694<Age<=-0.283, -0.445<BMI<=-0.187)\t\n",
    "            \n",
    "print(counterRULEDETECTED)\n",
    "print(counterRULENOTDETECTED)\n",
    "\n",
    "\n",
    "def listOfTensorsToListOfItems(listOfTensors):\n",
    "    newList = []\n",
    "    for i in listOfTensors:\n",
    "        newList.append(i.item())\n",
    "    return newList\n",
    "\n",
    "decision_rules_predictions = rulePreds\n",
    "model_predictions = listOfTensorsToListOfItems(predsList_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rosario/explainable/Bachelor/copyCega.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m        \u001b[39m#print(coverageList)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m        \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m applyRulesOnDataset(X_eval,chr_rules,predsList_eval, lableList_model_eval)\n",
      "\u001b[1;32m/home/rosario/explainable/Bachelor/copyCega.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapplyRulesOnDataset\u001b[39m(dataset,rules_df, predsList, lableList_model):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m        rulesList \u001b[39m=\u001b[39m rules_df[\u001b[39m\"\u001b[39;49m\u001b[39mitemset\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mlist\u001b[39;49m(x)))\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m        labelList_rules \u001b[39m=\u001b[39m rules_df[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mlist\u001b[39m(x)))\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m        \u001b[39m#lableList_model \u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1157\u001b[0m             values,\n\u001b[1;32m   1158\u001b[0m             f,\n\u001b[1;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/home/rosario/explainable/Bachelor/copyCega.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapplyRulesOnDataset\u001b[39m(dataset,rules_df, predsList, lableList_model):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m        rulesList \u001b[39m=\u001b[39m rules_df[\u001b[39m\"\u001b[39m\u001b[39mitemset\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mlist\u001b[39;49m(x)))\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m        labelList_rules \u001b[39m=\u001b[39m rules_df[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mlist\u001b[39m(x)))\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rosario/explainable/Bachelor/copyCega.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m        \u001b[39m#lableList_model \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example condition\n",
    "\n",
    "\n",
    "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "       'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "\n",
    "dict= {'Pregnancies':0, 'Glucose':1, 'BloodPressure':2, 'SkinThickness':3, 'Insulin':4,\n",
    "       'BMI':5, 'DiabetesPedigreeFunction':6, 'Age':7}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def applyRulesOnDataset(dataset,rules_df, predsList, lableList_model):\n",
    "       \n",
    "       rulesList = rules_df[\"itemset\"].apply(lambda x: ', '.join(list(x))).astype(\"unicode\")\n",
    "       labelList_rules = rules_df[\"label\"].apply(lambda x: ', '.join(list(x))).astype(\"unicode\")\n",
    "       #lableList_model \n",
    "       \n",
    "       predictedAsModel = [] # adds 0 if not predicted as model , adds 1 if predicted as model  / fidelity\n",
    "\n",
    "       correctPredicted = [] # adds 1 if correct predicted ( like orignial data lable)  /Precision\n",
    "\n",
    "       #coverageList =[] # add  how many instances does this rule cover  (is aplicable ? how) /how useful is a rule to the model\n",
    "       \n",
    "      #samplesCoverdByARule = [] # do i even need this ?  i can get this by comparing the colums of coverageList right ? # add 0 if not covered by any rule  \n",
    "\n",
    "       # coverage = how many samples are at least covered by a rule \n",
    "       #   \n",
    "       for i,(ruleInstance,lable) in enumerate(zip(rulesList,labelList_rules)): # loop through rules\n",
    "              #tempCoverageList = []\n",
    "              tempPredictedAsModel = []\n",
    "              tempCorrectPredicted = []\n",
    "              for j in range(len(dataset)): #for every sample \n",
    "\n",
    "                     #condition = \"00.323<SkinThickness<=-0.00671\"\n",
    "                     condition =  ruleInstance\n",
    "                     split_string = re.split(r\",\\s*\", condition)\n",
    "                     #split_string = tuple(split_string)\n",
    "\n",
    "                     # Regular expression pattern to match the condition\n",
    "                     pattern = r\"([-+]?\\d+\\.\\d+)<(\\w+)<=([-+]?\\d+\\.\\d+)\"\n",
    "                     #print(split_string)\n",
    "\n",
    "                     # Extracting the lower and upper limits and the variable name from the condition\n",
    "\n",
    "\n",
    "\n",
    "                     wholeRuleFit = True\n",
    "                     for k in split_string:\n",
    "\n",
    "                            lower_limit, variable_name, upper_limit = re.match(pattern, k).groups()\n",
    "\n",
    "                            #print(lower_limit)\n",
    "                            #print(variable_name)\n",
    "                            #print(upper_limit)\n",
    "                            # Building the equivalent `if` statement\n",
    "\n",
    "                            if float(lower_limit) < dataset[j][dict[variable_name]] <= float(upper_limit):\n",
    "                                pass\n",
    "                                #print(\"ok\")\n",
    "                            else:\n",
    "                                wholeRuleFit = False\n",
    "                                #print(\"A Condition is false\")\n",
    "                                break\n",
    "                     if wholeRuleFit: #  rule is aplicable on that sample  \n",
    "                            #tempCoverageList.append(1)\n",
    "                            if lable ==  predsList[i]:\n",
    "                                 tempPredictedAsModel.append(1)\n",
    "                            else:\n",
    "                                 tempPredictedAsModel.append(0)\n",
    "\n",
    "\n",
    "                            if lableList_model[i] == lable:\n",
    "                                   tempCorrectPredicted.append(1)\n",
    "                            else:\n",
    "                                   tempCorrectPredicted.append(0)\n",
    "                     else:\n",
    "                           #tempCoverageList.append(0)\n",
    "                           tempPredictedAsModel.append(-1) # not aplicable\n",
    "                           tempCorrectPredicted.append(-1) # not aplicable\n",
    "                           #tempCoverageList.append(-1)  # not aplicable\n",
    "\n",
    "\n",
    "              predictedAsModel.append(tempPredictedAsModel)\n",
    "              correctPredicted.append(tempCorrectPredicted)\n",
    "              #coverageList.append(tempCoverageList)\n",
    "\n",
    "\n",
    "       #print(str(len(rulesList))+\" \"+  str(len(predsList)))\n",
    "\n",
    "\n",
    "      # print(np.shape(np.transpose(predictedAsModel)))      # print(np.shape(np.transpose(predictedAsModel)))\n",
    "\n",
    "       #print(correctPredicted)\n",
    "       predictedAsModel_T = np.transpose(predictedAsModel)\n",
    "       aplicableRulesCounter =  0 # aplicable rules\n",
    "       for i in range(len(predictedAsModel_T)):\n",
    "              if list(predictedAsModel_T[i]).count(-1) != len(rulesList):# and list(predictedAsModel_T[i]).count(0) == 0 :            \n",
    "       \n",
    "                     aplicableRulesCounter += 1\n",
    "\n",
    "\n",
    "       #print(counter)\n",
    "       #print(len(predsList))\n",
    "\n",
    "       coverage = aplicableRulesCounter/ len(predsList)\n",
    "       \n",
    "       #print(coverageList)\n",
    "       return None\n",
    "\n",
    "applyRulesOnDataset(X_eval,chr_rules,predsList_eval, lableList_model_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def calculate_fidelity(model_predictions, decision_rules_predictions):\n",
    "    \"\"\"\n",
    "    Calculates fidelity between model predictions and decision rules predictions.\n",
    "\n",
    "    Args:\n",
    "    model_predictions (numpy array): Array of predicted labels from the trained model.\n",
    "    decision_rules_predictions (numpy array): Array of predicted labels from decision rules.\n",
    "\n",
    "    Returns:\n",
    "    float: Fidelity score between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that the arrays have the same length\n",
    "    assert len(model_predictions) == len(decision_rules_predictions), \"Arrays have different lengths.\"\n",
    "    print(type(model_predictions[0]))\n",
    "    print(type(decision_rules_predictions[0]))\n",
    "    # Calculate the number of agreements between model and decision rules predictions\n",
    "    num_agreements = 0\n",
    "    for i in range(len(model_predictions)):\n",
    "        \n",
    "        if model_predictions[i] == decision_rules_predictions[i]:\n",
    "            num_agreements +=1\n",
    "\n",
    "    #num_agreements = np.sum(model_predictions == decision_rules_predictions)\n",
    "\n",
    "    # Calculate the fidelity score\n",
    "    fidelity_score = num_agreements / len(model_predictions)\n",
    "\n",
    "    return fidelity_score\n",
    "\n",
    "calculate_fidelity(model_predictions, decision_rules_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 424.80it/s]\n",
      "Acc: 0.8441558441558441\n",
      "macro rules recall: 0.5\n",
      "macro rules prec: 0.42207792207792205\n",
      "macro rules f1_score: 0.45774647887323944\n",
      "12 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosario/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0000017672790054]\n",
      "coverage: 0.36363636363636365\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rules_model import *\n",
    "\n",
    "#eval_pred = predsList_eval #xgb_clf.predict(X_dev)\n",
    "rules_model = RulesModel(ohe_df, discr_rules, y_eval, pos_label, neg_label)\n",
    "\n",
    "test_pred = np.array(predsList_test)\n",
    "rules_test_sol = rules_model.eval_rules(df_X_test, test_pred, alpha=10, beta =1, decision_thr=0.97)\n",
    "\n",
    "\n",
    "#cc: 0.19480519480519481\n",
    "#macro rules recall: 0.5\n",
    "#macro rules prec: 0.09740259740259741\n",
    "#macro rules f1_score: 0.16304347826086957\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
